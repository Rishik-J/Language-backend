{
  "FirecrawlMapApi": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API key to use Firecrawl API.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.custom import Component\nfrom langflow.io import (\n    BoolInput,\n    MultilineInput,\n    Output,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\n\n\nclass FirecrawlMapApi(Component):\n    display_name: str = \"FirecrawlMapApi\"\n    description: str = \"Firecrawl Map API.\"\n    name = \"FirecrawlMapApi\"\n\n    documentation: str = \"https://docs.firecrawl.dev/api-reference/endpoint/map\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            required=True,\n            password=True,\n            info=\"The API key to use Firecrawl API.\",\n        ),\n        MultilineInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            required=True,\n            info=\"List of URLs to create maps from (separated by commas or new lines).\",\n            tool_mode=True,\n        ),\n        BoolInput(\n            name=\"ignore_sitemap\",\n            display_name=\"Ignore Sitemap\",\n            info=\"When true, the sitemap.xml file will be ignored during crawling.\",\n        ),\n        BoolInput(\n            name=\"sitemap_only\",\n            display_name=\"Sitemap Only\",\n            info=\"When true, only links found in the sitemap will be returned.\",\n        ),\n        BoolInput(\n            name=\"include_subdomains\",\n            display_name=\"Include Subdomains\",\n            info=\"When true, subdomains of the provided URL will also be scanned.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"map\"),\n    ]\n\n    def map(self) -> Data:\n        try:\n            from firecrawl import FirecrawlApp\n        except ImportError as e:\n            msg = \"Could not import firecrawl integration package. Please install it with `pip install firecrawl-py`.\"\n            raise ImportError(msg) from e\n\n        # Validate URLs\n        if not self.urls:\n            msg = \"URLs are required\"\n            raise ValueError(msg)\n\n        # Split and validate URLs (handle both commas and newlines)\n        urls = [url.strip() for url in self.urls.replace(\"\\n\", \",\").split(\",\") if url.strip()]\n        if not urls:\n            msg = \"No valid URLs provided\"\n            raise ValueError(msg)\n\n        params = {\n            \"ignoreSitemap\": self.ignore_sitemap,\n            \"sitemapOnly\": self.sitemap_only,\n            \"includeSubdomains\": self.include_subdomains,\n        }\n\n        app = FirecrawlApp(api_key=self.api_key)\n\n        # Map all provided URLs and combine results\n        combined_links = []\n        for url in urls:\n            result = app.map_url(url, params=params)\n            if isinstance(result, dict) and \"links\" in result:\n                combined_links.extend(result[\"links\"])\n\n        map_result = {\"success\": True, \"links\": combined_links}\n\n        return Data(data=map_result)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "ignore_sitemap": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "ignore_sitemap",
        "value": false,
        "display_name": "Ignore Sitemap",
        "advanced": false,
        "dynamic": false,
        "info": "When true, the sitemap.xml file will be ignored during crawling.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "include_subdomains": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "include_subdomains",
        "value": false,
        "display_name": "Include Subdomains",
        "advanced": false,
        "dynamic": false,
        "info": "When true, subdomains of the provided URL will also be scanned.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "sitemap_only": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "sitemap_only",
        "value": false,
        "display_name": "Sitemap Only",
        "advanced": false,
        "dynamic": false,
        "info": "When true, only links found in the sitemap will be returned.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "urls": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "urls",
        "value": "",
        "display_name": "URLs",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "List of URLs to create maps from (separated by commas or new lines).",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Firecrawl Map API.",
    "base_classes": [
      "Data"
    ],
    "display_name": "FirecrawlMapApi",
    "documentation": "https://docs.firecrawl.dev/api-reference/endpoint/map",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "data",
        "display_name": "Data",
        "method": "map",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "urls",
      "ignore_sitemap",
      "sitemap_only",
      "include_subdomains"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "FirecrawlCrawlApi": {
    "template": {
      "_type": "Component",
      "crawlerOptions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "crawlerOptions",
        "value": "",
        "display_name": "Crawler Options",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "The crawler options to send with the request.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "scrapeOptions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "scrapeOptions",
        "value": "",
        "display_name": "Scrape Options",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "The page options to send with the request.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API key to use Firecrawl API.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import uuid\n\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, IntInput, MultilineInput, Output, SecretStrInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FirecrawlCrawlApi(Component):\n    display_name: str = \"FirecrawlCrawlApi\"\n    description: str = \"Firecrawl Crawl API.\"\n    name = \"FirecrawlCrawlApi\"\n\n    documentation: str = \"https://docs.firecrawl.dev/v1/api-reference/endpoint/crawl-post\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            required=True,\n            password=True,\n            info=\"The API key to use Firecrawl API.\",\n        ),\n        MultilineInput(\n            name=\"url\",\n            display_name=\"URL\",\n            required=True,\n            info=\"The URL to scrape.\",\n            tool_mode=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout in milliseconds for the request.\",\n        ),\n        StrInput(\n            name=\"idempotency_key\",\n            display_name=\"Idempotency Key\",\n            info=\"Optional idempotency key to ensure unique requests.\",\n        ),\n        DataInput(\n            name=\"crawlerOptions\",\n            display_name=\"Crawler Options\",\n            info=\"The crawler options to send with the request.\",\n        ),\n        DataInput(\n            name=\"scrapeOptions\",\n            display_name=\"Scrape Options\",\n            info=\"The page options to send with the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"crawl\"),\n    ]\n    idempotency_key: str | None = None\n\n    def crawl(self) -> Data:\n        try:\n            from firecrawl import FirecrawlApp\n        except ImportError as e:\n            msg = \"Could not import firecrawl integration package. Please install it with `pip install firecrawl-py`.\"\n            raise ImportError(msg) from e\n\n        params = self.crawlerOptions.__dict__[\"data\"] if self.crawlerOptions else {}\n        scrape_options_dict = self.scrapeOptions.__dict__[\"data\"] if self.scrapeOptions else {}\n        if scrape_options_dict:\n            params[\"scrapeOptions\"] = scrape_options_dict\n\n        # Set default values for new parameters in v1\n        params.setdefault(\"maxDepth\", 2)\n        params.setdefault(\"limit\", 10000)\n        params.setdefault(\"allowExternalLinks\", False)\n        params.setdefault(\"allowBackwardLinks\", False)\n        params.setdefault(\"ignoreSitemap\", False)\n        params.setdefault(\"ignoreQueryParameters\", False)\n\n        # Ensure onlyMainContent is explicitly set if not provided\n        if \"scrapeOptions\" in params:\n            params[\"scrapeOptions\"].setdefault(\"onlyMainContent\", True)\n        else:\n            params[\"scrapeOptions\"] = {\"onlyMainContent\": True}\n\n        if not self.idempotency_key:\n            self.idempotency_key = str(uuid.uuid4())\n\n        app = FirecrawlApp(api_key=self.api_key)\n        crawl_result = app.crawl_url(self.url, params=params, idempotency_key=self.idempotency_key)\n        return Data(data={\"results\": crawl_result})\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "idempotency_key": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "idempotency_key",
        "value": "",
        "display_name": "Idempotency Key",
        "advanced": false,
        "dynamic": false,
        "info": "Optional idempotency key to ensure unique requests.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": "",
        "display_name": "Timeout",
        "advanced": false,
        "dynamic": false,
        "info": "Timeout in milliseconds for the request.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "url": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "url",
        "value": "",
        "display_name": "URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The URL to scrape.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Firecrawl Crawl API.",
    "base_classes": [
      "Data"
    ],
    "display_name": "FirecrawlCrawlApi",
    "documentation": "https://docs.firecrawl.dev/v1/api-reference/endpoint/crawl-post",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "data",
        "display_name": "Data",
        "method": "crawl",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "url",
      "timeout",
      "idempotency_key",
      "crawlerOptions",
      "scrapeOptions"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "FirecrawlExtractApi": {
    "template": {
      "_type": "Component",
      "schema": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "schema",
        "value": "",
        "display_name": "Schema",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "Schema to define the structure of the extracted data.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API key to use Firecrawl API.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from loguru import logger\n\nfrom langflow.custom import Component\nfrom langflow.io import (\n    BoolInput,\n    DataInput,\n    MultilineInput,\n    Output,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\n\n\nclass FirecrawlExtractApi(Component):\n    display_name: str = \"FirecrawlExtractApi\"\n    description: str = \"Firecrawl Extract API.\"\n    name = \"FirecrawlExtractApi\"\n\n    documentation: str = \"https://docs.firecrawl.dev/api-reference/endpoint/extract\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            required=True,\n            password=True,\n            info=\"The API key to use Firecrawl API.\",\n        ),\n        MultilineInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            required=True,\n            info=\"List of URLs to extract data from (separated by commas or new lines).\",\n            tool_mode=True,\n        ),\n        MultilineInput(\n            name=\"prompt\",\n            display_name=\"Prompt\",\n            required=True,\n            info=\"Prompt to guide the extraction process.\",\n            tool_mode=True,\n        ),\n        DataInput(\n            name=\"schema\",\n            display_name=\"Schema\",\n            required=False,\n            info=\"Schema to define the structure of the extracted data.\",\n        ),\n        BoolInput(\n            name=\"enable_web_search\",\n            display_name=\"Enable Web Search\",\n            info=\"When true, the extraction will use web search to find additional data.\",\n        ),\n        # # Optional: Not essential for basic extraction\n        # BoolInput(\n        #     name=\"ignore_sitemap\",\n        #     display_name=\"Ignore Sitemap\",\n        #     info=\"When true, sitemap.xml files will be ignored during website scanning.\",\n        # ),\n        # # Optional: Not essential for basic extraction\n        # BoolInput(\n        #     name=\"include_subdomains\",\n        #     display_name=\"Include Subdomains\",\n        #     info=\"When true, subdomains of the provided URLs will also be scanned.\",\n        # ),\n        # # Optional: Not essential for basic extraction\n        # BoolInput(\n        #     name=\"show_sources\",\n        #     display_name=\"Show Sources\",\n        #     info=\"When true, the sources used to extract the data will be included in the response.\",\n        # ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"extract\"),\n    ]\n\n    def extract(self) -> Data:\n        try:\n            from firecrawl import FirecrawlApp\n        except ImportError as e:\n            msg = \"Could not import firecrawl integration package. Please install it with `pip install firecrawl-py`.\"\n            raise ImportError(msg) from e\n\n        # Validate API key\n        if not self.api_key:\n            msg = \"API key is required\"\n            raise ValueError(msg)\n\n        # Validate URLs\n        if not self.urls:\n            msg = \"URLs are required\"\n            raise ValueError(msg)\n\n        # Split and validate URLs (handle both commas and newlines)\n        urls = [url.strip() for url in self.urls.replace(\"\\n\", \",\").split(\",\") if url.strip()]\n        if not urls:\n            msg = \"No valid URLs provided\"\n            raise ValueError(msg)\n\n        # Validate and process prompt\n        if not self.prompt:\n            msg = \"Prompt is required\"\n            raise ValueError(msg)\n\n        # Get the prompt text (handling both string and multiline input)\n        prompt_text = self.prompt.strip()\n\n        # Enhance the prompt to encourage comprehensive extraction\n        enhanced_prompt = prompt_text\n        if \"schema\" not in prompt_text.lower():\n            enhanced_prompt = f\"{prompt_text}. Please extract all instances in a comprehensive, structured format.\"\n\n        params = {\n            \"prompt\": enhanced_prompt,\n            \"enableWebSearch\": self.enable_web_search,\n            # Optional parameters - not essential for basic extraction\n            \"ignoreSitemap\": self.ignore_sitemap,\n            \"includeSubdomains\": self.include_subdomains,\n            \"showSources\": self.show_sources,\n            \"timeout\": 300,\n        }\n\n        # Only add schema to params if it's provided and is a valid schema structure\n        if self.schema:\n            try:\n                if isinstance(self.schema, dict) and \"type\" in self.schema:\n                    params[\"schema\"] = self.schema\n                elif hasattr(self.schema, \"dict\") and \"type\" in self.schema.dict():\n                    params[\"schema\"] = self.schema.dict()\n                else:\n                    # Skip invalid schema without raising an error\n                    pass\n            except Exception as e:  # noqa: BLE001\n                logger.error(f\"Invalid schema: {e!s}\")\n\n        try:\n            app = FirecrawlApp(api_key=self.api_key)\n            extract_result = app.extract(urls, params=params)\n            return Data(data=extract_result)\n        except Exception as e:\n            msg = f\"Error during extraction: {e!s}\"\n            raise ValueError(msg) from e\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "enable_web_search": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "enable_web_search",
        "value": false,
        "display_name": "Enable Web Search",
        "advanced": false,
        "dynamic": false,
        "info": "When true, the extraction will use web search to find additional data.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "prompt": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "prompt",
        "value": "",
        "display_name": "Prompt",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Prompt to guide the extraction process.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "urls": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "urls",
        "value": "",
        "display_name": "URLs",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "List of URLs to extract data from (separated by commas or new lines).",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Firecrawl Extract API.",
    "base_classes": [
      "Data"
    ],
    "display_name": "FirecrawlExtractApi",
    "documentation": "https://docs.firecrawl.dev/api-reference/endpoint/extract",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "data",
        "display_name": "Data",
        "method": "extract",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "urls",
      "prompt",
      "schema",
      "enable_web_search"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "FirecrawlScrapeApi": {
    "template": {
      "_type": "Component",
      "extractorOptions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "extractorOptions",
        "value": "",
        "display_name": "Extractor Options",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "The extractor options to send with the request.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "scrapeOptions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "scrapeOptions",
        "value": "",
        "display_name": "Scrape Options",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "The page options to send with the request.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API key to use Firecrawl API.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.custom import Component\nfrom langflow.io import (\n    DataInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    SecretStrInput,\n)\nfrom langflow.schema import Data\n\n\nclass FirecrawlScrapeApi(Component):\n    display_name: str = \"FirecrawlScrapeApi\"\n    description: str = \"Firecrawl Scrape API.\"\n    name = \"FirecrawlScrapeApi\"\n\n    documentation: str = \"https://docs.firecrawl.dev/api-reference/endpoint/scrape\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            required=True,\n            password=True,\n            info=\"The API key to use Firecrawl API.\",\n        ),\n        MultilineInput(\n            name=\"url\",\n            display_name=\"URL\",\n            required=True,\n            info=\"The URL to scrape.\",\n            tool_mode=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout in milliseconds for the request.\",\n        ),\n        DataInput(\n            name=\"scrapeOptions\",\n            display_name=\"Scrape Options\",\n            info=\"The page options to send with the request.\",\n        ),\n        DataInput(\n            name=\"extractorOptions\",\n            display_name=\"Extractor Options\",\n            info=\"The extractor options to send with the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"scrape\"),\n    ]\n\n    def scrape(self) -> Data:\n        try:\n            from firecrawl import FirecrawlApp\n        except ImportError as e:\n            msg = \"Could not import firecrawl integration package. Please install it with `pip install firecrawl-py`.\"\n            raise ImportError(msg) from e\n\n        params = self.scrapeOptions.__dict__.get(\"data\", {}) if self.scrapeOptions else {}\n        extractor_options_dict = self.extractorOptions.__dict__.get(\"data\", {}) if self.extractorOptions else {}\n        if extractor_options_dict:\n            params[\"extract\"] = extractor_options_dict\n\n        # Set default values for parameters\n        params.setdefault(\"formats\", [\"markdown\"])  # Default output format\n        params.setdefault(\"onlyMainContent\", True)  # Default to only main content\n\n        app = FirecrawlApp(api_key=self.api_key)\n        results = app.scrape_url(self.url, params=params)\n        return Data(data=results)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": "",
        "display_name": "Timeout",
        "advanced": false,
        "dynamic": false,
        "info": "Timeout in milliseconds for the request.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "url": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "url",
        "value": "",
        "display_name": "URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The URL to scrape.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Firecrawl Scrape API.",
    "base_classes": [
      "Data"
    ],
    "display_name": "FirecrawlScrapeApi",
    "documentation": "https://docs.firecrawl.dev/api-reference/endpoint/scrape",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "data",
        "display_name": "Data",
        "method": "scrape",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "url",
      "timeout",
      "scrapeOptions",
      "extractorOptions"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  }
}