{
  "AssistantsRun": {
    "template": {
      "_type": "Component",
      "assistant_id": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "assistant_id",
        "value": "",
        "display_name": "Assistant ID",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The ID of the assistant to run. \n\nCan be retrieved using the List Assistants component or created with the Create Assistant component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom openai.lib.streaming import AssistantEventHandler\n\nfrom langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema import dotdict\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass AssistantsRun(ComponentWithCache):\n    display_name = \"Run Assistant\"\n    description = \"Executes an Assistant Run against a thread\"\n    icon = \"AstraDB\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.client = get_patched_openai_client(self._shared_component_cache)\n        self.thread_id = None\n\n    def update_build_config(\n        self,\n        build_config: dotdict,\n        field_value: Any,\n        field_name: str | None = None,\n    ) -> None:\n        if field_name == \"thread_id\":\n            if field_value is None:\n                thread = self.client.beta.threads.create()\n                self.thread_id = thread.id\n            build_config[\"thread_id\"] = field_value\n\n    inputs = [\n        MultilineInput(\n            name=\"assistant_id\",\n            display_name=\"Assistant ID\",\n            info=(\n                \"The ID of the assistant to run. \\n\\n\"\n                \"Can be retrieved using the List Assistants component or created with the Create Assistant component.\"\n            ),\n        ),\n        MultilineInput(\n            name=\"user_message\",\n            display_name=\"User Message\",\n            info=\"User message to pass to the run.\",\n        ),\n        MultilineInput(\n            name=\"thread_id\",\n            display_name=\"Thread ID\",\n            required=False,\n            info=\"Thread ID to use with the run. If not provided, a new thread will be created.\",\n        ),\n        MultilineInput(\n            name=\"env_set\",\n            display_name=\"Environment Set\",\n            info=\"Dummy input to allow chaining with Dotenv Component.\",\n        ),\n    ]\n\n    outputs = [Output(display_name=\"Assistant Response\", name=\"assistant_response\", method=\"process_inputs\")]\n\n    def process_inputs(self) -> Message:\n        text = \"\"\n\n        if self.thread_id is None:\n            thread = self.client.beta.threads.create()\n            self.thread_id = thread.id\n\n        # add the user message\n        self.client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=self.user_message)\n\n        class EventHandler(AssistantEventHandler):\n            def __init__(self) -> None:\n                super().__init__()\n\n            def on_exception(self, exception: Exception) -> None:\n                raise exception\n\n        event_handler = EventHandler()\n        with self.client.beta.threads.runs.create_and_stream(\n            thread_id=self.thread_id,\n            assistant_id=self.assistant_id,\n            event_handler=event_handler,\n        ) as stream:\n            for part in stream.text_deltas:\n                text += part\n        return Message(text=text)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_set": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_set",
        "value": "",
        "display_name": "Environment Set",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Dummy input to allow chaining with Dotenv Component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "thread_id": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "thread_id",
        "value": "",
        "display_name": "Thread ID",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Thread ID to use with the run. If not provided, a new thread will be created.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "user_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "user_message",
        "value": "",
        "display_name": "User Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "User message to pass to the run.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Executes an Assistant Run against a thread",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Run Assistant",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "assistant_response",
        "display_name": "Assistant Response",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "assistant_id",
      "user_message",
      "thread_id",
      "env_set"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AssistantsCreateAssistant": {
    "template": {
      "_type": "Component",
      "assistant_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "assistant_name",
        "value": "",
        "display_name": "Assistant Name",
        "advanced": false,
        "dynamic": false,
        "info": "Name for the assistant being created",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from loguru import logger\n\nfrom langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput, StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass AssistantsCreateAssistant(ComponentWithCache):\n    icon = \"AstraDB\"\n    display_name = \"Create Assistant\"\n    description = \"Creates an Assistant and returns it's id\"\n\n    inputs = [\n        StrInput(\n            name=\"assistant_name\",\n            display_name=\"Assistant Name\",\n            info=\"Name for the assistant being created\",\n        ),\n        StrInput(\n            name=\"instructions\",\n            display_name=\"Instructions\",\n            info=\"Instructions for the assistant, think of these as the system prompt.\",\n        ),\n        StrInput(\n            name=\"model\",\n            display_name=\"Model name\",\n            info=(\n                \"Model for the assistant.\\n\\n\"\n                \"Environment variables for provider credentials can be set with the Dotenv Component.\\n\\n\"\n                \"Models are supported via LiteLLM, \"\n                \"see (https://docs.litellm.ai/docs/providers) for supported model names and env vars.\"\n            ),\n            # refresh_model=True\n        ),\n        MultilineInput(\n            name=\"env_set\",\n            display_name=\"Environment Set\",\n            info=\"Dummy input to allow chaining with Dotenv Component.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Assistant ID\", name=\"assistant_id\", method=\"process_inputs\"),\n    ]\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.client = get_patched_openai_client(self._shared_component_cache)\n\n    def process_inputs(self) -> Message:\n        logger.info(f\"env_set is {self.env_set}\")\n        assistant = self.client.beta.assistants.create(\n            name=self.assistant_name,\n            instructions=self.instructions,\n            model=self.model,\n        )\n        return Message(text=assistant.id)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_set": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_set",
        "value": "",
        "display_name": "Environment Set",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Dummy input to allow chaining with Dotenv Component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "instructions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "instructions",
        "value": "",
        "display_name": "Instructions",
        "advanced": false,
        "dynamic": false,
        "info": "Instructions for the assistant, think of these as the system prompt.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "",
        "display_name": "Model name",
        "advanced": false,
        "dynamic": false,
        "info": "Model for the assistant.\n\nEnvironment variables for provider credentials can be set with the Dotenv Component.\n\nModels are supported via LiteLLM, see (https://docs.litellm.ai/docs/providers) for supported model names and env vars.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      }
    },
    "description": "Creates an Assistant and returns it's id",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Create Assistant",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "assistant_id",
        "display_name": "Assistant ID",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "assistant_name",
      "instructions",
      "model",
      "env_set"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "Astra Assistant Agent": {
    "template": {
      "_type": "Component",
      "file": {
        "trace_as_metadata": true,
        "file_path": "",
        "fileTypes": [
          "txt",
          "md",
          "mdx",
          "csv",
          "json",
          "yaml",
          "yml",
          "xml",
          "html",
          "htm",
          "pdf",
          "docx",
          "py",
          "sh",
          "sql",
          "js",
          "ts",
          "tsx",
          "jpg",
          "jpeg",
          "png",
          "bmp",
          "image",
          "zip",
          "tar",
          "tgz",
          "bz2",
          "gz",
          "c",
          "cpp",
          "cs",
          "css",
          "go",
          "java",
          "php",
          "rb",
          "tex",
          "doc",
          "docx",
          "ppt",
          "pptx",
          "xls",
          "xlsx",
          "jsonl"
        ],
        "temp_file": false,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "file",
        "value": "",
        "display_name": "File(s) for retrieval",
        "advanced": false,
        "dynamic": false,
        "info": "Files to be sent with the message.",
        "title_case": false,
        "type": "file",
        "_input_type": "FileInput"
      },
      "input_tools": {
        "trace_as_metadata": true,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_tools",
        "value": "",
        "display_name": "Tools",
        "advanced": false,
        "input_types": [
          "Tool"
        ],
        "dynamic": false,
        "info": "These are the tools that the agent can use to help with tasks.",
        "title_case": false,
        "type": "other",
        "_input_type": "HandleInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import asyncio\nfrom asyncio import to_thread\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom astra_assistants.astra_assistants_manager import AssistantManager\nfrom langchain_core.agents import AgentFinish\nfrom loguru import logger\n\nfrom langflow.base.agents.events import ExceptionWithMessageError, process_agent_events\nfrom langflow.base.astra_assistants.util import (\n    get_patched_openai_client,\n    litellm_model_names,\n    sync_upload,\n    wrap_base_tool_as_tool_interface,\n)\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import DropdownInput, FileInput, HandleInput, MultilineInput\nfrom langflow.memory import delete_message\nfrom langflow.schema.content_block import ContentBlock\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\nfrom langflow.utils.constants import MESSAGE_SENDER_AI\n\nif TYPE_CHECKING:\n    from langflow.schema.log import SendMessageFunctionType\n\n\nclass AstraAssistantManager(ComponentWithCache):\n    display_name = \"Astra Assistant Agent\"\n    name = \"Astra Assistant Agent\"\n    description = \"Manages Assistant Interactions\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            advanced=False,\n            options=litellm_model_names,\n            value=\"gpt-4o-mini\",\n        ),\n        MultilineInput(\n            name=\"instructions\",\n            display_name=\"Agent Instructions\",\n            info=\"Instructions for the assistant, think of these as the system prompt.\",\n        ),\n        HandleInput(\n            name=\"input_tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            required=False,\n            info=\"These are the tools that the agent can use to help with tasks.\",\n        ),\n        # DropdownInput(\n        #    display_name=\"Tools\",\n        #    name=\"tool\",\n        #    options=tool_names,\n        # ),\n        MultilineInput(\n            name=\"user_message\", display_name=\"User Message\", info=\"User message to pass to the run.\", tool_mode=True\n        ),\n        FileInput(\n            name=\"file\",\n            display_name=\"File(s) for retrieval\",\n            list=True,\n            info=\"Files to be sent with the message.\",\n            required=False,\n            show=True,\n            file_types=[\n                \"txt\",\n                \"md\",\n                \"mdx\",\n                \"csv\",\n                \"json\",\n                \"yaml\",\n                \"yml\",\n                \"xml\",\n                \"html\",\n                \"htm\",\n                \"pdf\",\n                \"docx\",\n                \"py\",\n                \"sh\",\n                \"sql\",\n                \"js\",\n                \"ts\",\n                \"tsx\",\n                \"jpg\",\n                \"jpeg\",\n                \"png\",\n                \"bmp\",\n                \"image\",\n                \"zip\",\n                \"tar\",\n                \"tgz\",\n                \"bz2\",\n                \"gz\",\n                \"c\",\n                \"cpp\",\n                \"cs\",\n                \"css\",\n                \"go\",\n                \"java\",\n                \"php\",\n                \"rb\",\n                \"tex\",\n                \"doc\",\n                \"docx\",\n                \"ppt\",\n                \"pptx\",\n                \"xls\",\n                \"xlsx\",\n                \"jsonl\",\n            ],\n        ),\n        MultilineInput(\n            name=\"input_thread_id\",\n            display_name=\"Thread ID (optional)\",\n            info=\"ID of the thread\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"input_assistant_id\",\n            display_name=\"Assistant ID (optional)\",\n            info=\"ID of the assistant\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"env_set\",\n            display_name=\"Environment Set\",\n            info=\"Dummy input to allow chaining with Dotenv Component.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Assistant Response\", name=\"assistant_response\", method=\"get_assistant_response\"),\n        Output(display_name=\"Tool output\", name=\"tool_output\", method=\"get_tool_output\", hidden=True),\n        Output(display_name=\"Thread Id\", name=\"output_thread_id\", method=\"get_thread_id\", hidden=True),\n        Output(display_name=\"Assistant Id\", name=\"output_assistant_id\", method=\"get_assistant_id\", hidden=True),\n        Output(display_name=\"Vector Store Id\", name=\"output_vs_id\", method=\"get_vs_id\", hidden=True),\n    ]\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.lock = asyncio.Lock()\n        self.initialized: bool = False\n        self._assistant_response: Message = None  # type: ignore[assignment]\n        self._tool_output: Message = None  # type: ignore[assignment]\n        self._thread_id: Message = None  # type: ignore[assignment]\n        self._assistant_id: Message = None  # type: ignore[assignment]\n        self._vs_id: Message = None  # type: ignore[assignment]\n        self.client = get_patched_openai_client(self._shared_component_cache)\n        self.input_tools: list[Any]\n\n    async def get_assistant_response(self) -> Message:\n        await self.initialize()\n        self.status = self._assistant_response\n        return self._assistant_response\n\n    async def get_vs_id(self) -> Message:\n        await self.initialize()\n        self.status = self._vs_id\n        return self._vs_id\n\n    async def get_tool_output(self) -> Message:\n        await self.initialize()\n        self.status = self._tool_output\n        return self._tool_output\n\n    async def get_thread_id(self) -> Message:\n        await self.initialize()\n        self.status = self._thread_id\n        return self._thread_id\n\n    async def get_assistant_id(self) -> Message:\n        await self.initialize()\n        self.status = self._assistant_id\n        return self._assistant_id\n\n    async def initialize(self) -> None:\n        async with self.lock:\n            if not self.initialized:\n                await self.process_inputs()\n                self.initialized = True\n\n    async def process_inputs(self) -> None:\n        logger.info(f\"env_set is {self.env_set}\")\n        logger.info(self.input_tools)\n        tools = []\n        tool_obj = None\n        if self.input_tools is None:\n            self.input_tools = []\n        for tool in self.input_tools:\n            tool_obj = wrap_base_tool_as_tool_interface(tool)\n            tools.append(tool_obj)\n\n        assistant_id = None\n        thread_id = None\n        if self.input_assistant_id:\n            assistant_id = self.input_assistant_id\n        if self.input_thread_id:\n            thread_id = self.input_thread_id\n\n        if hasattr(self, \"graph\"):\n            session_id = self.graph.session_id\n        elif hasattr(self, \"_session_id\"):\n            session_id = self._session_id\n        else:\n            session_id = None\n\n        agent_message = Message(\n            sender=MESSAGE_SENDER_AI,\n            sender_name=self.display_name or \"Astra Assistant\",\n            properties={\"icon\": \"Bot\", \"state\": \"partial\"},\n            content_blocks=[ContentBlock(title=\"Assistant Steps\", contents=[])],\n            session_id=session_id,\n        )\n\n        assistant_manager = AssistantManager(\n            instructions=self.instructions,\n            model=self.model_name,\n            name=\"managed_assistant\",\n            tools=tools,\n            client=self.client,\n            thread_id=thread_id,\n            assistant_id=assistant_id,\n        )\n\n        if self.file:\n            file = await to_thread(sync_upload, self.file, assistant_manager.client)\n            vector_store = assistant_manager.client.beta.vector_stores.create(name=\"my_vs\", file_ids=[file.id])\n            assistant_tools = assistant_manager.assistant.tools\n            assistant_tools += [{\"type\": \"file_search\"}]\n            assistant = assistant_manager.client.beta.assistants.update(\n                assistant_manager.assistant.id,\n                tools=assistant_tools,\n                tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n            )\n            assistant_manager.assistant = assistant\n\n        async def step_iterator():\n            # Initial event\n            yield {\"event\": \"on_chain_start\", \"name\": \"AstraAssistant\", \"data\": {\"input\": {\"text\": self.user_message}}}\n\n            content = self.user_message\n            result = await assistant_manager.run_thread(content=content, tool=tool_obj)\n\n            # Tool usage if present\n            if \"output\" in result and \"arguments\" in result:\n                yield {\"event\": \"on_tool_start\", \"name\": \"tool\", \"data\": {\"input\": {\"text\": str(result[\"arguments\"])}}}\n                yield {\"event\": \"on_tool_end\", \"name\": \"tool\", \"data\": {\"output\": result[\"output\"]}}\n\n            if \"file_search\" in result and result[\"file_search\"] is not None:\n                yield {\"event\": \"on_tool_start\", \"name\": \"tool\", \"data\": {\"input\": {\"text\": self.user_message}}}\n                file_search_str = \"\"\n                for chunk in result[\"file_search\"].to_dict().get(\"chunks\", []):\n                    file_search_str += f\"## Chunk ID: `{chunk['chunk_id']}`\\n\"\n                    file_search_str += f\"**Content:**\\n\\n```\\n{chunk['content']}\\n```\\n\\n\"\n                    if \"score\" in chunk:\n                        file_search_str += f\"**Score:** {chunk['score']}\\n\\n\"\n                    if \"file_id\" in chunk:\n                        file_search_str += f\"**File ID:** `{chunk['file_id']}`\\n\\n\"\n                    if \"file_name\" in chunk:\n                        file_search_str += f\"**File Name:** `{chunk['file_name']}`\\n\\n\"\n                    if \"bytes\" in chunk:\n                        file_search_str += f\"**Bytes:** {chunk['bytes']}\\n\\n\"\n                    if \"search_string\" in chunk:\n                        file_search_str += f\"**Search String:** {chunk['search_string']}\\n\\n\"\n                yield {\"event\": \"on_tool_end\", \"name\": \"tool\", \"data\": {\"output\": file_search_str}}\n\n            if \"text\" not in result:\n                msg = f\"No text in result, {result}\"\n                raise ValueError(msg)\n\n            self._assistant_response = Message(text=result[\"text\"])\n            if \"decision\" in result:\n                self._tool_output = Message(text=str(result[\"decision\"].is_complete))\n            else:\n                self._tool_output = Message(text=result[\"text\"])\n            self._thread_id = Message(text=assistant_manager.thread.id)\n            self._assistant_id = Message(text=assistant_manager.assistant.id)\n\n            # Final event - format it like AgentFinish to match the expected format\n            yield {\n                \"event\": \"on_chain_end\",\n                \"name\": \"AstraAssistant\",\n                \"data\": {\"output\": AgentFinish(return_values={\"output\": result[\"text\"]}, log=\"\")},\n            }\n\n        try:\n            if hasattr(self, \"send_message\"):\n                processed_result = await process_agent_events(\n                    step_iterator(),\n                    agent_message,\n                    cast(\"SendMessageFunctionType\", self.send_message),\n                )\n                self.status = processed_result\n        except ExceptionWithMessageError as e:\n            msg_id = e.agent_message.id\n            await delete_message(id_=msg_id)\n            await self._send_message_event(e.agent_message, category=\"remove_message\")\n            raise\n        except Exception:\n            raise\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_set": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_set",
        "value": "",
        "display_name": "Environment Set",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Dummy input to allow chaining with Dotenv Component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "input_assistant_id": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_assistant_id",
        "value": "",
        "display_name": "Assistant ID (optional)",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "ID of the assistant",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "input_thread_id": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_thread_id",
        "value": "",
        "display_name": "Thread ID (optional)",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "ID of the thread",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "instructions": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "instructions",
        "value": "",
        "display_name": "Agent Instructions",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Instructions for the assistant, think of these as the system prompt.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "omni-moderation-latest",
          "omni-moderation-latest-intents",
          "omni-moderation-2024-09-26",
          "gpt-4",
          "gpt-4.1",
          "gpt-4.1-2025-04-14",
          "gpt-4.1-mini",
          "gpt-4.1-mini-2025-04-14",
          "gpt-4.1-nano",
          "gpt-4.1-nano-2025-04-14",
          "gpt-4o",
          "watsonx/ibm/granite-3-8b-instruct",
          "gpt-4o-search-preview-2025-03-11",
          "gpt-4o-search-preview",
          "gpt-4.5-preview",
          "gpt-4.5-preview-2025-02-27",
          "gpt-4o-audio-preview",
          "gpt-4o-audio-preview-2024-12-17",
          "gpt-4o-audio-preview-2024-10-01",
          "gpt-4o-mini-audio-preview-2024-12-17",
          "gpt-4o-mini",
          "gpt-4o-mini-search-preview-2025-03-11",
          "gpt-4o-mini-search-preview",
          "gpt-4o-mini-2024-07-18",
          "o1-pro",
          "o1-pro-2025-03-19",
          "o1",
          "o1-mini",
          "computer-use-preview",
          "o3",
          "o3-2025-04-16",
          "o3-mini",
          "o3-mini-2025-01-31",
          "o4-mini",
          "o4-mini-2025-04-16",
          "o1-mini-2024-09-12",
          "o1-preview",
          "o1-preview-2024-09-12",
          "o1-2024-12-17",
          "chatgpt-4o-latest",
          "gpt-4o-2024-05-13",
          "gpt-4o-2024-08-06",
          "gpt-4o-2024-11-20",
          "gpt-4o-realtime-preview-2024-10-01",
          "gpt-4o-realtime-preview",
          "gpt-4o-realtime-preview-2024-12-17",
          "gpt-4o-mini-realtime-preview",
          "gpt-4o-mini-realtime-preview-2024-12-17",
          "gpt-4-turbo-preview",
          "gpt-4-0314",
          "gpt-4-0613",
          "gpt-4-32k",
          "gpt-4-32k-0314",
          "gpt-4-32k-0613",
          "gpt-4-turbo",
          "gpt-4-turbo-2024-04-09",
          "gpt-4-1106-preview",
          "gpt-4-0125-preview",
          "gpt-4-vision-preview",
          "gpt-4-1106-vision-preview",
          "gpt-3.5-turbo",
          "gpt-3.5-turbo-0301",
          "gpt-3.5-turbo-0613",
          "gpt-3.5-turbo-1106",
          "gpt-3.5-turbo-0125",
          "gpt-3.5-turbo-16k",
          "gpt-3.5-turbo-16k-0613",
          "ft:gpt-3.5-turbo",
          "ft:gpt-3.5-turbo-0125",
          "ft:gpt-3.5-turbo-1106",
          "ft:gpt-3.5-turbo-0613",
          "ft:gpt-4-0613",
          "ft:gpt-4o-2024-08-06",
          "ft:gpt-4o-2024-11-20",
          "ft:gpt-4o-mini-2024-07-18",
          "ft:davinci-002",
          "ft:babbage-002",
          "text-embedding-3-large",
          "text-embedding-3-small",
          "text-embedding-ada-002",
          "text-embedding-ada-002-v2",
          "text-moderation-stable",
          "text-moderation-007",
          "text-moderation-latest",
          "256-x-256/dall-e-2",
          "512-x-512/dall-e-2",
          "1024-x-1024/dall-e-2",
          "hd/1024-x-1792/dall-e-3",
          "hd/1792-x-1024/dall-e-3",
          "hd/1024-x-1024/dall-e-3",
          "standard/1024-x-1792/dall-e-3",
          "standard/1792-x-1024/dall-e-3",
          "standard/1024-x-1024/dall-e-3",
          "gpt-image-1",
          "low/1024-x-1024/gpt-image-1",
          "medium/1024-x-1024/gpt-image-1",
          "high/1024-x-1024/gpt-image-1",
          "low/1024-x-1536/gpt-image-1",
          "medium/1024-x-1536/gpt-image-1",
          "high/1024-x-1536/gpt-image-1",
          "low/1536-x-1024/gpt-image-1",
          "medium/1536-x-1024/gpt-image-1",
          "high/1536-x-1024/gpt-image-1",
          "gpt-4o-transcribe",
          "gpt-4o-mini-transcribe",
          "whisper-1",
          "tts-1",
          "tts-1-hd",
          "gpt-4o-mini-tts",
          "azure/gpt-4o-mini-tts",
          "azure/computer-use-preview",
          "azure/gpt-4o-audio-preview-2024-12-17",
          "azure/gpt-4o-mini-audio-preview-2024-12-17",
          "azure/gpt-4.1",
          "azure/gpt-4.1-2025-04-14",
          "azure/gpt-4.1-mini",
          "azure/gpt-4.1-mini-2025-04-14",
          "azure/gpt-4.1-nano",
          "azure/gpt-4.1-nano-2025-04-14",
          "azure/o3",
          "azure/o3-2025-04-16",
          "azure/o4-mini",
          "azure/gpt-4o-mini-realtime-preview-2024-12-17",
          "azure/eu/gpt-4o-mini-realtime-preview-2024-12-17",
          "azure/us/gpt-4o-mini-realtime-preview-2024-12-17",
          "azure/gpt-4o-realtime-preview-2024-12-17",
          "azure/us/gpt-4o-realtime-preview-2024-12-17",
          "azure/eu/gpt-4o-realtime-preview-2024-12-17",
          "azure/gpt-4o-realtime-preview-2024-10-01",
          "azure/us/gpt-4o-realtime-preview-2024-10-01",
          "azure/eu/gpt-4o-realtime-preview-2024-10-01",
          "azure/o4-mini-2025-04-16",
          "azure/o3-mini-2025-01-31",
          "azure/us/o3-mini-2025-01-31",
          "azure/eu/o3-mini-2025-01-31",
          "azure/tts-1",
          "azure/tts-1-hd",
          "azure/whisper-1",
          "azure/o3-mini",
          "azure/o1-mini",
          "azure/o1-mini-2024-09-12",
          "azure/us/o1-mini-2024-09-12",
          "azure/eu/o1-mini-2024-09-12",
          "azure/o1",
          "azure/o1-2024-12-17",
          "azure/us/o1-2024-12-17",
          "azure/eu/o1-2024-12-17",
          "azure/o1-preview",
          "azure/o1-preview-2024-09-12",
          "azure/us/o1-preview-2024-09-12",
          "azure/eu/o1-preview-2024-09-12",
          "azure/gpt-4.5-preview",
          "azure/gpt-4o",
          "azure/global/gpt-4o-2024-11-20",
          "azure/gpt-4o-2024-08-06",
          "azure/global/gpt-4o-2024-08-06",
          "azure/gpt-4o-2024-11-20",
          "azure/us/gpt-4o-2024-11-20",
          "azure/eu/gpt-4o-2024-11-20",
          "azure/gpt-4o-2024-05-13",
          "azure/global-standard/gpt-4o-2024-08-06",
          "azure/us/gpt-4o-2024-08-06",
          "azure/eu/gpt-4o-2024-08-06",
          "azure/global-standard/gpt-4o-2024-11-20",
          "azure/global-standard/gpt-4o-mini",
          "azure/gpt-4o-mini",
          "azure/gpt-4o-mini-2024-07-18",
          "azure/us/gpt-4o-mini-2024-07-18",
          "azure/eu/gpt-4o-mini-2024-07-18",
          "azure/gpt-4-turbo-2024-04-09",
          "azure/gpt-4-0125-preview",
          "azure/gpt-4-1106-preview",
          "azure/gpt-4-0613",
          "azure/gpt-4-32k-0613",
          "azure/gpt-4-32k",
          "azure/gpt-4",
          "azure/gpt-4-turbo",
          "azure/gpt-4-turbo-vision-preview",
          "azure/gpt-35-turbo-16k-0613",
          "azure/gpt-35-turbo-1106",
          "azure/gpt-35-turbo-0613",
          "azure/gpt-35-turbo-0301",
          "azure/gpt-35-turbo-0125",
          "azure/gpt-3.5-turbo-0125",
          "azure/gpt-35-turbo-16k",
          "azure/gpt-35-turbo",
          "azure/gpt-3.5-turbo",
          "azure/gpt-3.5-turbo-instruct-0914",
          "azure/gpt-35-turbo-instruct",
          "azure/gpt-35-turbo-instruct-0914",
          "azure/mistral-large-latest",
          "azure/mistral-large-2402",
          "azure/command-r-plus",
          "azure/ada",
          "azure/text-embedding-ada-002",
          "azure/text-embedding-3-large",
          "azure/text-embedding-3-small",
          "azure/gpt-image-1",
          "azure/low/1024-x-1024/gpt-image-1",
          "azure/medium/1024-x-1024/gpt-image-1",
          "azure/high/1024-x-1024/gpt-image-1",
          "azure/low/1024-x-1536/gpt-image-1",
          "azure/medium/1024-x-1536/gpt-image-1",
          "azure/high/1024-x-1536/gpt-image-1",
          "azure/low/1536-x-1024/gpt-image-1",
          "azure/medium/1536-x-1024/gpt-image-1",
          "azure/high/1536-x-1024/gpt-image-1",
          "azure/standard/1024-x-1024/dall-e-3",
          "azure/hd/1024-x-1024/dall-e-3",
          "azure/standard/1024-x-1792/dall-e-3",
          "azure/standard/1792-x-1024/dall-e-3",
          "azure/hd/1024-x-1792/dall-e-3",
          "azure/hd/1792-x-1024/dall-e-3",
          "azure/standard/1024-x-1024/dall-e-2",
          "azure_ai/deepseek-r1",
          "azure_ai/deepseek-v3",
          "azure_ai/deepseek-v3-0324",
          "azure_ai/jamba-instruct",
          "azure_ai/mistral-nemo",
          "azure_ai/mistral-large",
          "azure_ai/mistral-small",
          "azure_ai/mistral-small-2503",
          "azure_ai/mistral-large-2407",
          "azure_ai/mistral-large-latest",
          "azure_ai/ministral-3b",
          "azure_ai/Llama-3.2-11B-Vision-Instruct",
          "azure_ai/Llama-3.3-70B-Instruct",
          "azure_ai/Llama-4-Scout-17B-16E-Instruct",
          "azure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8",
          "azure_ai/Llama-3.2-90B-Vision-Instruct",
          "azure_ai/Meta-Llama-3-70B-Instruct",
          "azure_ai/Meta-Llama-3.1-8B-Instruct",
          "azure_ai/Meta-Llama-3.1-70B-Instruct",
          "azure_ai/Meta-Llama-3.1-405B-Instruct",
          "azure_ai/Phi-4-mini-instruct",
          "azure_ai/Phi-4-multimodal-instruct",
          "azure_ai/Phi-4",
          "azure_ai/Phi-3.5-mini-instruct",
          "azure_ai/Phi-3.5-vision-instruct",
          "azure_ai/Phi-3.5-MoE-instruct",
          "azure_ai/Phi-3-mini-4k-instruct",
          "azure_ai/Phi-3-mini-128k-instruct",
          "azure_ai/Phi-3-small-8k-instruct",
          "azure_ai/Phi-3-small-128k-instruct",
          "azure_ai/Phi-3-medium-4k-instruct",
          "azure_ai/Phi-3-medium-128k-instruct",
          "azure_ai/cohere-rerank-v3-multilingual",
          "azure_ai/cohere-rerank-v3-english",
          "azure_ai/Cohere-embed-v3-english",
          "azure_ai/Cohere-embed-v3-multilingual",
          "azure_ai/embed-v-4-0",
          "babbage-002",
          "davinci-002",
          "gpt-3.5-turbo-instruct",
          "gpt-3.5-turbo-instruct-0914",
          "claude-instant-1",
          "mistral/mistral-tiny",
          "mistral/mistral-small",
          "mistral/mistral-small-latest",
          "mistral/mistral-medium",
          "mistral/mistral-medium-latest",
          "mistral/mistral-medium-2312",
          "mistral/mistral-large-latest",
          "mistral/mistral-large-2411",
          "mistral/mistral-large-2402",
          "mistral/mistral-large-2407",
          "mistral/pixtral-large-latest",
          "mistral/pixtral-large-2411",
          "mistral/pixtral-12b-2409",
          "mistral/open-mistral-7b",
          "mistral/open-mixtral-8x7b",
          "mistral/open-mixtral-8x22b",
          "mistral/codestral-latest",
          "mistral/codestral-2405",
          "mistral/open-mistral-nemo",
          "mistral/open-mistral-nemo-2407",
          "mistral/open-codestral-mamba",
          "mistral/codestral-mamba-latest",
          "mistral/mistral-embed",
          "deepseek/deepseek-reasoner",
          "deepseek/deepseek-chat",
          "codestral/codestral-latest",
          "codestral/codestral-2405",
          "text-completion-codestral/codestral-latest",
          "text-completion-codestral/codestral-2405",
          "xai/grok-beta",
          "xai/grok-2-vision-1212",
          "xai/grok-2-vision-latest",
          "xai/grok-2-vision",
          "xai/grok-3-beta",
          "xai/grok-3-fast-beta",
          "xai/grok-3-fast-latest",
          "xai/grok-3-mini-beta",
          "xai/grok-3-mini-fast-beta",
          "xai/grok-3-mini-fast-latest",
          "xai/grok-vision-beta",
          "xai/grok-2-1212",
          "xai/grok-2",
          "xai/grok-2-latest",
          "deepseek/deepseek-coder",
          "groq/deepseek-r1-distill-llama-70b",
          "groq/llama-3.3-70b-versatile",
          "groq/llama-3.3-70b-specdec",
          "groq/llama-guard-3-8b",
          "groq/llama2-70b-4096",
          "groq/llama3-8b-8192",
          "groq/llama-3.2-1b-preview",
          "groq/llama-3.2-3b-preview",
          "groq/llama-3.2-11b-text-preview",
          "groq/llama-3.2-11b-vision-preview",
          "groq/llama-3.2-90b-text-preview",
          "groq/llama-3.2-90b-vision-preview",
          "groq/llama3-70b-8192",
          "groq/llama-3.1-8b-instant",
          "groq/llama-3.1-70b-versatile",
          "groq/llama-3.1-405b-reasoning",
          "groq/meta-llama/llama-4-scout-17b-16e-instruct",
          "groq/meta-llama/llama-4-maverick-17b-128e-instruct",
          "groq/mistral-saba-24b",
          "groq/mixtral-8x7b-32768",
          "groq/gemma-7b-it",
          "groq/gemma2-9b-it",
          "groq/llama3-groq-70b-8192-tool-use-preview",
          "groq/llama3-groq-8b-8192-tool-use-preview",
          "groq/qwen-qwq-32b",
          "groq/playai-tts",
          "groq/whisper-large-v3",
          "groq/whisper-large-v3-turbo",
          "groq/distil-whisper-large-v3-en",
          "cerebras/llama3.1-8b",
          "cerebras/llama3.1-70b",
          "cerebras/llama-3.3-70b",
          "friendliai/meta-llama-3.1-8b-instruct",
          "friendliai/meta-llama-3.1-70b-instruct",
          "claude-instant-1.2",
          "claude-2",
          "claude-2.1",
          "claude-3-haiku-20240307",
          "claude-3-5-haiku-20241022",
          "claude-3-5-haiku-latest",
          "claude-3-opus-latest",
          "claude-3-opus-20240229",
          "claude-3-sonnet-20240229",
          "claude-3-5-sonnet-latest",
          "claude-3-5-sonnet-20240620",
          "claude-3-7-sonnet-latest",
          "claude-3-7-sonnet-20250219",
          "claude-3-5-sonnet-20241022",
          "text-bison",
          "text-bison@001",
          "text-bison@002",
          "text-bison32k",
          "text-bison32k@002",
          "text-unicorn",
          "text-unicorn@001",
          "chat-bison",
          "chat-bison@001",
          "chat-bison@002",
          "chat-bison-32k",
          "chat-bison-32k@002",
          "code-bison",
          "code-bison@001",
          "code-bison@002",
          "code-bison32k",
          "code-bison-32k@002",
          "code-gecko@001",
          "code-gecko@002",
          "code-gecko",
          "code-gecko-latest",
          "codechat-bison@latest",
          "codechat-bison",
          "codechat-bison@001",
          "codechat-bison@002",
          "codechat-bison-32k",
          "codechat-bison-32k@002",
          "meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8",
          "meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
          "meta_llama/Llama-3.3-70B-Instruct",
          "meta_llama/Llama-3.3-8B-Instruct",
          "gemini-pro",
          "gemini-1.0-pro",
          "gemini-1.0-pro-001",
          "gemini-1.0-ultra",
          "gemini-1.0-ultra-001",
          "gemini-1.0-pro-002",
          "gemini-1.5-pro",
          "gemini-1.5-pro-002",
          "gemini-1.5-pro-001",
          "gemini-1.5-pro-preview-0514",
          "gemini-1.5-pro-preview-0215",
          "gemini-1.5-pro-preview-0409",
          "gemini-1.5-flash",
          "gemini-1.5-flash-exp-0827",
          "gemini-1.5-flash-002",
          "gemini-1.5-flash-001",
          "gemini-1.5-flash-preview-0514",
          "gemini-pro-experimental",
          "gemini-flash-experimental",
          "gemini-pro-vision",
          "gemini-1.0-pro-vision",
          "gemini-1.0-pro-vision-001",
          "medlm-medium",
          "medlm-large",
          "gemini-2.5-pro-exp-03-25",
          "gemini-2.0-pro-exp-02-05",
          "gemini-2.0-flash-exp",
          "gemini-2.0-flash-001",
          "gemini-2.0-flash-thinking-exp",
          "gemini-2.0-flash-thinking-exp-01-21",
          "gemini/gemini-2.5-pro-exp-03-25",
          "gemini/gemini-2.5-flash-preview-tts",
          "gemini/gemini-2.5-flash-preview-05-20",
          "gemini/gemini-2.5-flash-preview-04-17",
          "gemini-2.5-flash-preview-05-20",
          "gemini-2.5-flash-preview-04-17",
          "gemini-2.0-flash",
          "gemini-2.0-flash-lite",
          "gemini-2.0-flash-lite-001",
          "gemini-2.5-pro-preview-05-06",
          "gemini-2.5-pro-preview-03-25",
          "gemini-2.0-flash-preview-image-generation",
          "gemini-2.5-pro-preview-tts",
          "gemini/gemini-2.0-pro-exp-02-05",
          "gemini/gemini-2.0-flash-preview-image-generation",
          "gemini/gemini-2.0-flash",
          "gemini/gemini-2.0-flash-lite",
          "gemini/gemini-2.0-flash-001",
          "gemini/gemini-2.5-pro-preview-tts",
          "gemini/gemini-2.5-pro-preview-05-06",
          "gemini/gemini-2.5-pro-preview-03-25",
          "gemini/gemini-2.0-flash-exp",
          "gemini/gemini-2.0-flash-lite-preview-02-05",
          "gemini/gemini-2.0-flash-thinking-exp",
          "gemini/gemini-2.0-flash-thinking-exp-01-21",
          "gemini/gemma-3-27b-it",
          "gemini/learnlm-1.5-pro-experimental",
          "vertex_ai/claude-3-sonnet",
          "vertex_ai/claude-3-sonnet@20240229",
          "vertex_ai/claude-3-5-sonnet",
          "vertex_ai/claude-3-5-sonnet@20240620",
          "vertex_ai/claude-3-5-sonnet-v2",
          "vertex_ai/claude-3-5-sonnet-v2@20241022",
          "vertex_ai/claude-3-7-sonnet@20250219",
          "vertex_ai/claude-3-haiku",
          "vertex_ai/claude-3-haiku@20240307",
          "vertex_ai/claude-3-5-haiku",
          "vertex_ai/claude-3-5-haiku@20241022",
          "vertex_ai/claude-3-opus",
          "vertex_ai/claude-3-opus@20240229",
          "vertex_ai/meta/llama3-405b-instruct-maas",
          "vertex_ai/meta/llama-4-scout-17b-16e-instruct-maas",
          "vertex_ai/meta/llama-4-scout-17b-128e-instruct-maas",
          "vertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas",
          "vertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas",
          "vertex_ai/meta/llama3-70b-instruct-maas",
          "vertex_ai/meta/llama3-8b-instruct-maas",
          "vertex_ai/meta/llama-3.2-90b-vision-instruct-maas",
          "vertex_ai/mistral-large@latest",
          "vertex_ai/mistral-large@2411-001",
          "vertex_ai/mistral-large-2411",
          "vertex_ai/mistral-large@2407",
          "vertex_ai/mistral-nemo@latest",
          "vertex_ai/mistral-small-2503@001",
          "vertex_ai/mistral-small-2503",
          "vertex_ai/jamba-1.5-mini@001",
          "vertex_ai/jamba-1.5-large@001",
          "vertex_ai/jamba-1.5",
          "vertex_ai/jamba-1.5-mini",
          "vertex_ai/jamba-1.5-large",
          "vertex_ai/mistral-nemo@2407",
          "vertex_ai/codestral@latest",
          "vertex_ai/codestral@2405",
          "vertex_ai/codestral-2501",
          "vertex_ai/imagegeneration@006",
          "vertex_ai/imagen-3.0-generate-002",
          "vertex_ai/imagen-3.0-generate-001",
          "vertex_ai/imagen-3.0-fast-generate-001",
          "text-embedding-004",
          "text-embedding-005",
          "text-multilingual-embedding-002",
          "multimodalembedding",
          "multimodalembedding@001",
          "text-embedding-large-exp-03-07",
          "textembedding-gecko",
          "textembedding-gecko-multilingual",
          "textembedding-gecko-multilingual@001",
          "textembedding-gecko@001",
          "textembedding-gecko@003",
          "text-embedding-preview-0409",
          "text-multilingual-embedding-preview-0409",
          "palm/chat-bison",
          "palm/chat-bison-001",
          "palm/text-bison",
          "palm/text-bison-001",
          "palm/text-bison-safety-off",
          "palm/text-bison-safety-recitation-off",
          "gemini/gemini-1.5-flash-002",
          "gemini/gemini-1.5-flash-001",
          "gemini/gemini-1.5-flash",
          "gemini/gemini-1.5-flash-latest",
          "gemini/gemini-1.5-flash-8b",
          "gemini/gemini-1.5-flash-8b-exp-0924",
          "gemini/gemini-exp-1114",
          "gemini/gemini-exp-1206",
          "gemini/gemini-1.5-flash-exp-0827",
          "gemini/gemini-1.5-flash-8b-exp-0827",
          "gemini/gemini-pro",
          "gemini/gemini-1.5-pro",
          "gemini/gemini-1.5-pro-002",
          "gemini/gemini-1.5-pro-001",
          "gemini/gemini-1.5-pro-exp-0801",
          "gemini/gemini-1.5-pro-exp-0827",
          "gemini/gemini-1.5-pro-latest",
          "gemini/gemini-pro-vision",
          "gemini/gemini-gemma-2-27b-it",
          "gemini/gemini-gemma-2-9b-it",
          "command-a-03-2025",
          "command-r",
          "command-r-08-2024",
          "command-r7b-12-2024",
          "command-light",
          "command-r-plus",
          "command-r-plus-08-2024",
          "command-nightly",
          "command",
          "rerank-v3.5",
          "rerank-english-v3.0",
          "rerank-multilingual-v3.0",
          "rerank-english-v2.0",
          "rerank-multilingual-v2.0",
          "embed-english-light-v3.0",
          "embed-multilingual-v3.0",
          "embed-english-v2.0",
          "embed-english-light-v2.0",
          "embed-multilingual-v2.0",
          "embed-english-v3.0",
          "replicate/meta/llama-2-13b",
          "replicate/meta/llama-2-13b-chat",
          "replicate/meta/llama-2-70b",
          "replicate/meta/llama-2-70b-chat",
          "replicate/meta/llama-2-7b",
          "replicate/meta/llama-2-7b-chat",
          "replicate/meta/llama-3-70b",
          "replicate/meta/llama-3-70b-instruct",
          "replicate/meta/llama-3-8b",
          "replicate/meta/llama-3-8b-instruct",
          "replicate/mistralai/mistral-7b-v0.1",
          "replicate/mistralai/mistral-7b-instruct-v0.2",
          "replicate/mistralai/mixtral-8x7b-instruct-v0.1",
          "openrouter/deepseek/deepseek-r1",
          "openrouter/deepseek/deepseek-chat",
          "openrouter/deepseek/deepseek-coder",
          "openrouter/microsoft/wizardlm-2-8x22b:nitro",
          "openrouter/google/gemini-pro-1.5",
          "openrouter/google/gemini-2.0-flash-001",
          "openrouter/mistralai/mixtral-8x22b-instruct",
          "openrouter/cohere/command-r-plus",
          "openrouter/databricks/dbrx-instruct",
          "openrouter/anthropic/claude-3-haiku",
          "openrouter/anthropic/claude-3-5-haiku",
          "openrouter/anthropic/claude-3-haiku-20240307",
          "openrouter/anthropic/claude-3-5-haiku-20241022",
          "openrouter/anthropic/claude-3.5-sonnet",
          "openrouter/anthropic/claude-3.5-sonnet:beta",
          "openrouter/anthropic/claude-3.7-sonnet",
          "openrouter/anthropic/claude-3.7-sonnet:beta",
          "openrouter/anthropic/claude-3-sonnet",
          "openrouter/mistralai/mistral-large",
          "mistralai/mistral-small-3.1-24b-instruct",
          "openrouter/cognitivecomputations/dolphin-mixtral-8x7b",
          "openrouter/google/gemini-pro-vision",
          "openrouter/fireworks/firellava-13b",
          "openrouter/meta-llama/llama-3-8b-instruct:free",
          "openrouter/meta-llama/llama-3-8b-instruct:extended",
          "openrouter/meta-llama/llama-3-70b-instruct:nitro",
          "openrouter/meta-llama/llama-3-70b-instruct",
          "openrouter/openai/o1",
          "openrouter/openai/o1-mini",
          "openrouter/openai/o1-mini-2024-09-12",
          "openrouter/openai/o1-preview",
          "openrouter/openai/o1-preview-2024-09-12",
          "openrouter/openai/o3-mini",
          "openrouter/openai/o3-mini-high",
          "openrouter/openai/gpt-4o",
          "openrouter/openai/gpt-4o-2024-05-13",
          "openrouter/openai/gpt-4-vision-preview",
          "openrouter/openai/gpt-3.5-turbo",
          "openrouter/openai/gpt-3.5-turbo-16k",
          "openrouter/openai/gpt-4",
          "openrouter/anthropic/claude-instant-v1",
          "openrouter/anthropic/claude-2",
          "openrouter/anthropic/claude-3-opus",
          "openrouter/google/palm-2-chat-bison",
          "openrouter/google/palm-2-codechat-bison",
          "openrouter/meta-llama/llama-2-13b-chat",
          "openrouter/meta-llama/llama-2-70b-chat",
          "openrouter/meta-llama/codellama-34b-instruct",
          "openrouter/nousresearch/nous-hermes-llama2-13b",
          "openrouter/mancer/weaver",
          "openrouter/gryphe/mythomax-l2-13b",
          "openrouter/jondurbin/airoboros-l2-70b-2.1",
          "openrouter/undi95/remm-slerp-l2-13b",
          "openrouter/pygmalionai/mythalion-13b",
          "openrouter/mistralai/mistral-7b-instruct",
          "openrouter/mistralai/mistral-7b-instruct:free",
          "openrouter/qwen/qwen-2.5-coder-32b-instruct",
          "j2-ultra",
          "jamba-1.5-mini@001",
          "jamba-1.5-large@001",
          "jamba-1.5",
          "jamba-1.5-mini",
          "jamba-1.5-large",
          "jamba-large-1.6",
          "jamba-mini-1.6",
          "j2-mid",
          "j2-light",
          "dolphin",
          "chatdolphin",
          "luminous-base",
          "luminous-base-control",
          "luminous-extended",
          "luminous-extended-control",
          "luminous-supreme",
          "luminous-supreme-control",
          "ai21.j2-mid-v1",
          "ai21.j2-ultra-v1",
          "ai21.jamba-instruct-v1:0",
          "ai21.jamba-1-5-large-v1:0",
          "ai21.jamba-1-5-mini-v1:0",
          "amazon.rerank-v1:0",
          "amazon.titan-text-lite-v1",
          "amazon.titan-text-express-v1",
          "amazon.titan-text-premier-v1:0",
          "amazon.titan-embed-text-v1",
          "amazon.titan-embed-text-v2:0",
          "amazon.titan-embed-image-v1",
          "mistral.mistral-7b-instruct-v0:2",
          "mistral.mixtral-8x7b-instruct-v0:1",
          "mistral.mistral-large-2402-v1:0",
          "mistral.mistral-large-2407-v1:0",
          "mistral.mistral-small-2402-v1:0",
          "bedrock/us-west-2/mistral.mixtral-8x7b-instruct-v0:1",
          "bedrock/us-east-1/mistral.mixtral-8x7b-instruct-v0:1",
          "bedrock/eu-west-3/mistral.mixtral-8x7b-instruct-v0:1",
          "bedrock/us-west-2/mistral.mistral-7b-instruct-v0:2",
          "bedrock/us-east-1/mistral.mistral-7b-instruct-v0:2",
          "bedrock/eu-west-3/mistral.mistral-7b-instruct-v0:2",
          "bedrock/us-east-1/mistral.mistral-large-2402-v1:0",
          "bedrock/us-west-2/mistral.mistral-large-2402-v1:0",
          "bedrock/eu-west-3/mistral.mistral-large-2402-v1:0",
          "amazon.nova-micro-v1:0",
          "us.amazon.nova-micro-v1:0",
          "eu.amazon.nova-micro-v1:0",
          "amazon.nova-lite-v1:0",
          "us.amazon.nova-lite-v1:0",
          "eu.amazon.nova-lite-v1:0",
          "amazon.nova-pro-v1:0",
          "us.amazon.nova-pro-v1:0",
          "1024-x-1024/50-steps/bedrock/amazon.nova-canvas-v1:0",
          "eu.amazon.nova-pro-v1:0",
          "us.amazon.nova-premier-v1:0",
          "anthropic.claude-3-sonnet-20240229-v1:0",
          "bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0",
          "anthropic.claude-3-5-sonnet-20240620-v1:0",
          "anthropic.claude-3-7-sonnet-20250219-v1:0",
          "anthropic.claude-3-5-sonnet-20241022-v2:0",
          "anthropic.claude-3-haiku-20240307-v1:0",
          "anthropic.claude-3-5-haiku-20241022-v1:0",
          "anthropic.claude-3-opus-20240229-v1:0",
          "us.anthropic.claude-3-sonnet-20240229-v1:0",
          "us.anthropic.claude-3-5-sonnet-20240620-v1:0",
          "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
          "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
          "us.anthropic.claude-3-haiku-20240307-v1:0",
          "us.anthropic.claude-3-5-haiku-20241022-v1:0",
          "us.anthropic.claude-3-opus-20240229-v1:0",
          "eu.anthropic.claude-3-sonnet-20240229-v1:0",
          "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
          "eu.anthropic.claude-3-5-sonnet-20241022-v2:0",
          "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
          "eu.anthropic.claude-3-haiku-20240307-v1:0",
          "eu.anthropic.claude-3-5-haiku-20241022-v1:0",
          "eu.anthropic.claude-3-opus-20240229-v1:0",
          "anthropic.claude-v1",
          "bedrock/us-east-1/anthropic.claude-v1",
          "bedrock/us-west-2/anthropic.claude-v1",
          "bedrock/ap-northeast-1/anthropic.claude-v1",
          "bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v1",
          "bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v1",
          "bedrock/eu-central-1/anthropic.claude-v1",
          "bedrock/eu-central-1/1-month-commitment/anthropic.claude-v1",
          "bedrock/eu-central-1/6-month-commitment/anthropic.claude-v1",
          "bedrock/us-east-1/1-month-commitment/anthropic.claude-v1",
          "bedrock/us-east-1/6-month-commitment/anthropic.claude-v1",
          "bedrock/us-west-2/1-month-commitment/anthropic.claude-v1",
          "bedrock/us-west-2/6-month-commitment/anthropic.claude-v1",
          "anthropic.claude-v2",
          "bedrock/us-east-1/anthropic.claude-v2",
          "bedrock/us-west-2/anthropic.claude-v2",
          "bedrock/ap-northeast-1/anthropic.claude-v2",
          "bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2",
          "bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2",
          "bedrock/eu-central-1/anthropic.claude-v2",
          "bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2",
          "bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2",
          "bedrock/us-east-1/1-month-commitment/anthropic.claude-v2",
          "bedrock/us-east-1/6-month-commitment/anthropic.claude-v2",
          "bedrock/us-west-2/1-month-commitment/anthropic.claude-v2",
          "bedrock/us-west-2/6-month-commitment/anthropic.claude-v2",
          "anthropic.claude-v2:1",
          "bedrock/us-east-1/anthropic.claude-v2:1",
          "bedrock/us-west-2/anthropic.claude-v2:1",
          "bedrock/ap-northeast-1/anthropic.claude-v2:1",
          "bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2:1",
          "bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2:1",
          "bedrock/eu-central-1/anthropic.claude-v2:1",
          "bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2:1",
          "bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2:1",
          "bedrock/us-east-1/1-month-commitment/anthropic.claude-v2:1",
          "bedrock/us-east-1/6-month-commitment/anthropic.claude-v2:1",
          "bedrock/us-west-2/1-month-commitment/anthropic.claude-v2:1",
          "bedrock/us-west-2/6-month-commitment/anthropic.claude-v2:1",
          "anthropic.claude-instant-v1",
          "bedrock/us-east-1/anthropic.claude-instant-v1",
          "bedrock/us-east-1/1-month-commitment/anthropic.claude-instant-v1",
          "bedrock/us-east-1/6-month-commitment/anthropic.claude-instant-v1",
          "bedrock/us-west-2/1-month-commitment/anthropic.claude-instant-v1",
          "bedrock/us-west-2/6-month-commitment/anthropic.claude-instant-v1",
          "bedrock/us-west-2/anthropic.claude-instant-v1",
          "bedrock/ap-northeast-1/anthropic.claude-instant-v1",
          "bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1",
          "bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1",
          "bedrock/eu-central-1/anthropic.claude-instant-v1",
          "bedrock/eu-central-1/1-month-commitment/anthropic.claude-instant-v1",
          "bedrock/eu-central-1/6-month-commitment/anthropic.claude-instant-v1",
          "cohere.rerank-v3-5:0",
          "cohere.command-text-v14",
          "bedrock/*/1-month-commitment/cohere.command-text-v14",
          "bedrock/*/6-month-commitment/cohere.command-text-v14",
          "cohere.command-light-text-v14",
          "bedrock/*/1-month-commitment/cohere.command-light-text-v14",
          "bedrock/*/6-month-commitment/cohere.command-light-text-v14",
          "cohere.command-r-plus-v1:0",
          "cohere.command-r-v1:0",
          "cohere.embed-english-v3",
          "cohere.embed-multilingual-v3",
          "us.deepseek.r1-v1:0",
          "meta.llama3-3-70b-instruct-v1:0",
          "meta.llama2-13b-chat-v1",
          "meta.llama2-70b-chat-v1",
          "meta.llama3-8b-instruct-v1:0",
          "bedrock/us-east-1/meta.llama3-8b-instruct-v1:0",
          "bedrock/us-west-1/meta.llama3-8b-instruct-v1:0",
          "bedrock/ap-south-1/meta.llama3-8b-instruct-v1:0",
          "bedrock/ca-central-1/meta.llama3-8b-instruct-v1:0",
          "bedrock/eu-west-1/meta.llama3-8b-instruct-v1:0",
          "bedrock/eu-west-2/meta.llama3-8b-instruct-v1:0",
          "bedrock/sa-east-1/meta.llama3-8b-instruct-v1:0",
          "meta.llama3-70b-instruct-v1:0",
          "bedrock/us-east-1/meta.llama3-70b-instruct-v1:0",
          "bedrock/us-west-1/meta.llama3-70b-instruct-v1:0",
          "bedrock/ap-south-1/meta.llama3-70b-instruct-v1:0",
          "bedrock/ca-central-1/meta.llama3-70b-instruct-v1:0",
          "bedrock/eu-west-1/meta.llama3-70b-instruct-v1:0",
          "bedrock/eu-west-2/meta.llama3-70b-instruct-v1:0",
          "bedrock/sa-east-1/meta.llama3-70b-instruct-v1:0",
          "meta.llama3-1-8b-instruct-v1:0",
          "us.meta.llama3-1-8b-instruct-v1:0",
          "meta.llama3-1-70b-instruct-v1:0",
          "us.meta.llama3-1-70b-instruct-v1:0",
          "meta.llama3-1-405b-instruct-v1:0",
          "us.meta.llama3-1-405b-instruct-v1:0",
          "meta.llama3-2-1b-instruct-v1:0",
          "us.meta.llama3-2-1b-instruct-v1:0",
          "eu.meta.llama3-2-1b-instruct-v1:0",
          "meta.llama3-2-3b-instruct-v1:0",
          "us.meta.llama3-2-3b-instruct-v1:0",
          "eu.meta.llama3-2-3b-instruct-v1:0",
          "meta.llama3-2-11b-instruct-v1:0",
          "us.meta.llama3-2-11b-instruct-v1:0",
          "meta.llama3-2-90b-instruct-v1:0",
          "us.meta.llama3-2-90b-instruct-v1:0",
          "us.meta.llama3-3-70b-instruct-v1:0",
          "meta.llama4-maverick-17b-instruct-v1:0",
          "us.meta.llama4-maverick-17b-instruct-v1:0",
          "meta.llama4-scout-17b-instruct-v1:0",
          "us.meta.llama4-scout-17b-instruct-v1:0",
          "512-x-512/50-steps/stability.stable-diffusion-xl-v0",
          "512-x-512/max-steps/stability.stable-diffusion-xl-v0",
          "max-x-max/50-steps/stability.stable-diffusion-xl-v0",
          "max-x-max/max-steps/stability.stable-diffusion-xl-v0",
          "1024-x-1024/50-steps/stability.stable-diffusion-xl-v1",
          "1024-x-1024/max-steps/stability.stable-diffusion-xl-v1",
          "stability.sd3-large-v1:0",
          "stability.sd3-5-large-v1:0",
          "stability.stable-image-core-v1:0",
          "stability.stable-image-core-v1:1",
          "stability.stable-image-ultra-v1:0",
          "stability.stable-image-ultra-v1:1",
          "sagemaker/meta-textgeneration-llama-2-7b",
          "sagemaker/meta-textgeneration-llama-2-7b-f",
          "sagemaker/meta-textgeneration-llama-2-13b",
          "sagemaker/meta-textgeneration-llama-2-13b-f",
          "sagemaker/meta-textgeneration-llama-2-70b",
          "sagemaker/meta-textgeneration-llama-2-70b-b-f",
          "together-ai-up-to-4b",
          "together-ai-4.1b-8b",
          "together-ai-8.1b-21b",
          "together-ai-21.1b-41b",
          "together-ai-41.1b-80b",
          "together-ai-81.1b-110b",
          "together-ai-embedding-up-to-150m",
          "together-ai-embedding-151m-to-350m",
          "together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
          "together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
          "together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
          "together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo",
          "together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
          "together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1",
          "together_ai/mistralai/Mistral-7B-Instruct-v0.1",
          "together_ai/togethercomputer/CodeLlama-34b-Instruct",
          "together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
          "together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct",
          "together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo",
          "together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo",
          "together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo",
          "together_ai/deepseek-ai/DeepSeek-V3",
          "together_ai/mistralai/Mistral-Small-24B-Instruct-2501",
          "ollama/codegemma",
          "ollama/codegeex4",
          "ollama/deepseek-coder-v2-instruct",
          "ollama/deepseek-coder-v2-base",
          "ollama/deepseek-coder-v2-lite-instruct",
          "ollama/deepseek-coder-v2-lite-base",
          "ollama/internlm2_5-20b-chat",
          "ollama/llama2",
          "ollama/llama2:7b",
          "ollama/llama2:13b",
          "ollama/llama2:70b",
          "ollama/llama2-uncensored",
          "ollama/llama3",
          "ollama/llama3:8b",
          "ollama/llama3:70b",
          "ollama/llama3.1",
          "ollama/mistral-large-instruct-2407",
          "ollama/mistral",
          "ollama/mistral-7B-Instruct-v0.1",
          "ollama/mistral-7B-Instruct-v0.2",
          "ollama/mixtral-8x7B-Instruct-v0.1",
          "ollama/mixtral-8x22B-Instruct-v0.1",
          "ollama/codellama",
          "ollama/orca-mini",
          "ollama/vicuna",
          "deepinfra/lizpreciatior/lzlv_70b_fp16_hf",
          "deepinfra/Gryphe/MythoMax-L2-13b",
          "deepinfra/mistralai/Mistral-7B-Instruct-v0.1",
          "deepinfra/meta-llama/Llama-2-70b-chat-hf",
          "deepinfra/cognitivecomputations/dolphin-2.6-mixtral-8x7b",
          "deepinfra/codellama/CodeLlama-34b-Instruct-hf",
          "deepinfra/deepinfra/mixtral",
          "deepinfra/Phind/Phind-CodeLlama-34B-v2",
          "deepinfra/mistralai/Mixtral-8x7B-Instruct-v0.1",
          "deepinfra/deepinfra/airoboros-70b",
          "deepinfra/01-ai/Yi-34B-Chat",
          "deepinfra/01-ai/Yi-6B-200K",
          "deepinfra/jondurbin/airoboros-l2-70b-gpt4-1.4.1",
          "deepinfra/meta-llama/Llama-2-13b-chat-hf",
          "deepinfra/amazon/MistralLite",
          "deepinfra/meta-llama/Llama-2-7b-chat-hf",
          "deepinfra/meta-llama/Meta-Llama-3-8B-Instruct",
          "deepinfra/meta-llama/Meta-Llama-3-70B-Instruct",
          "deepinfra/meta-llama/Meta-Llama-3.1-405B-Instruct",
          "deepinfra/01-ai/Yi-34B-200K",
          "deepinfra/openchat/openchat_3.5",
          "perplexity/codellama-34b-instruct",
          "perplexity/codellama-70b-instruct",
          "perplexity/llama-3.1-70b-instruct",
          "perplexity/llama-3.1-8b-instruct",
          "perplexity/llama-3.1-sonar-huge-128k-online",
          "perplexity/llama-3.1-sonar-large-128k-online",
          "perplexity/llama-3.1-sonar-large-128k-chat",
          "perplexity/llama-3.1-sonar-small-128k-chat",
          "perplexity/llama-3.1-sonar-small-128k-online",
          "perplexity/pplx-7b-chat",
          "perplexity/pplx-70b-chat",
          "perplexity/pplx-7b-online",
          "perplexity/pplx-70b-online",
          "perplexity/llama-2-70b-chat",
          "perplexity/mistral-7b-instruct",
          "perplexity/mixtral-8x7b-instruct",
          "perplexity/sonar-small-chat",
          "perplexity/sonar-small-online",
          "perplexity/sonar-medium-chat",
          "perplexity/sonar-medium-online",
          "perplexity/sonar",
          "perplexity/sonar-pro",
          "perplexity/sonar-reasoning",
          "perplexity/sonar-reasoning-pro",
          "perplexity/sonar-deep-research",
          "fireworks_ai/accounts/fireworks/models/llama-v3p2-1b-instruct",
          "fireworks_ai/accounts/fireworks/models/llama-v3p2-3b-instruct",
          "fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct",
          "fireworks_ai/accounts/fireworks/models/llama-v3p2-11b-vision-instruct",
          "accounts/fireworks/models/llama-v3p2-90b-vision-instruct",
          "fireworks_ai/accounts/fireworks/models/firefunction-v2",
          "fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct-hf",
          "fireworks_ai/accounts/fireworks/models/qwen2-72b-instruct",
          "fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct",
          "fireworks_ai/accounts/fireworks/models/yi-large",
          "fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-instruct",
          "fireworks_ai/accounts/fireworks/models/deepseek-v3",
          "fireworks_ai/accounts/fireworks/models/deepseek-r1",
          "fireworks_ai/accounts/fireworks/models/deepseek-r1-basic",
          "fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct",
          "fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic",
          "fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic",
          "fireworks_ai/nomic-ai/nomic-embed-text-v1.5",
          "fireworks_ai/nomic-ai/nomic-embed-text-v1",
          "fireworks_ai/WhereIsAI/UAE-Large-V1",
          "fireworks_ai/thenlper/gte-large",
          "fireworks_ai/thenlper/gte-base",
          "fireworks-ai-up-to-4b",
          "fireworks-ai-4.1b-to-16b",
          "fireworks-ai-above-16b",
          "fireworks-ai-moe-up-to-56b",
          "fireworks-ai-56b-to-176b",
          "fireworks-ai-default",
          "fireworks-ai-embedding-up-to-150m",
          "fireworks-ai-embedding-150m-to-350m",
          "anyscale/mistralai/Mistral-7B-Instruct-v0.1",
          "anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1",
          "anyscale/mistralai/Mixtral-8x22B-Instruct-v0.1",
          "anyscale/HuggingFaceH4/zephyr-7b-beta",
          "anyscale/google/gemma-7b-it",
          "anyscale/meta-llama/Llama-2-7b-chat-hf",
          "anyscale/meta-llama/Llama-2-13b-chat-hf",
          "anyscale/meta-llama/Llama-2-70b-chat-hf",
          "anyscale/codellama/CodeLlama-34b-Instruct-hf",
          "anyscale/codellama/CodeLlama-70b-Instruct-hf",
          "anyscale/meta-llama/Meta-Llama-3-8B-Instruct",
          "anyscale/meta-llama/Meta-Llama-3-70B-Instruct",
          "cloudflare/@cf/meta/llama-2-7b-chat-fp16",
          "cloudflare/@cf/meta/llama-2-7b-chat-int8",
          "cloudflare/@cf/mistral/mistral-7b-instruct-v0.1",
          "cloudflare/@hf/thebloke/codellama-7b-instruct-awq",
          "voyage/voyage-01",
          "voyage/voyage-lite-01",
          "voyage/voyage-large-2",
          "voyage/voyage-finance-2",
          "voyage/voyage-lite-02-instruct",
          "voyage/voyage-law-2",
          "voyage/voyage-code-2",
          "voyage/voyage-2",
          "voyage/voyage-3-large",
          "voyage/voyage-3",
          "voyage/voyage-3-lite",
          "voyage/voyage-code-3",
          "voyage/voyage-multimodal-3",
          "voyage/rerank-2",
          "voyage/rerank-2-lite",
          "databricks/databricks-claude-3-7-sonnet",
          "databricks/databricks-meta-llama-3-1-405b-instruct",
          "databricks/databricks-meta-llama-3-1-70b-instruct",
          "databricks/databricks-meta-llama-3-3-70b-instruct",
          "databricks/databricks-dbrx-instruct",
          "databricks/databricks-meta-llama-3-70b-instruct",
          "databricks/databricks-llama-2-70b-chat",
          "databricks/databricks-mixtral-8x7b-instruct",
          "databricks/databricks-mpt-30b-instruct",
          "databricks/databricks-mpt-7b-instruct",
          "databricks/databricks-bge-large-en",
          "databricks/databricks-gte-large-en",
          "sambanova/Meta-Llama-3.1-8B-Instruct",
          "sambanova/Meta-Llama-3.1-405B-Instruct",
          "sambanova/Meta-Llama-3.2-1B-Instruct",
          "sambanova/Meta-Llama-3.2-3B-Instruct",
          "sambanova/Llama-4-Maverick-17B-128E-Instruct",
          "sambanova/Llama-4-Scout-17B-16E-Instruct",
          "sambanova/Meta-Llama-3.3-70B-Instruct",
          "sambanova/Meta-Llama-Guard-3-8B",
          "sambanova/Qwen3-32B",
          "sambanova/QwQ-32B",
          "sambanova/Qwen2-Audio-7B-Instruct",
          "sambanova/DeepSeek-R1-Distill-Llama-70B",
          "sambanova/DeepSeek-R1",
          "sambanova/DeepSeek-V3-0324",
          "assemblyai/nano",
          "assemblyai/best",
          "jina-reranker-v2-base-multilingual",
          "snowflake/deepseek-r1",
          "snowflake/snowflake-arctic",
          "snowflake/claude-3-5-sonnet",
          "snowflake/mistral-large",
          "snowflake/mistral-large2",
          "snowflake/reka-flash",
          "snowflake/reka-core",
          "snowflake/jamba-instruct",
          "snowflake/jamba-1.5-mini",
          "snowflake/jamba-1.5-large",
          "snowflake/mixtral-8x7b",
          "snowflake/llama2-70b-chat",
          "snowflake/llama3-8b",
          "snowflake/llama3-70b",
          "snowflake/llama3.1-8b",
          "snowflake/llama3.1-70b",
          "snowflake/llama3.3-70b",
          "snowflake/snowflake-llama-3.3-70b",
          "snowflake/llama3.1-405b",
          "snowflake/snowflake-llama-3.1-405b",
          "snowflake/llama3.2-1b",
          "snowflake/llama3.2-3b",
          "snowflake/mistral-7b",
          "snowflake/gemma-7b",
          "nscale/meta-llama/Llama-4-Scout-17B-16E-Instruct",
          "nscale/Qwen/Qwen2.5-Coder-3B-Instruct",
          "nscale/Qwen/Qwen2.5-Coder-7B-Instruct",
          "nscale/Qwen/Qwen2.5-Coder-32B-Instruct",
          "nscale/Qwen/QwQ-32B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
          "nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
          "nscale/mistralai/mixtral-8x22b-instruct-v0.1",
          "nscale/meta-llama/Llama-3.1-8B-Instruct",
          "nscale/meta-llama/Llama-3.3-70B-Instruct",
          "nscale/black-forest-labs/FLUX.1-schnell",
          "nscale/stabilityai/stable-diffusion-xl-base-1.0",
          "featherless_ai/featherless-ai/Qwerky-72B",
          "featherless_ai/featherless-ai/Qwerky-QwQ-32B"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gpt-4o-mini",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "user_message": {
        "tool_mode": true,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "user_message",
        "value": "",
        "display_name": "User Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "User message to pass to the run.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Manages Assistant Interactions",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Astra Assistant Agent",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "assistant_response",
        "display_name": "Assistant Response",
        "method": "get_assistant_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "tool_output",
        "hidden": true,
        "display_name": "Tool output",
        "method": "get_tool_output",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "output_thread_id",
        "hidden": true,
        "display_name": "Thread Id",
        "method": "get_thread_id",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "output_assistant_id",
        "hidden": true,
        "display_name": "Assistant Id",
        "method": "get_assistant_id",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "output_vs_id",
        "hidden": true,
        "display_name": "Vector Store Id",
        "method": "get_vs_id",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model_name",
      "instructions",
      "input_tools",
      "user_message",
      "file",
      "input_thread_id",
      "input_assistant_id",
      "env_set"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "Dotenv": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import io\n\nfrom dotenv import load_dotenv\n\nfrom langflow.custom import Component\nfrom langflow.inputs import MultilineSecretInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass Dotenv(Component):\n    display_name = \"Dotenv\"\n    description = \"Load .env file into env vars\"\n    icon = \"AstraDB\"\n    inputs = [\n        MultilineSecretInput(\n            name=\"dotenv_file_content\",\n            display_name=\"Dotenv file content\",\n            info=\"Paste the content of your .env file directly, since contents are sensitive, \"\n            \"using a Global variable set as 'password' is recommended\",\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"env_set\", name=\"env_set\", method=\"process_inputs\"),\n    ]\n\n    def process_inputs(self) -> Message:\n        fake_file = io.StringIO(self.dotenv_file_content)\n        result = load_dotenv(stream=fake_file, override=True)\n\n        message = Message(text=\"No variables found in .env\")\n        if result:\n            message = Message(text=\"Loaded .env\")\n        return message\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "dotenv_file_content": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "dotenv_file_content",
        "value": "",
        "display_name": "Dotenv file content",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Paste the content of your .env file directly, since contents are sensitive, using a Global variable set as 'password' is recommended",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "MultilineSecretInput"
      }
    },
    "description": "Load .env file into env vars",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Dotenv",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "env_set",
        "display_name": "env_set",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "dotenv_file_content"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AssistantsListAssistants": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.schema.message import Message\nfrom langflow.template.field.base import Output\n\n\nclass AssistantsListAssistants(ComponentWithCache):\n    display_name = \"List Assistants\"\n    description = \"Returns a list of assistant id's\"\n    icon = \"AstraDB\"\n    outputs = [\n        Output(display_name=\"Assistants\", name=\"assistants\", method=\"process_inputs\"),\n    ]\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.client = get_patched_openai_client(self._shared_component_cache)\n\n    def process_inputs(self) -> Message:\n        assistants = self.client.beta.assistants.list().data\n        id_list = [assistant.id for assistant in assistants]\n        return Message(\n            # get text from list\n            text=\"\\n\".join(id_list)\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      }
    },
    "description": "Returns a list of assistant id's",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "List Assistants",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "assistants",
        "display_name": "Assistants",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AssistantsGetAssistantName": {
    "template": {
      "_type": "Component",
      "assistant_id": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "assistant_id",
        "value": "",
        "display_name": "Assistant ID",
        "advanced": false,
        "dynamic": false,
        "info": "ID of the assistant",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput, StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass AssistantsGetAssistantName(ComponentWithCache):\n    display_name = \"Get Assistant name\"\n    description = \"Assistant by id\"\n    icon = \"AstraDB\"\n    inputs = [\n        StrInput(\n            name=\"assistant_id\",\n            display_name=\"Assistant ID\",\n            info=\"ID of the assistant\",\n        ),\n        MultilineInput(\n            name=\"env_set\",\n            display_name=\"Environment Set\",\n            info=\"Dummy input to allow chaining with Dotenv Component.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Assistant Name\", name=\"assistant_name\", method=\"process_inputs\"),\n    ]\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.client = get_patched_openai_client(self._shared_component_cache)\n\n    def process_inputs(self) -> Message:\n        assistant = self.client.beta.assistants.retrieve(\n            assistant_id=self.assistant_id,\n        )\n        return Message(text=assistant.name)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_set": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_set",
        "value": "",
        "display_name": "Environment Set",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Dummy input to allow chaining with Dotenv Component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Assistant by id",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Get Assistant name",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "assistant_name",
        "display_name": "Assistant Name",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "assistant_id",
      "env_set"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "GetEnvVar": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import os\n\nfrom langflow.custom import Component\nfrom langflow.inputs import StrInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass GetEnvVar(Component):\n    display_name = \"Get env var\"\n    description = \"Get env var\"\n    icon = \"AstraDB\"\n\n    inputs = [\n        StrInput(\n            name=\"env_var_name\",\n            display_name=\"Env var name\",\n            info=\"Name of the environment variable to get\",\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Env var value\", name=\"env_var_value\", method=\"process_inputs\"),\n    ]\n\n    def process_inputs(self) -> Message:\n        if self.env_var_name not in os.environ:\n            msg = f\"Environment variable {self.env_var_name} not set\"\n            raise ValueError(msg)\n        return Message(text=os.environ[self.env_var_name])\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_var_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_var_name",
        "value": "",
        "display_name": "Env var name",
        "advanced": false,
        "dynamic": false,
        "info": "Name of the environment variable to get",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      }
    },
    "description": "Get env var",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Get env var",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "env_var_value",
        "display_name": "Env var value",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "env_var_name"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AssistantsCreateThread": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.base.astra_assistants.util import get_patched_openai_client\nfrom langflow.custom.custom_component.component_with_cache import ComponentWithCache\nfrom langflow.inputs import MultilineInput\nfrom langflow.schema.message import Message\nfrom langflow.template import Output\n\n\nclass AssistantsCreateThread(ComponentWithCache):\n    display_name = \"Create Assistant Thread\"\n    description = \"Creates a thread and returns the thread id\"\n    icon = \"AstraDB\"\n    inputs = [\n        MultilineInput(\n            name=\"env_set\",\n            display_name=\"Environment Set\",\n            info=\"Dummy input to allow chaining with Dotenv Component.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Thread ID\", name=\"thread_id\", method=\"process_inputs\"),\n    ]\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.client = get_patched_openai_client(self._shared_component_cache)\n\n    def process_inputs(self) -> Message:\n        thread = self.client.beta.threads.create()\n        thread_id = thread.id\n\n        return Message(text=thread_id)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "env_set": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "env_set",
        "value": "",
        "display_name": "Environment Set",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Dummy input to allow chaining with Dotenv Component.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      }
    },
    "description": "Creates a thread and returns the thread id",
    "icon": "AstraDB",
    "base_classes": [
      "Message"
    ],
    "display_name": "Create Assistant Thread",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "thread_id",
        "display_name": "Thread ID",
        "method": "process_inputs",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "env_set"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  }
}