{
  "VertexAiModel": {
    "template": {
      "_type": "Component",
      "credentials": {
        "trace_as_metadata": true,
        "file_path": "",
        "fileTypes": [
          "json"
        ],
        "temp_file": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "credentials",
        "value": "",
        "display_name": "Credentials",
        "advanced": false,
        "dynamic": false,
        "info": "JSON credentials file. Leave empty to fallback to environment variables",
        "title_case": false,
        "type": "file",
        "_input_type": "FileInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import cast\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import BoolInput, FileInput, FloatInput, IntInput, StrInput\n\n\nclass ChatVertexAIComponent(LCModelComponent):\n    display_name = \"Vertex AI\"\n    description = \"Generate text using Vertex AI LLMs.\"\n    icon = \"VertexAI\"\n    name = \"VertexAiModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        FileInput(\n            name=\"credentials\",\n            display_name=\"Credentials\",\n            info=\"JSON credentials file. Leave empty to fallback to environment variables\",\n            file_types=[\"json\"],\n        ),\n        MessageTextInput(name=\"model_name\", display_name=\"Model Name\", value=\"gemini-1.5-pro\"),\n        StrInput(name=\"project\", display_name=\"Project\", info=\"The project ID.\", advanced=True),\n        StrInput(name=\"location\", display_name=\"Location\", value=\"us-central1\", advanced=True),\n        IntInput(name=\"max_output_tokens\", display_name=\"Max Output Tokens\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=1, advanced=True),\n        FloatInput(name=\"temperature\", value=0.0, display_name=\"Temperature\"),\n        IntInput(name=\"top_k\", display_name=\"Top K\", advanced=True),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", value=0.95, advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", value=False, advanced=True),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        try:\n            from langchain_google_vertexai import ChatVertexAI\n        except ImportError as e:\n            msg = \"Please install the langchain-google-vertexai package to use the VertexAIEmbeddings component.\"\n            raise ImportError(msg) from e\n        location = self.location or None\n        if self.credentials:\n            from google.cloud import aiplatform\n            from google.oauth2 import service_account\n\n            credentials = service_account.Credentials.from_service_account_file(self.credentials)\n            project = self.project or credentials.project_id\n            # ChatVertexAI sometimes skip manual credentials initialization\n            aiplatform.init(\n                project=project,\n                location=location,\n                credentials=credentials,\n            )\n        else:\n            project = self.project or None\n            credentials = None\n\n        return cast(\n            \"LanguageModel\",\n            ChatVertexAI(\n                credentials=credentials,\n                location=location,\n                project=project,\n                max_output_tokens=self.max_output_tokens or None,\n                max_retries=self.max_retries,\n                model_name=self.model_name,\n                temperature=self.temperature,\n                top_k=self.top_k or None,\n                top_p=self.top_p,\n                verbose=self.verbose,\n            ),\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "location": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "location",
        "value": "us-central1",
        "display_name": "Location",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "max_output_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_output_tokens",
        "value": "",
        "display_name": "Max Output Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 1,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gemini-1.5-pro",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "project": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "project",
        "value": "",
        "display_name": "Project",
        "advanced": true,
        "dynamic": false,
        "info": "The project ID.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 0.95,
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "verbose": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "verbose",
        "value": false,
        "display_name": "Verbose",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate text using Vertex AI LLMs.",
    "icon": "VertexAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Vertex AI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "credentials",
      "model_name",
      "project",
      "location",
      "max_output_tokens",
      "max_retries",
      "temperature",
      "top_k",
      "top_p",
      "verbose"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "BaiduQianfanChatModel": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing.constants import LanguageModel\nfrom langflow.io import DropdownInput, FloatInput, MessageTextInput, SecretStrInput\n\n\nclass QianfanChatEndpointComponent(LCModelComponent):\n    display_name: str = \"Qianfan\"\n    description: str = \"Generate text using Baidu Qianfan LLMs.\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint\"\n    icon = \"BaiduQianfan\"\n    name = \"BaiduQianfanChatModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            options=[\n                \"EB-turbo-AppBuilder\",\n                \"Llama-2-70b-chat\",\n                \"ERNIE-Bot-turbo-AI\",\n                \"ERNIE-Lite-8K-0308\",\n                \"ERNIE-Speed\",\n                \"Qianfan-Chinese-Llama-2-13B\",\n                \"ERNIE-3.5-8K\",\n                \"BLOOMZ-7B\",\n                \"Qianfan-Chinese-Llama-2-7B\",\n                \"XuanYuan-70B-Chat-4bit\",\n                \"AquilaChat-7B\",\n                \"ERNIE-Bot-4\",\n                \"Llama-2-13b-chat\",\n                \"ChatGLM2-6B-32K\",\n                \"ERNIE-Bot\",\n                \"ERNIE-Speed-128k\",\n                \"ERNIE-4.0-8K\",\n                \"Qianfan-BLOOMZ-7B-compressed\",\n                \"ERNIE Speed\",\n                \"Llama-2-7b-chat\",\n                \"Mixtral-8x7B-Instruct\",\n                \"ERNIE 3.5\",\n                \"ERNIE Speed-AppBuilder\",\n                \"ERNIE-Speed-8K\",\n                \"Yi-34B-Chat\",\n            ],\n            info=\"https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint\",\n            value=\"ERNIE-4.0-8K\",\n        ),\n        SecretStrInput(\n            name=\"qianfan_ak\",\n            display_name=\"Qianfan Ak\",\n            info=\"which you could get from  https://cloud.baidu.com/product/wenxinworkshop\",\n        ),\n        SecretStrInput(\n            name=\"qianfan_sk\",\n            display_name=\"Qianfan Sk\",\n            info=\"which you could get from  https://cloud.baidu.com/product/wenxinworkshop\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top p\",\n            info=\"Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\",\n            value=0.8,\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\",\n            value=0.95,\n        ),\n        FloatInput(\n            name=\"penalty_score\",\n            display_name=\"Penalty Score\",\n            info=\"Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo\",\n            value=1.0,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"endpoint\", display_name=\"Endpoint\", info=\"Endpoint of the Qianfan LLM, required if custom model used.\"\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        model = self.model\n        qianfan_ak = self.qianfan_ak\n        qianfan_sk = self.qianfan_sk\n        top_p = self.top_p\n        temperature = self.temperature\n        penalty_score = self.penalty_score\n        endpoint = self.endpoint\n\n        try:\n            kwargs = {\n                \"model\": model,\n                \"qianfan_ak\": qianfan_ak or None,\n                \"qianfan_sk\": qianfan_sk or None,\n                \"top_p\": top_p,\n                \"temperature\": temperature,\n                \"penalty_score\": penalty_score,\n            }\n\n            if endpoint:  # Only add endpoint if it has a value\n                kwargs[\"endpoint\"] = endpoint\n\n            output = QianfanChatEndpoint(**kwargs)\n\n        except Exception as e:\n            msg = \"Could not connect to Baidu Qianfan API.\"\n            raise ValueError(msg) from e\n\n        return output\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "endpoint": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "endpoint",
        "value": "",
        "display_name": "Endpoint",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Endpoint of the Qianfan LLM, required if custom model used.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "EB-turbo-AppBuilder",
          "Llama-2-70b-chat",
          "ERNIE-Bot-turbo-AI",
          "ERNIE-Lite-8K-0308",
          "ERNIE-Speed",
          "Qianfan-Chinese-Llama-2-13B",
          "ERNIE-3.5-8K",
          "BLOOMZ-7B",
          "Qianfan-Chinese-Llama-2-7B",
          "XuanYuan-70B-Chat-4bit",
          "AquilaChat-7B",
          "ERNIE-Bot-4",
          "Llama-2-13b-chat",
          "ChatGLM2-6B-32K",
          "ERNIE-Bot",
          "ERNIE-Speed-128k",
          "ERNIE-4.0-8K",
          "Qianfan-BLOOMZ-7B-compressed",
          "ERNIE Speed",
          "Llama-2-7b-chat",
          "Mixtral-8x7B-Instruct",
          "ERNIE 3.5",
          "ERNIE Speed-AppBuilder",
          "ERNIE-Speed-8K",
          "Yi-34B-Chat"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "ERNIE-4.0-8K",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "penalty_score": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "penalty_score",
        "value": 1,
        "display_name": "Penalty Score",
        "advanced": true,
        "dynamic": false,
        "info": "Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "qianfan_ak": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "qianfan_ak",
        "value": "",
        "display_name": "Qianfan Ak",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "which you could get from  https://cloud.baidu.com/product/wenxinworkshop",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "qianfan_sk": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "qianfan_sk",
        "value": "",
        "display_name": "Qianfan Sk",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "which you could get from  https://cloud.baidu.com/product/wenxinworkshop",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.95,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 0.8,
        "display_name": "Top p",
        "advanced": true,
        "dynamic": false,
        "info": "Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate text using Baidu Qianfan LLMs.",
    "icon": "BaiduQianfan",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Qianfan",
    "documentation": "https://python.langchain.com/docs/integrations/chat/baidu_qianfan_endpoint",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "model",
      "qianfan_ak",
      "qianfan_sk",
      "top_p",
      "temperature",
      "penalty_score",
      "endpoint"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "LanguageModelComponent": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "OpenAI API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Model Provider API key",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import ChatOpenAI\n\nfrom langflow.base.models.anthropic_constants import ANTHROPIC_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\n\n\nclass LanguageModelComponent(LCModelComponent):\n    display_name = \"Language Model\"\n    description = \"Runs a language model given a specified provider. \"\n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Model Provider\",\n            options=[\"OpenAI\", \"Anthropic\"],\n            value=\"OpenAI\",\n            info=\"Select the model provider\",\n            real_time_refresh=True,\n            options_metadata=[{\"icon\": \"OpenAI\"}, {\"icon\": \"Anthropic\"}],\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n            info=\"Select the model to use\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider API key\",\n            required=False,\n            show=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n            info=\"The input text to send to the model\",\n        ),\n        MessageTextInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"A system message that helps set the behavior of the assistant\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=\"Whether to stream the response\",\n            value=False,\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Controls randomness in responses\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        provider = self.provider\n        model_name = self.model_name\n        temperature = self.temperature\n        stream = self.stream\n\n        if provider == \"OpenAI\":\n            if not self.api_key:\n                msg = \"OpenAI API key is required when using OpenAI provider\"\n                raise ValueError(msg)\n            return ChatOpenAI(\n                model_name=model_name,\n                temperature=temperature,\n                streaming=stream,\n                openai_api_key=self.api_key,\n            )\n        if provider == \"Anthropic\":\n            if not self.api_key:\n                msg = \"Anthropic API key is required when using Anthropic provider\"\n                raise ValueError(msg)\n            return ChatAnthropic(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                anthropic_api_key=self.api_key,\n            )\n        msg = f\"Unknown provider: {provider}\"\n        raise ValueError(msg)\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None) -> dotdict:\n        if field_name == \"provider\":\n            if field_value == \"OpenAI\":\n                build_config[\"model_name\"][\"options\"] = OPENAI_MODEL_NAMES\n                build_config[\"model_name\"][\"value\"] = OPENAI_MODEL_NAMES[0]\n                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API Key\"\n            elif field_value == \"Anthropic\":\n                build_config[\"model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Anthropic API Key\"\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The input text to send to the model",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "gpt-4o-mini",
          "gpt-4o",
          "gpt-4.1",
          "gpt-4.1-mini",
          "gpt-4.1-nano",
          "gpt-4.5-preview",
          "gpt-4-turbo",
          "gpt-4-turbo-preview",
          "gpt-4",
          "gpt-3.5-turbo"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gpt-4o-mini",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "Select the model to use",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "provider": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "OpenAI",
          "Anthropic"
        ],
        "options_metadata": [
          {
            "icon": "OpenAI"
          },
          {
            "icon": "Anthropic"
          }
        ],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "provider",
        "value": "OpenAI",
        "display_name": "Model Provider",
        "advanced": false,
        "dynamic": false,
        "info": "Select the model provider",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Whether to stream the response",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "A system message that helps set the behavior of the assistant",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness in responses",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Runs a language model given a specified provider. ",
    "icon": "brain-circuit",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Language Model",
    "priority": 0,
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "provider",
      "model_name",
      "api_key",
      "input_value",
      "system_message",
      "stream",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "Maritalk": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "Maritalk API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Maritalk API Key to use for the OpenAI model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_community.chat_models import ChatMaritalk\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\n\n\nclass MaritalkModelComponent(LCModelComponent):\n    display_name = \"Maritalk\"\n    description = \"Generates text using Maritalk LLMs.\"\n    icon = \"Maritalk\"\n    name = \"Maritalk\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=512,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\"sabia-2-small\", \"sabia-2-medium\"],\n            value=[\"sabia-2-small\"],\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Maritalk API Key\",\n            info=\"The Maritalk API Key to use for the OpenAI model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=1)),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n\n        return ChatMaritalk(\n            max_tokens=max_tokens,\n            model=model_name,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": 512,
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "sabia-2-small",
          "sabia-2-medium"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": [
          "sabia-2-small"
        ],
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generates text using Maritalk LLMs.",
    "icon": "Maritalk",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Maritalk",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_name",
      "api_key",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "NVIDIAModelComponent": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "NVIDIA_API_KEY",
        "display_name": "NVIDIA API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The NVIDIA API Key.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "https://integrate.api.nvidia.com/v1",
        "display_name": "NVIDIA Base URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom loguru import logger\nfrom requests.exceptions import ConnectionError  # noqa: A004\nfrom urllib3.exceptions import MaxRetryError, NameResolutionError\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\n\n\nclass NVIDIAModelComponent(LCModelComponent):\n    display_name = \"NVIDIA\"\n    description = \"Generates text using NVIDIA LLMs.\"\n    icon = \"NVIDIA\"\n\n    try:\n        from langchain_nvidia_ai_endpoints import ChatNVIDIA\n\n        all_models = ChatNVIDIA().get_available_models()\n    except ImportError as e:\n        msg = \"Please install langchain-nvidia-ai-endpoints to use the NVIDIA model.\"\n        raise ImportError(msg) from e\n    except (ConnectionError, MaxRetryError, NameResolutionError):\n        logger.warning(\n            \"Failed to connect to NVIDIA API. Model list may be unavailable.\"\n            \" Please check your internet connection and API credentials.\"\n        )\n        all_models = []\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            info=\"The name of the NVIDIA model to use.\",\n            advanced=False,\n            value=None,\n            options=[model.id for model in all_models],\n            combobox=True,\n            refresh_button=True,\n        ),\n        BoolInput(\n            name=\"detailed_thinking\",\n            display_name=\"Detailed Thinking\",\n            info=\"If true, the model will return a detailed thought process. Only supported by reasoning models.\",\n            value=False,\n            show=False,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Enable Tool Models\",\n            info=\"If enabled, only show models that support tool-calling.\",\n            advanced=False,\n            value=False,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"NVIDIA Base URL\",\n            value=\"https://integrate.api.nvidia.com/v1\",\n            info=\"The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"NVIDIA API Key\",\n            info=\"The NVIDIA API Key.\",\n            advanced=False,\n            value=\"NVIDIA_API_KEY\",\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Run inference with this temperature.\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            from langchain_nvidia_ai_endpoints import ChatNVIDIA\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use the NVIDIA model.\"\n            raise ImportError(msg) from e\n\n        # Note: don't include the previous model, as it may not exist in available models from the new base url\n        model = ChatNVIDIA(base_url=self.base_url, api_key=self.api_key)\n        if tool_model_enabled:\n            tool_models = [m for m in model.get_available_models() if m.supports_tools]\n            return [m.id for m in tool_models]\n        return [m.id for m in model.available_models]\n\n    def update_build_config(self, build_config: dotdict, _field_value: Any, field_name: str | None = None):\n        if field_name in {\"model_name\", \"tool_model_enabled\", \"base_url\", \"api_key\"}:\n            try:\n                ids = self.get_models(self.tool_model_enabled)\n                build_config[\"model_name\"][\"options\"] = ids\n\n                if \"value\" not in build_config[\"model_name\"] or build_config[\"model_name\"][\"value\"] is None:\n                    build_config[\"model_name\"][\"value\"] = ids[0]\n                elif build_config[\"model_name\"][\"value\"] not in ids:\n                    build_config[\"model_name\"][\"value\"] = None\n\n                # TODO: use api to determine if model supports detailed thinking\n                if build_config[\"model_name\"][\"value\"] == \"nemotron\":\n                    build_config[\"detailed_thinking\"][\"show\"] = True\n                else:\n                    build_config[\"detailed_thinking\"][\"value\"] = False\n                    build_config[\"detailed_thinking\"][\"show\"] = False\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                build_config[\"model_name\"][\"value\"] = None\n                build_config[\"model_name\"][\"options\"] = []\n                raise ValueError(msg) from e\n\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_nvidia_ai_endpoints import ChatNVIDIA\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use the NVIDIA model.\"\n            raise ImportError(msg) from e\n        api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        seed = self.seed\n        return ChatNVIDIA(\n            max_tokens=max_tokens or None,\n            model=model_name,\n            base_url=self.base_url,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "detailed_thinking": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": false,
        "name": "detailed_thinking",
        "value": false,
        "display_name": "Detailed Thinking",
        "advanced": false,
        "dynamic": false,
        "info": "If true, the model will return a detailed thought process. Only supported by reasoning models.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "writer/palmyra-fin-70b-32k",
          "baichuan-inc/baichuan2-13b-chat",
          "nv-mistralai/mistral-nemo-12b-instruct",
          "yentinglin/llama-3-taiwan-70b-instruct",
          "qwen/qwen2.5-coder-32b-instruct",
          "google/gemma-2-2b-it",
          "meta/llama-3.3-70b-instruct",
          "mistralai/mixtral-8x7b-instruct-v0.1",
          "microsoft/phi-3-medium-4k-instruct",
          "nvidia/llama-3.1-nemotron-70b-reward",
          "abacusai/dracarys-llama-3.1-70b-instruct",
          "meta/llama-3.1-70b-instruct",
          "meta/llama-3.2-3b-instruct",
          "nvidia/mistral-nemo-minitron-8b-8k-instruct",
          "nvidia/llama3-chatqa-1.5-8b",
          "meta/llama2-70b",
          "nvidia/neva-22b",
          "rakuten/rakutenai-7b-instruct",
          "meta/llama-3.1-8b-instruct",
          "google/recurrentgemma-2b",
          "microsoft/phi-3.5-vision-instruct",
          "google/gemma-2-27b-it",
          "mistralai/mathstral-7b-v0.1",
          "ibm/granite-3.0-3b-a800m-instruct",
          "mistralai/mixtral-8x22b-instruct-v0.1",
          "01-ai/yi-large",
          "adept/fuyu-8b",
          "qwen/qwen2-7b-instruct",
          "microsoft/phi-3.5-moe-instruct",
          "microsoft/phi-3-small-128k-instruct",
          "nvidia/nemotron-mini-4b-instruct",
          "institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1",
          "meta/codellama-70b",
          "google/gemma-2b",
          "thudm/chatglm3-6b",
          "upstage/solar-10.7b-instruct",
          "ibm/granite-34b-code-instruct",
          "nvidia/vila",
          "microsoft/phi-3-medium-128k-instruct",
          "google/gemma-7b",
          "mistralai/mistral-large-2-instruct",
          "google/paligemma",
          "writer/palmyra-med-70b",
          "deepseek-ai/deepseek-coder-6.7b-instruct",
          "mistralai/mistral-7b-instruct-v0.2",
          "microsoft/phi-3-small-8k-instruct",
          "meta/llama-3.2-90b-vision-instruct",
          "microsoft/kosmos-2",
          "nvidia/nemotron-4-mini-hindi-4b-instruct",
          "mistralai/mistral-7b-instruct-v0.3",
          "nvidia/llama3-chatqa-1.5-70b",
          "google/deplot",
          "microsoft/phi-3-mini-4k-instruct",
          "meta/llama-3.2-1b-instruct",
          "mistralai/mamba-codestral-7b-v0.1",
          "ai21labs/jamba-1.5-mini-instruct",
          "writer/palmyra-med-70b-32k",
          "meta/llama3-8b-instruct",
          "rakuten/rakutenai-7b-chat",
          "mistralai/codestral-22b-instruct-v0.1",
          "zyphra/zamba2-7b-instruct",
          "nvidia/usdcode-llama-3.1-70b-instruct",
          "nvidia/nemotron-4-340b-instruct",
          "tokyotech-llm/llama-3-swallow-70b-instruct-v0.1",
          "microsoft/phi-3.5-mini-instruct",
          "nvidia/llama-3.1-nemotron-51b-instruct",
          "seallms/seallm-7b-v2.5",
          "mediatek/breeze-7b-instruct",
          "nvidia/usdcode-llama3-70b-instruct",
          "meta/llama-3.1-405b-instruct",
          "microsoft/phi-3-mini-128k-instruct",
          "databricks/dbrx-instruct",
          "snowflake/arctic",
          "google/codegemma-7b",
          "google/codegemma-1.1-7b",
          "google/gemma-2-9b-it",
          "ibm/granite-8b-code-instruct",
          "ai21labs/jamba-1.5-large-instruct",
          "nvidia/llama-3.1-nemotron-70b-instruct",
          "microsoft/phi-3-vision-128k-instruct",
          "ibm/granite-3.0-8b-instruct",
          "mistralai/mistral-large",
          "institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1",
          "qwen/qwen2.5-coder-7b-instruct",
          "aisingapore/sea-lion-7b-instruct",
          "meta/llama3-70b-instruct",
          "meta/llama-3.2-11b-vision-instruct"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "The name of the NVIDIA model to use.",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Run inference with this temperature.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "tool_model_enabled": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tool_model_enabled",
        "value": false,
        "display_name": "Enable Tool Models",
        "advanced": false,
        "dynamic": false,
        "info": "If enabled, only show models that support tool-calling.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generates text using NVIDIA LLMs.",
    "icon": "NVIDIA",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "NVIDIA",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_name",
      "detailed_thinking",
      "tool_model_enabled",
      "base_url",
      "api_key",
      "temperature",
      "seed"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "MistralModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "MISTRAL_API_KEY",
        "display_name": "Mistral API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Mistral API Key to use for the Mistral model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_mistralai import ChatMistralAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass MistralAIModelComponent(LCModelComponent):\n    display_name = \"MistralAI\"\n    description = \"Generates text using MistralAI LLMs.\"\n    icon = \"MistralAI\"\n    name = \"MistralModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"open-mixtral-8x7b\",\n                \"open-mixtral-8x22b\",\n                \"mistral-small-latest\",\n                \"mistral-medium-latest\",\n                \"mistral-large-latest\",\n                \"codestral-latest\",\n            ],\n            value=\"codestral-latest\",\n        ),\n        StrInput(\n            name=\"mistral_api_base\",\n            display_name=\"Mistral API Base\",\n            advanced=True,\n            info=\"The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Mistral API Key\",\n            info=\"The Mistral API Key to use for the Mistral model.\",\n            advanced=False,\n            required=True,\n            value=\"MISTRAL_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            advanced=True,\n            value=60,\n        ),\n        IntInput(\n            name=\"max_concurrent_requests\",\n            display_name=\"Max Concurrent Requests\",\n            advanced=True,\n            value=3,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"random_seed\",\n            display_name=\"Random Seed\",\n            value=1,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"safe_mode\",\n            display_name=\"Safe Mode\",\n            advanced=True,\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            return ChatMistralAI(\n                model_name=self.model_name,\n                mistral_api_key=SecretStr(self.api_key).get_secret_value() if self.api_key else None,\n                endpoint=self.mistral_api_base or \"https://api.mistral.ai/v1\",\n                max_tokens=self.max_tokens or None,\n                temperature=self.temperature,\n                max_retries=self.max_retries,\n                timeout=self.timeout,\n                max_concurrent_requests=self.max_concurrent_requests,\n                top_p=self.top_p,\n                random_seed=self.random_seed,\n                safe_mode=self.safe_mode,\n                streaming=self.stream,\n            )\n        except Exception as e:\n            msg = \"Could not connect to MistralAI API.\"\n            raise ValueError(msg) from e\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_concurrent_requests": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_concurrent_requests",
        "value": 3,
        "display_name": "Max Concurrent Requests",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 5,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "mistral_api_base": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "mistral_api_base",
        "value": "",
        "display_name": "Mistral API Base",
        "advanced": true,
        "dynamic": false,
        "info": "The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "open-mixtral-8x7b",
          "open-mixtral-8x22b",
          "mistral-small-latest",
          "mistral-medium-latest",
          "mistral-large-latest",
          "codestral-latest"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "codestral-latest",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "random_seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "random_seed",
        "value": 1,
        "display_name": "Random Seed",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "safe_mode": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "safe_mode",
        "value": false,
        "display_name": "Safe Mode",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": 60,
        "display_name": "Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 1,
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generates text using MistralAI LLMs.",
    "icon": "MistralAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "MistralAI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_name",
      "mistral_api_base",
      "api_key",
      "temperature",
      "max_retries",
      "timeout",
      "max_concurrent_requests",
      "top_p",
      "random_seed",
      "safe_mode"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "SambaNovaModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "SAMBANOVA_API_KEY",
        "display_name": "Sambanova API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Sambanova API Key to use for the Sambanova model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "",
        "display_name": "SambaNova Cloud Base Url",
        "advanced": true,
        "dynamic": false,
        "info": "The base URL of the Sambanova Cloud API. Defaults to https://api.sambanova.ai/v1/chat/completions. You can change this to use other urls like Sambastudio",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_sambanova import ChatSambaNovaCloud\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.sambanova_constants import SAMBANOVA_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\n\nclass SambaNovaComponent(LCModelComponent):\n    display_name = \"SambaNova\"\n    description = \"Generate text using Sambanova LLMs.\"\n    documentation = \"https://cloud.sambanova.ai/\"\n    icon = \"SambaNova\"\n    name = \"SambaNovaModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"SambaNova Cloud Base Url\",\n            advanced=True,\n            info=\"The base URL of the Sambanova Cloud API. \"\n            \"Defaults to https://api.sambanova.ai/v1/chat/completions. \"\n            \"You can change this to use other urls like Sambastudio\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=SAMBANOVA_MODEL_NAMES,\n            value=SAMBANOVA_MODEL_NAMES[0],\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Sambanova API Key\",\n            info=\"The Sambanova API Key to use for the Sambanova model.\",\n            advanced=False,\n            value=\"SAMBANOVA_API_KEY\",\n            required=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=2048,\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        SliderInput(\n            name=\"top_p\",\n            display_name=\"top_p\",\n            advanced=True,\n            value=1.0,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            info=\"Model top_p\",\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        sambanova_url = self.base_url\n        sambanova_api_key = self.api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        top_p = self.top_p\n        temperature = self.temperature\n\n        api_key = SecretStr(sambanova_api_key).get_secret_value() if sambanova_api_key else None\n\n        return ChatSambaNovaCloud(\n            model=model_name,\n            max_tokens=max_tokens or 1024,\n            temperature=temperature or 0.07,\n            top_p=top_p,\n            sambanova_url=sambanova_url,\n            sambanova_api_key=api_key,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": 2048,
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Meta-Llama-3.3-70B-Instruct",
          "Meta-Llama-3.1-8B-Instruct",
          "Meta-Llama-3.1-70B-Instruct",
          "Meta-Llama-3.1-405B-Instruct",
          "DeepSeek-R1-Distill-Llama-70B",
          "DeepSeek-R1",
          "Meta-Llama-3.2-1B-Instruct",
          "Meta-Llama-3.2-3B-Instruct",
          "Llama-3.2-11B-Vision-Instruct",
          "Llama-3.2-90B-Vision-Instruct",
          "Qwen2.5-Coder-32B-Instruct",
          "Qwen2.5-72B-Instruct",
          "QwQ-32B-Preview",
          "Qwen2-Audio-7B-Instruct"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "Meta-Llama-3.3-70B-Instruct",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "top_p": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 1,
        "display_name": "top_p",
        "advanced": true,
        "dynamic": false,
        "info": "Model top_p",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generate text using Sambanova LLMs.",
    "icon": "SambaNova",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "SambaNova",
    "documentation": "https://cloud.sambanova.ai/",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "base_url",
      "model_name",
      "api_key",
      "max_tokens",
      "top_p",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "OpenRouterComponent": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "OpenRouter API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Your OpenRouter API key",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "app_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "app_name",
        "value": "",
        "display_name": "App Name",
        "advanced": true,
        "dynamic": false,
        "info": "Your app name for OpenRouter rankings",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from collections import defaultdict\nfrom typing import Any\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    DropdownInput,\n    IntInput,\n    SecretStrInput,\n    SliderInput,\n    StrInput,\n)\n\n\nclass OpenRouterComponent(LCModelComponent):\n    \"\"\"OpenRouter API component for language models.\"\"\"\n\n    display_name = \"OpenRouter\"\n    description = (\n        \"OpenRouter provides unified access to multiple AI models from different providers through a single API.\"\n    )\n    icon = \"OpenRouter\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(\n            name=\"api_key\", display_name=\"OpenRouter API Key\", required=True, info=\"Your OpenRouter API key\"\n        ),\n        StrInput(\n            name=\"site_url\",\n            display_name=\"Site URL\",\n            info=\"Your site URL for OpenRouter rankings\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"app_name\",\n            display_name=\"App Name\",\n            info=\"Your app name for OpenRouter rankings\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Provider\",\n            info=\"The AI model provider\",\n            options=[\"Loading providers...\"],\n            value=\"Loading providers...\",\n            real_time_refresh=True,\n            required=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The model to use for chat completion\",\n            options=[\"Select a provider first\"],\n            value=\"Select a provider first\",\n            real_time_refresh=True,\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            info=\"Maximum number of tokens to generate\",\n            advanced=True,\n        ),\n    ]\n\n    def fetch_models(self) -> dict[str, list]:\n        \"\"\"Fetch available models from OpenRouter API and organize them by provider.\"\"\"\n        url = \"https://openrouter.ai/api/v1/models\"\n\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n\n                models_data = response.json().get(\"data\", [])\n                provider_models = defaultdict(list)\n\n                for model in models_data:\n                    model_id = model.get(\"id\", \"\")\n                    if \"/\" in model_id:\n                        provider = model_id.split(\"/\")[0].title()\n                        provider_models[provider].append(\n                            {\n                                \"id\": model_id,\n                                \"name\": model.get(\"name\", \"\"),\n                                \"description\": model.get(\"description\", \"\"),\n                                \"context_length\": model.get(\"context_length\", 0),\n                            }\n                        )\n\n                return dict(provider_models)\n\n        except httpx.HTTPError as e:\n            self.log(f\"Error fetching models: {e!s}\")\n            return {\"Error\": [{\"id\": \"error\", \"name\": f\"Error fetching models: {e!s}\"}]}\n\n    def build_model(self) -> LanguageModel:\n        \"\"\"Build and return the OpenRouter language model.\"\"\"\n        model_not_selected = \"Please select a model\"\n        api_key_required = \"API key is required\"\n\n        if not self.model_name or self.model_name == \"Select a provider first\":\n            raise ValueError(model_not_selected)\n\n        if not self.api_key:\n            raise ValueError(api_key_required)\n\n        api_key = SecretStr(self.api_key).get_secret_value()\n\n        # Build base configuration\n        kwargs: dict[str, Any] = {\n            \"model\": self.model_name,\n            \"openai_api_key\": api_key,\n            \"openai_api_base\": \"https://openrouter.ai/api/v1\",\n            \"temperature\": self.temperature if self.temperature is not None else 0.7,\n        }\n\n        # Add optional parameters\n        if self.max_tokens:\n            kwargs[\"max_tokens\"] = self.max_tokens\n\n        headers = {}\n        if self.site_url:\n            headers[\"HTTP-Referer\"] = self.site_url\n        if self.app_name:\n            headers[\"X-Title\"] = self.app_name\n\n        if headers:\n            kwargs[\"default_headers\"] = headers\n\n        try:\n            return ChatOpenAI(**kwargs)\n        except (ValueError, httpx.HTTPError) as err:\n            error_msg = f\"Failed to build model: {err!s}\"\n            self.log(error_msg)\n            raise ValueError(error_msg) from err\n\n    def _get_exception_message(self, e: Exception) -> str | None:\n        \"\"\"Get a message from an OpenRouter exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str | None: The message from the exception, or None if no specific message can be extracted.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n\n            if isinstance(e, BadRequestError):\n                message = e.body.get(\"message\")\n                if message:\n                    return message\n        except ImportError:\n            pass\n        return None\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n        \"\"\"Update build configuration based on field updates.\"\"\"\n        try:\n            if field_name is None or field_name == \"provider\":\n                provider_models = self.fetch_models()\n                build_config[\"provider\"][\"options\"] = sorted(provider_models.keys())\n                if build_config[\"provider\"][\"value\"] not in provider_models:\n                    build_config[\"provider\"][\"value\"] = build_config[\"provider\"][\"options\"][0]\n\n            if field_name == \"provider\" and field_value in self.fetch_models():\n                provider_models = self.fetch_models()\n                models = provider_models[field_value]\n\n                build_config[\"model_name\"][\"options\"] = [model[\"id\"] for model in models]\n                if models:\n                    build_config[\"model_name\"][\"value\"] = models[0][\"id\"]\n\n                tooltips = {\n                    model[\"id\"]: (f\"{model['name']}\\nContext Length: {model['context_length']}\\n{model['description']}\")\n                    for model in models\n                }\n                build_config[\"model_name\"][\"tooltips\"] = tooltips\n\n        except httpx.HTTPError as e:\n            self.log(f\"Error updating build config: {e!s}\")\n            build_config[\"provider\"][\"options\"] = [\"Error loading providers\"]\n            build_config[\"provider\"][\"value\"] = \"Error loading providers\"\n            build_config[\"model_name\"][\"options\"] = [\"Error loading models\"]\n            build_config[\"model_name\"][\"value\"] = \"Error loading models\"\n\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "Maximum number of tokens to generate",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Select a provider first"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "Select a provider first",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "The model to use for chat completion",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "provider": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Loading providers..."
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "provider",
        "value": "Loading providers...",
        "display_name": "Provider",
        "advanced": false,
        "dynamic": false,
        "info": "The AI model provider",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "site_url": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "site_url",
        "value": "",
        "display_name": "Site URL",
        "advanced": true,
        "dynamic": false,
        "info": "Your site URL for OpenRouter rankings",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.7,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "OpenRouter provides unified access to multiple AI models from different providers through a single API.",
    "icon": "OpenRouter",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "OpenRouter",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key",
          "model_name"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "api_key",
      "site_url",
      "app_name",
      "provider",
      "model_name",
      "temperature",
      "max_tokens"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AIMLModel": {
    "template": {
      "_type": "Component",
      "aiml_api_base": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "aiml_api_base",
        "value": "",
        "display_name": "AIML API Base",
        "advanced": true,
        "dynamic": false,
        "info": "The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . You can change this to use other APIs like JinaChat, LocalAI and Prem.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "AIML_API_KEY",
        "display_name": "AIML API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The AIML API Key to use for the OpenAI model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.aiml_constants import AimlModels\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    DictInput,\n    DropdownInput,\n    IntInput,\n    SecretStrInput,\n    SliderInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(LCModelComponent):\n    display_name = \"AIML\"\n    description = \"Generates text using AIML LLMs.\"\n    icon = \"AIML\"\n    name = \"AIMLModel\"\n    documentation = \"https://docs.aimlapi.com/api-reference\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[],\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"aiml_api_base\",\n            display_name=\"AIML API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"AIML API Key\",\n            info=\"The AIML API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"AIML_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=2, step=0.01)\n        ),\n    ]\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"api_key\", \"aiml_api_base\", \"model_name\"}:\n            aiml = AimlModels()\n            aiml.get_aiml_models()\n            build_config[\"model_name\"][\"options\"] = aiml.chat_models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        aiml_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        aiml_api_base = self.aiml_api_base or \"https://api.aimlapi.com/v2\"\n\n        openai_api_key = aiml_api_key.get_secret_value() if isinstance(aiml_api_key, SecretStr) else aiml_api_key\n\n        # TODO: Once OpenAI fixes their o1 models, this part will need to be removed\n        # to work correctly with o1 temperature settings.\n        if \"o1\" in model_name:\n            temperature = 1\n\n        return ChatOpenAI(\n            model=model_name,\n            temperature=temperature,\n            api_key=openai_api_key,\n            base_url=aiml_api_base,\n            max_tokens=max_tokens or None,\n            **model_kwargs,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai.error import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.json_body.get(\"error\", {}).get(\"message\", \"\")\n            if message:\n                return message\n        return None\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generates text using AIML LLMs.",
    "icon": "AIML",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "AIML",
    "documentation": "https://docs.aimlapi.com/api-reference",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "model_name",
      "aiml_api_base",
      "api_key",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "NovitaModel": {
    "template": {
      "_type": "Component",
      "output_parser": {
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "output_parser",
        "value": "",
        "display_name": "Output Parser",
        "advanced": true,
        "input_types": [
          "OutputParser"
        ],
        "dynamic": false,
        "info": "The parser to use to parse the output of the model",
        "title_case": false,
        "type": "other",
        "_input_type": "HandleInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "NOVITA_API_KEY",
        "display_name": "Novita API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Novita API Key to use for Novita AI models.",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import requests\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.novita_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    IntInput,\n    SecretStrInput,\n    SliderInput,\n)\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass NovitaModelComponent(LCModelComponent):\n    display_name = \"Novita AI\"\n    description = \"Generates text using Novita AI LLMs (OpenAI compatible).\"\n    icon = \"Novita\"\n    name = \"NovitaModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=MODEL_NAMES,\n            value=MODEL_NAMES[0],\n            refresh_button=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Novita API Key\",\n            info=\"The Novita API Key to use for Novita AI models.\",\n            advanced=False,\n            value=\"NOVITA_API_KEY\",\n            real_time_refresh=True,\n        ),\n        SliderInput(name=\"temperature\", display_name=\"Temperature\", value=0.1, range_spec=RangeSpec(min=0, max=1)),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        base_url = \"https://api.novita.ai/v3/openai\"\n        url = f\"{base_url}/models\"\n\n        headers = {\"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return MODEL_NAMES\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"api_key\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        json_mode = self.json_mode\n        seed = self.seed\n\n        try:\n            output = ChatOpenAI(\n                model=model_name,\n                api_key=(SecretStr(api_key).get_secret_value() if api_key else None),\n                max_tokens=max_tokens or None,\n                temperature=temperature,\n                model_kwargs=model_kwargs,\n                streaming=self.stream,\n                seed=seed,\n                base_url=\"https://api.novita.ai/v3/openai\",\n            )\n        except Exception as e:\n            msg = \"Could not connect to Novita API.\"\n            raise ValueError(msg) from e\n\n        if json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "json_mode": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "json_mode",
        "value": false,
        "display_name": "JSON Mode",
        "advanced": true,
        "dynamic": false,
        "info": "If True, it will output JSON regardless of passing a schema.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "Additional keyword arguments to pass to the model.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "deepseek/deepseek-r1",
          "deepseek/deepseek_v3",
          "meta-llama/llama-3.3-70b-instruct",
          "meta-llama/llama-3.1-8b-instruct",
          "meta-llama/llama-3.1-70b-instruct",
          "mistralai/mistral-nemo",
          "Sao10K/L3-8B-Stheno-v3.2",
          "gryphe/mythomax-l2-13b",
          "qwen/qwen-2.5-72b-instruct",
          "meta-llama/llama-3-8b-instruct",
          "microsoft/wizardlm-2-8x22b",
          "google/gemma-2-9b-it",
          "mistralai/mistral-7b-instruct",
          "meta-llama/llama-3-70b-instruct",
          "openchat/openchat-7b",
          "nousresearch/hermes-2-pro-llama-3-8b",
          "sao10k/l3-70b-euryale-v2.1",
          "cognitivecomputations/dolphin-mixtral-8x22b",
          "jondurbin/airoboros-l2-70b",
          "nousresearch/nous-hermes-llama2-13b",
          "teknium/openhermes-2.5-mistral-7b",
          "sophosympatheia/midnight-rose-70b",
          "meta-llama/llama-3.1-8b-instruct-max",
          "sao10k/l3-8b-lunaris",
          "qwen/qwen-2-vl-72b-instruct",
          "meta-llama/llama-3.2-1b-instruct",
          "meta-llama/llama-3.2-11b-vision-instruct",
          "meta-llama/llama-3.2-3b-instruct",
          "meta-llama/llama-3.1-8b-instruct-bf16",
          "sao10k/l31-70b-euryale-v2.2",
          "qwen/qwen-2-7b-instruct",
          "qwen/qwen-2-72b-instruct"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "deepseek/deepseek-r1",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.1
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generates text using Novita AI LLMs (OpenAI compatible).",
    "icon": "Novita",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Novita AI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "json_mode",
      "model_name",
      "api_key",
      "temperature",
      "seed",
      "output_parser"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "PerplexityModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "Perplexity API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Perplexity API Key to use for the Perplexity model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\n\n\nclass PerplexityComponent(LCModelComponent):\n    display_name = \"Perplexity\"\n    description = \"Generate text using Perplexity LLMs.\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"llama-3.1-sonar-small-128k-online\",\n                \"llama-3.1-sonar-large-128k-online\",\n                \"llama-3.1-sonar-huge-128k-online\",\n                \"llama-3.1-sonar-small-128k-chat\",\n                \"llama-3.1-sonar-large-128k-chat\",\n                \"llama-3.1-8b-instruct\",\n                \"llama-3.1-70b-instruct\",\n            ],\n            value=\"llama-3.1-sonar-small-128k-online\",\n        ),\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Perplexity API Key\",\n            info=\"The Perplexity API Key to use for the Perplexity model.\",\n            advanced=False,\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\", display_name=\"Temperature\", value=0.75, range_spec=RangeSpec(min=0, max=2, step=0.05)\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = SecretStr(self.api_key).get_secret_value()\n        temperature = self.temperature\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatPerplexity(\n            model=model,\n            temperature=temperature or 0.75,\n            pplx_api_key=api_key,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            max_output_tokens=max_output_tokens,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_output_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_output_tokens",
        "value": "",
        "display_name": "Max Output Tokens",
        "advanced": false,
        "dynamic": false,
        "info": "The maximum number of tokens to generate.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "llama-3.1-sonar-small-128k-online",
          "llama-3.1-sonar-large-128k-online",
          "llama-3.1-sonar-huge-128k-online",
          "llama-3.1-sonar-small-128k-chat",
          "llama-3.1-sonar-large-128k-chat",
          "llama-3.1-8b-instruct",
          "llama-3.1-70b-instruct"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "llama-3.1-sonar-small-128k-online",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "n": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "n",
        "value": "",
        "display_name": "N",
        "advanced": true,
        "dynamic": false,
        "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.05
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.75,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": "",
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum cumulative probability of tokens to consider when sampling.",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate text using Perplexity LLMs.",
    "icon": "Perplexity",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Perplexity",
    "documentation": "https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "model_name",
      "max_output_tokens",
      "api_key",
      "temperature",
      "top_p",
      "n",
      "top_k"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "GoogleGenerativeAIModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "Google API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Google API Key to use for the Google Generative AI.",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in {\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\"} and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_output_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_output_tokens",
        "value": "",
        "display_name": "Max Output Tokens",
        "advanced": false,
        "dynamic": false,
        "info": "The maximum number of tokens to generate.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "gemini-1.5-pro",
          "gemini-1.5-flash",
          "gemini-1.5-flash-8b",
          "gemini-2.0-flash",
          "gemini-exp-1206",
          "gemini-2.0-flash-thinking-exp-01-21",
          "learnlm-1.5-pro-experimental",
          "gemma-2-2b",
          "gemma-2-9b",
          "gemma-2-27b"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gemini-1.5-pro",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "The name of the model to use.",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "n": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "n",
        "value": "",
        "display_name": "N",
        "advanced": true,
        "dynamic": false,
        "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "tool_model_enabled": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tool_model_enabled",
        "value": false,
        "display_name": "Tool Model Enabled",
        "advanced": false,
        "dynamic": false,
        "info": "Whether to use the tool model.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": "",
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum cumulative probability of tokens to consider when sampling.",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate text using Google Generative AI.",
    "icon": "GoogleGenerativeAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Google Generative AI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_output_tokens",
      "model_name",
      "api_key",
      "top_p",
      "temperature",
      "n",
      "top_k",
      "tool_model_enabled"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "GroqModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "Groq API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "API key for the Groq API.",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "https://api.groq.com",
        "display_name": "Groq API Base",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Base URL path for API requests, leave blank if not using a proxy or service emulator.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.groq_constants import (\n    GROQ_MODELS,\n    TOOL_CALLING_UNSUPPORTED_GROQ_MODELS,\n    UNSUPPORTED_GROQ_MODELS,\n)\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(\n            name=\"api_key\", display_name=\"Groq API Key\", info=\"API key for the Groq API.\", real_time_refresh=True\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n            real_time_refresh=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GROQ_MODELS,\n            value=GROQ_MODELS[0],\n            refresh_button=True,\n            combobox=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Enable Tool Models\",\n            info=(\n                \"Select if you want to use models that can work with tools. If yes, only those models will be shown.\"\n            ),\n            advanced=False,\n            value=False,\n            real_time_refresh=True,\n        ),\n    ]\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            url = f\"{self.base_url}/openai/v1/models\"\n            headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            model_ids = [\n                model[\"id\"] for model in model_list.get(\"data\", []) if model[\"id\"] not in UNSUPPORTED_GROQ_MODELS\n            ]\n        except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GROQ_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_groq import ChatGroq\n            except ImportError as e:\n                msg = \"langchain_groq is not installed. Please install it with `pip install langchain_groq`.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGroq(\n                    model=model,\n                    api_key=self.api_key,\n                    base_url=self.base_url,\n                )\n                if not self.supports_tool_calling(model_with_tool) or model in TOOL_CALLING_UNSUPPORTED_GROQ_MODELS:\n                    model_ids.remove(model)\n            return model_ids\n        return model_ids\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\"} and field_value:\n            try:\n                if len(self.api_key) != 0:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GROQ_MODELS\n                    build_config[\"model_name\"][\"options\"] = ids\n                    build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_groq import ChatGroq\n        except ImportError as e:\n            msg = \"langchain-groq is not installed. Please install it with `pip install langchain-groq`.\"\n            raise ImportError(msg) from e\n\n        return ChatGroq(\n            model=self.model_name,\n            max_tokens=self.max_tokens or None,\n            temperature=self.temperature,\n            base_url=self.base_url,\n            n=self.n or 1,\n            api_key=SecretStr(self.api_key).get_secret_value(),\n            streaming=self.stream,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Output Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "gemma2-9b-it",
          "llama-3.3-70b-versatile",
          "llama-3.1-8b-instant",
          "llama-guard-3-8b",
          "llama3-70b-8192",
          "llama3-8b-8192",
          "meta-llama/llama-4-scout-17b-16e-instruct",
          "meta-llama/llama-4-maverick-17b-128e-instruct",
          "qwen-qwq-32b",
          "qwen-2.5-coder-32b",
          "qwen-2.5-32b",
          "deepseek-r1-distill-qwen-32b",
          "deepseek-r1-distill-llama-70b",
          "llama-3.3-70b-specdec",
          "llama-3.2-1b-preview",
          "llama-3.2-3b-preview",
          "llama-3.2-11b-vision-preview",
          "llama-3.2-90b-vision-preview",
          "allam-2-7b"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gemma2-9b-it",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "The name of the model to use.",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "n": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "n",
        "value": "",
        "display_name": "N",
        "advanced": true,
        "dynamic": false,
        "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "tool_model_enabled": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tool_model_enabled",
        "value": false,
        "display_name": "Enable Tool Models",
        "advanced": false,
        "dynamic": false,
        "info": "Select if you want to use models that can work with tools. If yes, only those models will be shown.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate text using Groq.",
    "icon": "Groq",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Groq",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "api_key",
      "base_url",
      "max_tokens",
      "temperature",
      "n",
      "model_name",
      "tool_model_enabled"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AzureOpenAIModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "api_version": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "2024-10-01-preview",
          "2024-09-01-preview",
          "2024-08-01-preview",
          "2024-07-01-preview",
          "2024-06-01",
          "2024-03-01-preview",
          "2024-02-15-preview",
          "2023-12-01-preview",
          "2023-05-15"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_version",
        "value": "2024-06-01",
        "display_name": "API Version",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "azure_deployment": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "azure_deployment",
        "value": "",
        "display_name": "Deployment Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "azure_endpoint": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "azure_endpoint",
        "value": "",
        "display_name": "Azure Endpoint",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_openai import AzureChatOpenAI\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import MessageTextInput\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput\n\n\nclass AzureChatOpenAIComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI\"\n    description: str = \"Generate text using Azure OpenAI LLMs.\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/llms/azure_openai\"\n    beta = False\n    icon = \"Azure\"\n    name = \"AzureOpenAIModel\"\n\n    AZURE_OPENAI_API_VERSIONS = [\n        \"2024-06-01\",\n        \"2024-07-01-preview\",\n        \"2024-08-01-preview\",\n        \"2024-09-01-preview\",\n        \"2024-10-01-preview\",\n        \"2023-05-15\",\n        \"2023-12-01-preview\",\n        \"2024-02-15-preview\",\n        \"2024-03-01-preview\",\n    ]\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        MessageTextInput(\n            name=\"azure_endpoint\",\n            display_name=\"Azure Endpoint\",\n            info=\"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\",\n            required=True,\n        ),\n        MessageTextInput(name=\"azure_deployment\", display_name=\"Deployment Name\", required=True),\n        SecretStrInput(name=\"api_key\", display_name=\"API Key\", required=True),\n        DropdownInput(\n            name=\"api_version\",\n            display_name=\"API Version\",\n            options=sorted(AZURE_OPENAI_API_VERSIONS, reverse=True),\n            value=next(\n                (\n                    version\n                    for version in sorted(AZURE_OPENAI_API_VERSIONS, reverse=True)\n                    if not version.endswith(\"-preview\")\n                ),\n                AZURE_OPENAI_API_VERSIONS[0],\n            ),\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        azure_endpoint = self.azure_endpoint\n        azure_deployment = self.azure_deployment\n        api_version = self.api_version\n        api_key = self.api_key\n        temperature = self.temperature\n        max_tokens = self.max_tokens\n        stream = self.stream\n\n        try:\n            output = AzureChatOpenAI(\n                azure_endpoint=azure_endpoint,\n                azure_deployment=azure_deployment,\n                api_version=api_version,\n                api_key=api_key,\n                temperature=temperature,\n                max_tokens=max_tokens or None,\n                streaming=stream,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to AzureOpenAI API: {e}\"\n            raise ValueError(msg) from e\n\n        return output\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.7,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generate text using Azure OpenAI LLMs.",
    "icon": "Azure",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Azure OpenAI",
    "documentation": "https://python.langchain.com/docs/integrations/llms/azure_openai",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key",
          "azure_deployment",
          "azure_endpoint"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "azure_endpoint",
      "azure_deployment",
      "api_key",
      "api_version",
      "temperature",
      "max_tokens"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "LMStudioModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "LMSTUDIO_API_KEY",
        "display_name": "LM Studio API Key",
        "advanced": true,
        "input_types": [],
        "dynamic": false,
        "info": "The LM Studio API Key to use for LM Studio.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "http://localhost:1234/v1",
        "display_name": "Base URL",
        "advanced": false,
        "dynamic": false,
        "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate text using LM Studio Local LLMs.",
    "icon": "LMStudio",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "LM Studio",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "model_name",
      "base_url",
      "api_key",
      "temperature",
      "seed"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "OpenAIModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "OPENAI_API_KEY",
        "display_name": "OpenAI API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The OpenAI API Key to use for the OpenAI model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import (\n    OPENAI_MODEL_NAMES,\n    OPENAI_REASONING_MODEL_NAMES,\n)\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.logging import logger\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[1],\n            combobox=True,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            show=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            info=\"The maximum number of retries to make when generating.\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"The timeout for requests to OpenAI completion API.\",\n            advanced=True,\n            value=700,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        parameters = {\n            \"api_key\": SecretStr(self.api_key).get_secret_value() if self.api_key else None,\n            \"model_name\": self.model_name,\n            \"max_tokens\": self.max_tokens or None,\n            \"model_kwargs\": self.model_kwargs or {},\n            \"base_url\": self.openai_api_base or \"https://api.openai.com/v1\",\n            \"seed\": self.seed,\n            \"max_retries\": self.max_retries,\n            \"timeout\": self.timeout,\n            \"temperature\": self.temperature if self.temperature is not None else 0.1,\n        }\n\n        logger.info(f\"Model name: {self.model_name}\")\n        if self.model_name in OPENAI_REASONING_MODEL_NAMES:\n            logger.info(\"Getting reasoning model parameters\")\n            parameters.pop(\"temperature\")\n            parameters.pop(\"seed\")\n        output = ChatOpenAI(**parameters)\n        if self.json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_REASONING_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = False\n            build_config[\"seed\"][\"show\"] = False\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = True\n            build_config[\"seed\"][\"show\"] = True\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "json_mode": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "json_mode",
        "value": false,
        "display_name": "JSON Mode",
        "advanced": true,
        "dynamic": false,
        "info": "If True, it will output JSON regardless of passing a schema.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 5,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of retries to make when generating.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "Additional keyword arguments to pass to the model.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "gpt-4o-mini",
          "gpt-4o",
          "gpt-4.1",
          "gpt-4.1-mini",
          "gpt-4.1-nano",
          "gpt-4.5-preview",
          "gpt-4-turbo",
          "gpt-4-turbo-preview",
          "gpt-4",
          "gpt-3.5-turbo",
          "o1"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "gpt-4o",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "openai_api_base": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_api_base",
        "value": "",
        "display_name": "OpenAI API Base",
        "advanced": true,
        "dynamic": false,
        "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": 700,
        "display_name": "Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "The timeout for requests to OpenAI completion API.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      }
    },
    "description": "Generates text using OpenAI LLMs.",
    "icon": "OpenAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "OpenAI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "json_mode",
      "model_name",
      "openai_api_base",
      "api_key",
      "temperature",
      "seed",
      "max_retries",
      "timeout"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "HuggingFaceModel": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\nfrom tenacity import retry, stop_after_attempt, wait_fixed\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput, StrInput\n\n# TODO: langchain_community.llms.huggingface_endpoint is depreciated.\n#  Need to update to langchain_huggingface, but have dependency with langchain_core 0.3.0\n\n# Constants\nDEFAULT_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n\n\nclass HuggingFaceEndpointsComponent(LCModelComponent):\n    display_name: str = \"HuggingFace\"\n    description: str = \"Generate text using Hugging Face Inference APIs.\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(\n            name=\"model_id\",\n            display_name=\"Model ID\",\n            info=\"Select a model from HuggingFace Hub\",\n            options=[\n                DEFAULT_MODEL,\n                \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n                \"mistralai/Mistral-7B-Instruct-v0.3\",\n                \"meta-llama/Llama-3.1-8B-Instruct\",\n                \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n                \"Qwen/QwQ-32B-Preview\",\n                \"openai-community/gpt2\",\n                \"custom\",\n            ],\n            value=DEFAULT_MODEL,\n            required=True,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"custom_model\",\n            display_name=\"Custom Model ID\",\n            info=\"Enter a custom model ID from HuggingFace Hub\",\n            value=\"\",\n            show=False,\n            required=True,\n        ),\n        IntInput(\n            name=\"max_new_tokens\", display_name=\"Max New Tokens\", value=512, info=\"Maximum number of generated tokens\"\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            advanced=True,\n            info=\"The number of highest probability vocabulary tokens to keep for top-k-filtering\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            value=0.95,\n            advanced=True,\n            info=(\n                \"If set to < 1, only the smallest set of most probable tokens with \"\n                \"probabilities that add up to `top_p` or higher are kept for generation\"\n            ),\n        ),\n        FloatInput(\n            name=\"typical_p\",\n            display_name=\"Typical P\",\n            value=0.95,\n            advanced=True,\n            info=\"Typical Decoding mass.\",\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.8,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"The value used to module the logits distribution\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repetition_penalty\",\n            display_name=\"Repetition Penalty\",\n            info=\"The parameter for repetition penalty. 1.0 means no penalty.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"inference_endpoint\",\n            display_name=\"Inference Endpoint\",\n            value=\"https://api-inference.huggingface.co/models/\",\n            info=\"Custom inference endpoint URL.\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"task\",\n            display_name=\"Task\",\n            options=[\"text2text-generation\", \"text-generation\", \"summarization\", \"translation\"],\n            value=\"text-generation\",\n            advanced=True,\n            info=\"The task to call the model with. Should be a task that returns `generated_text` or `summary_text`.\",\n        ),\n        SecretStrInput(name=\"huggingfacehub_api_token\", display_name=\"API Token\", password=True, required=True),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Keyword Arguments\", advanced=True),\n        IntInput(name=\"retry_attempts\", display_name=\"Retry Attempts\", value=1, advanced=True),\n    ]\n\n    def get_api_url(self) -> str:\n        if \"huggingface\" in self.inference_endpoint.lower():\n            if self.model_id == \"custom\":\n                if not self.custom_model:\n                    error_msg = \"Custom model ID is required when 'custom' is selected\"\n                    raise ValueError(error_msg)\n                return f\"{self.inference_endpoint}{self.custom_model}\"\n            return f\"{self.inference_endpoint}{self.model_id}\"\n        return self.inference_endpoint\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:\n        \"\"\"Update build configuration based on field updates.\"\"\"\n        try:\n            if field_name is None or field_name == \"model_id\":\n                # If model_id is custom, show custom model field\n                if field_value == \"custom\":\n                    build_config[\"custom_model\"][\"show\"] = True\n                    build_config[\"custom_model\"][\"required\"] = True\n                else:\n                    build_config[\"custom_model\"][\"show\"] = False\n                    build_config[\"custom_model\"][\"value\"] = \"\"\n\n        except (KeyError, AttributeError) as e:\n            self.log(f\"Error updating build config: {e!s}\")\n        return build_config\n\n    def create_huggingface_endpoint(\n        self,\n        task: str | None,\n        huggingfacehub_api_token: str | None,\n        model_kwargs: dict[str, Any],\n        max_new_tokens: int,\n        top_k: int | None,\n        top_p: float,\n        typical_p: float | None,\n        temperature: float | None,\n        repetition_penalty: float | None,\n    ) -> HuggingFaceEndpoint:\n        retry_attempts = self.retry_attempts\n        endpoint_url = self.get_api_url()\n\n        @retry(stop=stop_after_attempt(retry_attempts), wait=wait_fixed(2))\n        def _attempt_create():\n            return HuggingFaceEndpoint(\n                endpoint_url=endpoint_url,\n                task=task,\n                huggingfacehub_api_token=huggingfacehub_api_token,\n                model_kwargs=model_kwargs,\n                max_new_tokens=max_new_tokens,\n                top_k=top_k,\n                top_p=top_p,\n                typical_p=typical_p,\n                temperature=temperature,\n                repetition_penalty=repetition_penalty,\n            )\n\n        return _attempt_create()\n\n    def build_model(self) -> LanguageModel:\n        task = self.task or None\n        huggingfacehub_api_token = self.huggingfacehub_api_token\n        model_kwargs = self.model_kwargs or {}\n        max_new_tokens = self.max_new_tokens\n        top_k = self.top_k or None\n        top_p = self.top_p\n        typical_p = self.typical_p or None\n        temperature = self.temperature or 0.8\n        repetition_penalty = self.repetition_penalty or None\n\n        try:\n            llm = self.create_huggingface_endpoint(\n                task=task,\n                huggingfacehub_api_token=huggingfacehub_api_token,\n                model_kwargs=model_kwargs,\n                max_new_tokens=max_new_tokens,\n                top_k=top_k,\n                top_p=top_p,\n                typical_p=typical_p,\n                temperature=temperature,\n                repetition_penalty=repetition_penalty,\n            )\n        except Exception as e:\n            msg = \"Could not connect to HuggingFace Endpoints API.\"\n            raise ValueError(msg) from e\n\n        return llm\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "custom_model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": false,
        "name": "custom_model",
        "value": "",
        "display_name": "Custom Model ID",
        "advanced": false,
        "dynamic": false,
        "info": "Enter a custom model ID from HuggingFace Hub",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "huggingfacehub_api_token": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "huggingfacehub_api_token",
        "value": "",
        "display_name": "API Token",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "inference_endpoint": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "inference_endpoint",
        "value": "https://api-inference.huggingface.co/models/",
        "display_name": "Inference Endpoint",
        "advanced": false,
        "dynamic": false,
        "info": "Custom inference endpoint URL.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_new_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_new_tokens",
        "value": 512,
        "display_name": "Max New Tokens",
        "advanced": false,
        "dynamic": false,
        "info": "Maximum number of generated tokens",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_id": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "meta-llama/Llama-3.3-70B-Instruct",
          "mistralai/Mixtral-8x7B-Instruct-v0.1",
          "mistralai/Mistral-7B-Instruct-v0.3",
          "meta-llama/Llama-3.1-8B-Instruct",
          "Qwen/Qwen2.5-Coder-32B-Instruct",
          "Qwen/QwQ-32B-Preview",
          "openai-community/gpt2",
          "custom"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_id",
        "value": "meta-llama/Llama-3.3-70B-Instruct",
        "display_name": "Model ID",
        "advanced": false,
        "dynamic": false,
        "info": "Select a model from HuggingFace Hub",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Keyword Arguments",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "repetition_penalty": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "repetition_penalty",
        "value": "",
        "display_name": "Repetition Penalty",
        "advanced": true,
        "dynamic": false,
        "info": "The parameter for repetition penalty. 1.0 means no penalty.",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "retry_attempts": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "retry_attempts",
        "value": 1,
        "display_name": "Retry Attempts",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "task": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "text2text-generation",
          "text-generation",
          "summarization",
          "translation"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "task",
        "value": "text-generation",
        "display_name": "Task",
        "advanced": true,
        "dynamic": false,
        "info": "The task to call the model with. Should be a task that returns `generated_text` or `summary_text`.",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.8,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "The value used to module the logits distribution",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "The number of highest probability vocabulary tokens to keep for top-k-filtering",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 0.95,
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "typical_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "typical_p",
        "value": 0.95,
        "display_name": "Typical P",
        "advanced": true,
        "dynamic": false,
        "info": "Typical Decoding mass.",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate text using Hugging Face Inference APIs.",
    "icon": "HuggingFace",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "HuggingFace",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "custom_model",
          "huggingfacehub_api_token",
          "inference_endpoint",
          "model_id"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "model_id",
      "custom_model",
      "max_new_tokens",
      "top_k",
      "top_p",
      "typical_p",
      "temperature",
      "repetition_penalty",
      "inference_endpoint",
      "task",
      "huggingfacehub_api_token",
      "model_kwargs",
      "retry_attempts"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "DeepSeekModelComponent": {
    "template": {
      "_type": "Component",
      "api_base": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_base",
        "value": "https://api.deepseek.com",
        "display_name": "DeepSeek API Base",
        "advanced": true,
        "dynamic": false,
        "info": "Base URL for API requests. Defaults to https://api.deepseek.com",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "DeepSeek API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The DeepSeek API Key",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import requests\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\n\nDEEPSEEK_MODELS = [\"deepseek-chat\"]\n\n\nclass DeepSeekModelComponent(LCModelComponent):\n    display_name = \"DeepSeek\"\n    description = \"Generate text using DeepSeek LLMs.\"\n    icon = \"DeepSeek\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"Maximum number of tokens to generate. Set to 0 for unlimited.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            info=\"DeepSeek model to use\",\n            options=DEEPSEEK_MODELS,\n            value=\"deepseek-chat\",\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"api_base\",\n            display_name=\"DeepSeek API Base\",\n            advanced=True,\n            info=\"Base URL for API requests. Defaults to https://api.deepseek.com\",\n            value=\"https://api.deepseek.com\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"DeepSeek API Key\",\n            info=\"The DeepSeek API Key\",\n            advanced=False,\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Controls randomness in responses\",\n            value=1.0,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        if not self.api_key:\n            return DEEPSEEK_MODELS\n\n        url = f\"{self.api_base}/models\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Accept\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return DEEPSEEK_MODELS\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in {\"api_key\", \"api_base\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:\n        try:\n            from langchain_openai import ChatOpenAI\n        except ImportError as e:\n            msg = \"langchain-openai not installed. Please install with `pip install langchain-openai`\"\n            raise ImportError(msg) from e\n\n        api_key = SecretStr(self.api_key).get_secret_value() if self.api_key else None\n        output = ChatOpenAI(\n            model=self.model_name,\n            temperature=self.temperature if self.temperature is not None else 0.1,\n            max_tokens=self.max_tokens or None,\n            model_kwargs=self.model_kwargs or {},\n            base_url=self.api_base,\n            api_key=api_key,\n            streaming=self.stream if hasattr(self, \"stream\") else False,\n            seed=self.seed,\n        )\n\n        if self.json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get message from DeepSeek API exception.\"\"\"\n        try:\n            from openai import BadRequestError\n\n            if isinstance(e, BadRequestError):\n                message = e.body.get(\"message\")\n                if message:\n                    return message\n        except ImportError:\n            pass\n        return None\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "json_mode": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "json_mode",
        "value": false,
        "display_name": "JSON Mode",
        "advanced": true,
        "dynamic": false,
        "info": "If True, it will output JSON regardless of passing a schema.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "Maximum number of tokens to generate. Set to 0 for unlimited.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "Additional keyword arguments to pass to the model.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "deepseek-chat"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "deepseek-chat",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "DeepSeek model to use",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness in responses",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generate text using DeepSeek LLMs.",
    "icon": "DeepSeek",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "DeepSeek",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "json_mode",
      "model_name",
      "api_base",
      "api_key",
      "temperature",
      "seed"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AnthropicModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "display_name": "Anthropic API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Your Anthropic API key.",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "https://api.anthropic.com",
        "display_name": "Anthropic API URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\n\nfrom langflow.base.models.anthropic_constants import (\n    ANTHROPIC_MODELS,\n    TOOL_CALLING_SUPPORTED_ANTHROPIC_MODELS,\n    TOOL_CALLING_UNSUPPORTED_ANTHROPIC_MODELS,\n)\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\nfrom langflow.schema.dotdict import dotdict\n\n\nclass AnthropicModelComponent(LCModelComponent):\n    display_name = \"Anthropic\"\n    description = \"Generate text using Anthropic Chat&Completion LLMs with prefill support.\"\n    icon = \"Anthropic\"\n    name = \"AnthropicModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            value=4096,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=ANTHROPIC_MODELS,\n            refresh_button=True,\n            value=ANTHROPIC_MODELS[0],\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Anthropic API Key\",\n            info=\"Your Anthropic API key.\",\n            value=None,\n            required=True,\n            real_time_refresh=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Anthropic API URL\",\n            info=\"Endpoint of the Anthropic API. Defaults to 'https://api.anthropic.com' if not specified.\",\n            value=\"https://api.anthropic.com\",\n            real_time_refresh=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Enable Tool Models\",\n            info=(\n                \"Select if you want to use models that can work with tools. If yes, only those models will be shown.\"\n            ),\n            advanced=False,\n            value=False,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"prefill\", display_name=\"Prefill\", info=\"Prefill text to guide the model's response.\", advanced=True\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_anthropic.chat_models import ChatAnthropic\n        except ImportError as e:\n            msg = \"langchain_anthropic is not installed. Please install it with `pip install langchain_anthropic`.\"\n            raise ImportError(msg) from e\n        try:\n            output = ChatAnthropic(\n                model=self.model_name,\n                anthropic_api_key=self.api_key,\n                max_tokens_to_sample=self.max_tokens,\n                temperature=self.temperature,\n                anthropic_api_url=self.base_url,\n                streaming=self.stream,\n            )\n        except Exception as e:\n            msg = \"Could not connect to Anthropic API.\"\n            raise ValueError(msg) from e\n\n        return output\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import anthropic\n\n            client = anthropic.Anthropic(api_key=self.api_key)\n            models = client.models.list(limit=20).data\n            model_ids = ANTHROPIC_MODELS + [model.id for model in models]\n        except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = ANTHROPIC_MODELS\n\n        if tool_model_enabled:\n            try:\n                from langchain_anthropic.chat_models import ChatAnthropic\n            except ImportError as e:\n                msg = \"langchain_anthropic is not installed. Please install it with `pip install langchain_anthropic`.\"\n                raise ImportError(msg) from e\n\n            # Create a new list instead of modifying while iterating\n            filtered_models = []\n            for model in model_ids:\n                if model in TOOL_CALLING_SUPPORTED_ANTHROPIC_MODELS:\n                    filtered_models.append(model)\n                    continue\n\n                model_with_tool = ChatAnthropic(\n                    model=model,  # Use the current model being checked\n                    anthropic_api_key=self.api_key,\n                    anthropic_api_url=self.base_url,\n                )\n\n                if (\n                    not self.supports_tool_calling(model_with_tool)\n                    or model in TOOL_CALLING_UNSUPPORTED_ANTHROPIC_MODELS\n                ):\n                    continue\n\n                filtered_models.append(model)\n\n            return filtered_models\n\n        return model_ids\n\n    def _get_exception_message(self, exception: Exception) -> str | None:\n        \"\"\"Get a message from an Anthropic exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from anthropic import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(exception, BadRequestError):\n            message = exception.body.get(\"error\", {}).get(\"message\")\n            if message:\n                return message\n        return None\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in {\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\"} and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = ANTHROPIC_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = ANTHROPIC_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n                build_config[\"model_name\"][\"combobox\"] = True\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": 4096,
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "claude-3-7-sonnet-latest",
          "claude-3-5-sonnet-latest",
          "claude-3-5-haiku-latest",
          "claude-3-opus-latest",
          "claude-3-sonnet-20240229",
          "claude-3-haiku-20240307"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "claude-3-7-sonnet-latest",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "prefill": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "prefill",
        "value": "",
        "display_name": "Prefill",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Prefill text to guide the model's response.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "tool_model_enabled": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tool_model_enabled",
        "value": false,
        "display_name": "Enable Tool Models",
        "advanced": false,
        "dynamic": false,
        "info": "Select if you want to use models that can work with tools. If yes, only those models will be shown.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate text using Anthropic Chat&Completion LLMs with prefill support.",
    "icon": "Anthropic",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Anthropic",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_name",
      "api_key",
      "temperature",
      "base_url",
      "tool_model_enabled",
      "prefill"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "xAIModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "XAI_API_KEY",
        "display_name": "xAI API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The xAI API Key to use for the model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "https://api.x.ai/v1",
        "display_name": "xAI API Base",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The base URL of the xAI API. Defaults to https://api.x.ai/v1",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import requests\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\nfrom typing_extensions import override\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, IntInput, MessageTextInput, SecretStrInput, SliderInput\n\nXAI_DEFAULT_MODELS = [\"grok-2-latest\"]\n\n\nclass XAIModelComponent(LCModelComponent):\n    display_name = \"xAI\"\n    description = \"Generates text using xAI models like Grok.\"\n    icon = \"xAI\"\n    name = \"xAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=XAI_DEFAULT_MODELS,\n            value=XAI_DEFAULT_MODELS[0],\n            refresh_button=True,\n            combobox=True,\n            info=\"The xAI model to use\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"xAI API Base\",\n            advanced=True,\n            info=\"The base URL of the xAI API. Defaults to https://api.x.ai/v1\",\n            value=\"https://api.x.ai/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"xAI API Key\",\n            info=\"The xAI API Key to use for the model.\",\n            advanced=False,\n            value=\"XAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        \"\"\"Fetch available models from xAI API.\"\"\"\n        if not self.api_key:\n            return XAI_DEFAULT_MODELS\n\n        base_url = self.base_url or \"https://api.x.ai/v1\"\n        url = f\"{base_url}/language-models\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Accept\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n\n            # Extract model IDs and any aliases\n            models = set()\n            for model in data.get(\"models\", []):\n                models.add(model[\"id\"])\n                models.update(model.get(\"aliases\", []))\n\n            return sorted(models) if models else XAI_DEFAULT_MODELS\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {e}\"\n            return XAI_DEFAULT_MODELS\n\n    @override\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        \"\"\"Update build configuration with fresh model list when key fields change.\"\"\"\n        if field_name in {\"api_key\", \"base_url\", \"model_name\"}:\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:\n        api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"https://api.x.ai/v1\"\n        json_mode = self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(api_key).get_secret_value() if api_key else None\n\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n        if json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an xAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "json_mode": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "json_mode",
        "value": false,
        "display_name": "JSON Mode",
        "advanced": true,
        "dynamic": false,
        "info": "If True, it will output JSON regardless of passing a schema.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 128000,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": "",
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "Additional keyword arguments to pass to the model.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "grok-2-latest"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "grok-2-latest",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "The xAI model to use",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 1,
        "display_name": "Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The seed controls the reproducibility of the job.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generates text using xAI models like Grok.",
    "icon": "xAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "xAI",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "max_tokens",
      "model_kwargs",
      "json_mode",
      "model_name",
      "base_url",
      "api_key",
      "temperature",
      "seed"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "IBMwatsonxModel": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API Key to use for the model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import json\nimport logging\nfrom typing import Any\n\nimport requests\nfrom langchain_ibm import ChatWatsonx\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.schema.dotdict import dotdict\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass WatsonxAIComponent(LCModelComponent):\n    display_name = \"IBM watsonx.ai\"\n    description = \"Generate text using IBM watsonx.ai foundation models.\"\n    icon = \"WatsonxAI\"\n    name = \"IBMwatsonxModel\"\n    beta = False\n\n    _default_models = [\"ibm/granite-3-2b-instruct\", \"ibm/granite-3-8b-instruct\", \"ibm/granite-13b-instruct-v2\"]\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(\n            name=\"url\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API.\",\n            value=None,\n            options=[\n                \"https://us-south.ml.cloud.ibm.com\",\n                \"https://eu-de.ml.cloud.ibm.com\",\n                \"https://eu-gb.ml.cloud.ibm.com\",\n                \"https://au-syd.ml.cloud.ibm.com\",\n                \"https://jp-tok.ml.cloud.ibm.com\",\n                \"https://ca-tor.ml.cloud.ibm.com\",\n            ],\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx Project ID\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The API Key to use for the model.\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[],\n            value=None,\n            dynamic=True,\n            required=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate.\",\n            range_spec=RangeSpec(min=1, max=4096),\n            value=1000,\n        ),\n        StrInput(\n            name=\"stop_sequence\",\n            display_name=\"Stop Sequence\",\n            advanced=True,\n            info=\"Sequence where generation should stop.\",\n            field_type=\"str\",\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Controls randomness, higher values increase diversity.\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The cumulative probability cutoff for token selection. \"\n            \"Lower values mean sampling from a smaller, more top-weighted nucleus.\",\n            value=0.9,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"frequency_penalty\",\n            display_name=\"Frequency Penalty\",\n            info=\"Penalty for frequency of token usage.\",\n            value=0.5,\n            range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"presence_penalty\",\n            display_name=\"Presence Penalty\",\n            info=\"Penalty for token presence in prior text.\",\n            value=0.3,\n            range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Random Seed\",\n            advanced=True,\n            info=\"The random seed for the model.\",\n            value=8,\n        ),\n        BoolInput(\n            name=\"logprobs\",\n            display_name=\"Log Probabilities\",\n            advanced=True,\n            info=\"Whether to return log probabilities of the output tokens.\",\n            value=True,\n        ),\n        IntInput(\n            name=\"top_logprobs\",\n            display_name=\"Top Log Probabilities\",\n            advanced=True,\n            info=\"Number of most likely tokens to return at each position.\",\n            value=3,\n            range_spec=RangeSpec(min=1, max=20),\n        ),\n        StrInput(\n            name=\"logit_bias\",\n            display_name=\"Logit Bias\",\n            advanced=True,\n            info='JSON string of token IDs to bias or suppress (e.g., {\"1003\": -100, \"1004\": 100}).',\n            field_type=\"str\",\n        ),\n    ]\n\n    @staticmethod\n    def fetch_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\"version\": \"2024-09-16\", \"filters\": \"function_text_chat,!lifecycle_withdrawn\"}\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:\n            logger.exception(\"Error fetching models. Using default models.\")\n            return WatsonxAIComponent._default_models\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        \"\"\"Update model options when URL or API key changes.\"\"\"\n        logger.info(\"Updating build config. Field name: %s, Field value: %s\", field_name, field_value)\n\n        if field_name == \"url\" and field_value:\n            try:\n                models = self.fetch_models(base_url=build_config.url.value)\n                build_config.model_name.options = models\n                if build_config.model_name.value:\n                    build_config.model_name.value = models[0]\n                info_message = f\"Updated model options: {len(models)} models found in {build_config.url.value}\"\n                logger.info(info_message)\n            except Exception:\n                logger.exception(\"Error updating model options.\")\n\n    def build_model(self) -> LanguageModel:\n        # Parse logit_bias from JSON string if provided\n        logit_bias = None\n        if hasattr(self, \"logit_bias\") and self.logit_bias:\n            try:\n                logit_bias = json.loads(self.logit_bias)\n            except json.JSONDecodeError:\n                logger.warning(\"Invalid logit_bias JSON format. Using default instead.\")\n                logit_bias = {\"1003\": -100, \"1004\": -100}\n\n        chat_params = {\n            \"max_tokens\": getattr(self, \"max_tokens\", None),\n            \"temperature\": getattr(self, \"temperature\", None),\n            \"top_p\": getattr(self, \"top_p\", None),\n            \"frequency_penalty\": getattr(self, \"frequency_penalty\", None),\n            \"presence_penalty\": getattr(self, \"presence_penalty\", None),\n            \"seed\": getattr(self, \"seed\", None),\n            \"stop\": [self.stop_sequence] if self.stop_sequence else [],\n            \"n\": 1,\n            \"logprobs\": getattr(self, \"logprobs\", True),\n            \"top_logprobs\": getattr(self, \"top_logprobs\", None),\n            \"time_limit\": 600000,\n            \"logit_bias\": logit_bias,\n        }\n\n        return ChatWatsonx(\n            apikey=SecretStr(self.api_key).get_secret_value(),\n            url=self.url,\n            project_id=self.project_id,\n            model_id=self.model_name,\n            params=chat_params,\n            streaming=self.stream,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "frequency_penalty": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": -2,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "frequency_penalty",
        "value": 0.5,
        "display_name": "Frequency Penalty",
        "advanced": true,
        "dynamic": false,
        "info": "Penalty for frequency of token usage.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "logit_bias": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "logit_bias",
        "value": "",
        "display_name": "Logit Bias",
        "advanced": true,
        "dynamic": false,
        "info": "JSON string of token IDs to bias or suppress (e.g., {\"1003\": -100, \"1004\": 100}).",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "logprobs": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "logprobs",
        "value": true,
        "display_name": "Log Probabilities",
        "advanced": true,
        "dynamic": false,
        "info": "Whether to return log probabilities of the output tokens.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "max_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 1,
          "max": 4096,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_tokens",
        "value": 1000,
        "display_name": "Max Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "The maximum number of tokens to generate.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": true,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "presence_penalty": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": -2,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "presence_penalty",
        "value": 0.3,
        "display_name": "Presence Penalty",
        "advanced": true,
        "dynamic": false,
        "info": "Penalty for token presence in prior text.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "project_id": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "project_id",
        "value": "",
        "display_name": "watsonx Project ID",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "seed": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "seed",
        "value": 8,
        "display_name": "Random Seed",
        "advanced": true,
        "dynamic": false,
        "info": "The random seed for the model.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stop_sequence": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stop_sequence",
        "value": "",
        "display_name": "Stop Sequence",
        "advanced": true,
        "dynamic": false,
        "info": "Sequence where generation should stop.",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness, higher values increase diversity.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "top_logprobs": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "range_spec": {
          "step_type": "float",
          "min": 1,
          "max": 20,
          "step": 0.1
        },
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_logprobs",
        "value": 3,
        "display_name": "Top Log Probabilities",
        "advanced": true,
        "dynamic": false,
        "info": "Number of most likely tokens to return at each position.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 0.9,
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "url": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "https://us-south.ml.cloud.ibm.com",
          "https://eu-de.ml.cloud.ibm.com",
          "https://eu-gb.ml.cloud.ibm.com",
          "https://au-syd.ml.cloud.ibm.com",
          "https://jp-tok.ml.cloud.ibm.com",
          "https://ca-tor.ml.cloud.ibm.com"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "url",
        "display_name": "watsonx API Endpoint",
        "advanced": false,
        "dynamic": false,
        "info": "The base URL of the API.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Generate text using IBM watsonx.ai foundation models.",
    "icon": "WatsonxAI",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "IBM watsonx.ai",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key",
          "model_name"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "url",
      "project_id",
      "api_key",
      "model_name",
      "max_tokens",
      "stop_sequence",
      "temperature",
      "top_p",
      "frequency_penalty",
      "presence_penalty",
      "seed",
      "logprobs",
      "top_logprobs",
      "logit_bias"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "CohereModel": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_cohere import ChatCohere\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import SecretStrInput, SliderInput\n\n\nclass CohereComponent(LCModelComponent):\n    display_name = \"Cohere\"\n    description = \"Generate text using Cohere LLMs.\"\n    documentation = \"https://python.langchain.com/docs/modules/model_io/models/llms/integrations/cohere\"\n    icon = \"Cohere\"\n    name = \"CohereModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(\n            name=\"cohere_api_key\",\n            display_name=\"Cohere API Key\",\n            info=\"The Cohere API Key to use for the Cohere model.\",\n            advanced=False,\n            value=\"COHERE_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.75,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        cohere_api_key = self.cohere_api_key\n        temperature = self.temperature\n\n        api_key = SecretStr(cohere_api_key).get_secret_value() if cohere_api_key else None\n\n        return ChatCohere(\n            temperature=temperature or 0.75,\n            cohere_api_key=api_key,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "cohere_api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "cohere_api_key",
        "value": "COHERE_API_KEY",
        "display_name": "Cohere API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The Cohere API Key to use for the Cohere model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 2,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.75,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      }
    },
    "description": "Generate text using Cohere LLMs.",
    "icon": "Cohere",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Cohere",
    "documentation": "https://python.langchain.com/docs/modules/model_io/models/llms/integrations/cohere",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "cohere_api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "input_value",
      "system_message",
      "stream",
      "cohere_api_key",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "OllamaModel": {
    "template": {
      "_type": "Component",
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "",
        "display_name": "Base URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Endpoint of the Ollama API.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, OLLAMA_TOOL_MODELS_BASE, URL_LIST\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SliderInput\n\nHTTP_STATUS_OK = 200\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API.\",\n            value=\"\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[],\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n            real_time_refresh=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\", advanced=True),\n        MessageTextInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to enable tool calling in the model.\",\n            value=False,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True\n        ),\n        *LCModelComponent._base_inputs,\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n            \"template\": self.template,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = (\n                \"Unable to connect to the Ollama API. \",\n                \"Please verify the base URL, ensure the relevant Ollama model is pulled, and try again.\",\n            )\n            raise ValueError(msg) from e\n\n        return output\n\n    async def is_valid_ollama_url(self, url: str) -> bool:\n        try:\n            async with httpx.AsyncClient() as client:\n                return (await client.get(urljoin(url, \"api/tags\"))).status_code == HTTP_STATUS_OK\n        except httpx.RequestError:\n            return False\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name in {\"base_url\", \"model_name\"} and not await self.is_valid_ollama_url(\n            build_config[\"base_url\"].get(\"value\", \"\")\n        ):\n            # Check if any URL in the list is valid\n            valid_url = \"\"\n            for url in URL_LIST:\n                if await self.is_valid_ollama_url(url):\n                    valid_url = url\n                    break\n            if valid_url != \"\":\n                build_config[\"base_url\"][\"value\"] = valid_url\n            else:\n                msg = \"No valid Ollama URL found.\"\n                raise ValueError(msg)\n        if field_name in {\"model_name\", \"base_url\", \"tool_model_enabled\"}:\n            if await self.is_valid_ollama_url(self.base_url):\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\n                build_config[\"model_name\"][\"options\"] = await self.get_model(self.base_url, tool_model_enabled)\n            elif await self.is_valid_ollama_url(build_config[\"base_url\"].get(\"value\", \"\")):\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\n                build_config[\"model_name\"][\"options\"] = await self.get_model(\n                    build_config[\"base_url\"].get(\"value\", \"\"), tool_model_enabled\n                )\n            else:\n                build_config[\"model_name\"][\"options\"] = []\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    async def get_model(self, base_url_value: str, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"api/tags\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n            model_ids = [model[\"name\"] for model in data.get(\"models\", [])]\n            # this to ensure that not embedding models are included.\n            # not even the base models since models can have 1b 2b etc\n            # handles cases when embeddings models have tags like :latest - etc.\n            model_ids = [\n                model\n                for model in model_ids\n                if not any(\n                    model == embedding_model or model.startswith(embedding_model.split(\"-\")[0])\n                    for embedding_model in OLLAMA_EMBEDDING_MODELS\n                )\n            ]\n\n        except (ImportError, ValueError, httpx.RequestError, Exception) as e:\n            msg = \"Could not get model names from Ollama.\"\n            raise ValueError(msg) from e\n        return (\n            model_ids if not tool_model_enabled else [model for model in model_ids if self.supports_tool_calling(model)]\n        )\n\n    def supports_tool_calling(self, model: str) -> bool:\n        \"\"\"Check if model name is in the base of any models example llama3.3 can have 1b and 2b.\"\"\"\n        return any(model.startswith(f\"{tool_model}\") for tool_model in OLLAMA_TOOL_MODELS_BASE)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "format": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "format",
        "value": "",
        "display_name": "Format",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Specify the format of the output (e.g., json).",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "input_value": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_value",
        "value": "",
        "display_name": "Input",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      },
      "metadata": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "metadata",
        "value": {},
        "display_name": "Metadata",
        "advanced": true,
        "dynamic": false,
        "info": "Metadata to add to the run trace.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "mirostat": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Disabled",
          "Mirostat",
          "Mirostat 2.0"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "mirostat",
        "value": "Disabled",
        "display_name": "Mirostat",
        "advanced": true,
        "dynamic": false,
        "info": "Enable/disable Mirostat sampling for controlling perplexity.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "mirostat_eta": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "mirostat_eta",
        "value": "",
        "display_name": "Mirostat Eta",
        "advanced": true,
        "dynamic": false,
        "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "mirostat_tau": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "mirostat_tau",
        "value": "",
        "display_name": "Mirostat Tau",
        "advanced": true,
        "dynamic": false,
        "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "Refer to https://ollama.com/library for more models.",
        "real_time_refresh": true,
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "num_ctx": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "num_ctx",
        "value": "",
        "display_name": "Context Window Size",
        "advanced": true,
        "dynamic": false,
        "info": "Size of the context window for generating tokens. (Default: 2048)",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "num_gpu": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "num_gpu",
        "value": "",
        "display_name": "Number of GPUs",
        "advanced": true,
        "dynamic": false,
        "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "num_thread": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "num_thread",
        "value": "",
        "display_name": "Number of Threads",
        "advanced": true,
        "dynamic": false,
        "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "repeat_last_n": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "repeat_last_n",
        "value": "",
        "display_name": "Repeat Last N",
        "advanced": true,
        "dynamic": false,
        "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "repeat_penalty": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "repeat_penalty",
        "value": "",
        "display_name": "Repeat Penalty",
        "advanced": true,
        "dynamic": false,
        "info": "Penalty for repetitions in generated text. (Default: 1.1)",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "stop_tokens": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stop_tokens",
        "value": "",
        "display_name": "Stop Tokens",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Comma-separated list of tokens to signal the model to stop generating text.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "stream": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stream",
        "value": false,
        "display_name": "Stream",
        "advanced": true,
        "dynamic": false,
        "info": "Stream the response from the model. Streaming works only in Chat.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "system": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system",
        "value": "",
        "display_name": "System",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System to use for generating text.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "system_message": {
        "tool_mode": false,
        "trace_as_input": true,
        "multiline": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "system_message",
        "value": "",
        "display_name": "System Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "System message to pass to the model.",
        "title_case": false,
        "copy_field": false,
        "type": "str",
        "_input_type": "MultilineInput"
      },
      "tags": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tags",
        "value": "",
        "display_name": "Tags",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Comma-separated list of tags to add to the run trace.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "temperature": {
        "tool_mode": false,
        "min_label": "",
        "max_label": "",
        "min_label_icon": "",
        "max_label_icon": "",
        "slider_buttons": false,
        "slider_buttons_options": [],
        "slider_input": false,
        "range_spec": {
          "step_type": "float",
          "min": 0,
          "max": 1,
          "step": 0.01
        },
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "slider",
        "_input_type": "SliderInput"
      },
      "template": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "template",
        "value": "",
        "display_name": "Template",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Template to use for generating text.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "tfs_z": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tfs_z",
        "value": "",
        "display_name": "TFS Z",
        "advanced": true,
        "dynamic": false,
        "info": "Tail free sampling value. (Default: 1)",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": "",
        "display_name": "Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "Timeout for the request stream.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "tool_model_enabled": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tool_model_enabled",
        "value": false,
        "display_name": "Tool Model Enabled",
        "advanced": false,
        "dynamic": false,
        "info": "Whether to enable tool calling in the model.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "Limits token selection to top K. (Default: 40)",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": "",
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "Works together with top-k. (Default: 0.9)",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "verbose": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "verbose",
        "value": false,
        "display_name": "Verbose",
        "advanced": true,
        "dynamic": false,
        "info": "Whether to print out response text.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate text using Ollama Local LLMs.",
    "icon": "Ollama",
    "base_classes": [
      "LanguageModel",
      "Message"
    ],
    "display_name": "Ollama",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Message"
        ],
        "selected": "Message",
        "name": "text_output",
        "display_name": "Message",
        "method": "text_response",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      },
      {
        "types": [
          "LanguageModel"
        ],
        "selected": "LanguageModel",
        "name": "model_output",
        "display_name": "Language Model",
        "method": "build_model",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "base_url",
      "model_name",
      "temperature",
      "format",
      "metadata",
      "mirostat",
      "mirostat_eta",
      "mirostat_tau",
      "num_ctx",
      "num_gpu",
      "num_thread",
      "repeat_last_n",
      "repeat_penalty",
      "tfs_z",
      "timeout",
      "top_k",
      "top_p",
      "verbose",
      "tags",
      "stop_tokens",
      "system",
      "tool_model_enabled",
      "template",
      "input_value",
      "system_message",
      "stream"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  }
}