{
  "VertexAIEmbeddings": {
    "template": {
      "_type": "Component",
      "credentials": {
        "trace_as_metadata": true,
        "file_path": "",
        "fileTypes": [
          "json"
        ],
        "temp_file": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "credentials",
        "value": "",
        "display_name": "Credentials",
        "advanced": false,
        "dynamic": false,
        "info": "JSON credentials file. Leave empty to fallback to environment variables",
        "title_case": false,
        "type": "file",
        "_input_type": "FileInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, FileInput, FloatInput, IntInput, MessageTextInput, Output\n\n\nclass VertexAIEmbeddingsComponent(LCModelComponent):\n    display_name = \"VertexAI Embeddings\"\n    description = \"Generate embeddings using Google Cloud VertexAI models.\"\n    icon = \"VertexAI\"\n    name = \"VertexAIEmbeddings\"\n\n    inputs = [\n        FileInput(\n            name=\"credentials\",\n            display_name=\"Credentials\",\n            info=\"JSON credentials file. Leave empty to fallback to environment variables\",\n            value=\"\",\n            file_types=[\"json\"],\n            required=True,\n        ),\n        MessageTextInput(name=\"location\", display_name=\"Location\", value=\"us-central1\", advanced=True),\n        MessageTextInput(name=\"project\", display_name=\"Project\", info=\"The project ID.\", advanced=True),\n        IntInput(name=\"max_output_tokens\", display_name=\"Max Output Tokens\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=1, advanced=True),\n        MessageTextInput(name=\"model_name\", display_name=\"Model Name\", value=\"textembedding-gecko\", required=True),\n        IntInput(name=\"n\", display_name=\"N\", value=1, advanced=True),\n        IntInput(name=\"request_parallelism\", value=5, display_name=\"Request Parallelism\", advanced=True),\n        MessageTextInput(name=\"stop_sequences\", display_name=\"Stop\", advanced=True, is_list=True),\n        BoolInput(name=\"streaming\", display_name=\"Streaming\", value=False, advanced=True),\n        FloatInput(name=\"temperature\", value=0.0, display_name=\"Temperature\"),\n        IntInput(name=\"top_k\", display_name=\"Top K\", advanced=True),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", value=0.95, advanced=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_google_vertexai import VertexAIEmbeddings\n        except ImportError as e:\n            msg = \"Please install the langchain-google-vertexai package to use the VertexAIEmbeddings component.\"\n            raise ImportError(msg) from e\n\n        from google.oauth2 import service_account\n\n        if self.credentials:\n            gcloud_credentials = service_account.Credentials.from_service_account_file(self.credentials)\n        else:\n            # will fallback to environment variable or inferred from gcloud CLI\n            gcloud_credentials = None\n        return VertexAIEmbeddings(\n            credentials=gcloud_credentials,\n            location=self.location,\n            max_output_tokens=self.max_output_tokens or None,\n            max_retries=self.max_retries,\n            model_name=self.model_name,\n            n=self.n,\n            project=self.project,\n            request_parallelism=self.request_parallelism,\n            stop=self.stop_sequences or None,\n            streaming=self.streaming,\n            temperature=self.temperature,\n            top_k=self.top_k or None,\n            top_p=self.top_p,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "location": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "location",
        "value": "us-central1",
        "display_name": "Location",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "max_output_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_output_tokens",
        "value": "",
        "display_name": "Max Output Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 1,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "textembedding-gecko",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "n": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "n",
        "value": 1,
        "display_name": "N",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "project": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "project",
        "value": "",
        "display_name": "Project",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The project ID.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "request_parallelism": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "request_parallelism",
        "value": 5,
        "display_name": "Request Parallelism",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "stop_sequences": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "stop_sequences",
        "value": "",
        "display_name": "Stop",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "streaming": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "streaming",
        "value": false,
        "display_name": "Streaming",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0,
        "display_name": "Temperature",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "top_k": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_k",
        "value": "",
        "display_name": "Top K",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "top_p": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "top_p",
        "value": 0.95,
        "display_name": "Top P",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate embeddings using Google Cloud VertexAI models.",
    "icon": "VertexAI",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "VertexAI Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "credentials",
      "location",
      "project",
      "max_output_tokens",
      "max_retries",
      "model_name",
      "n",
      "request_parallelism",
      "stop_sequences",
      "streaming",
      "temperature",
      "top_k",
      "top_p"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "HuggingFaceInferenceAPIEmbeddings": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Required for non-local inference endpoints. Local inference does not require an API Key.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from urllib.parse import urlparse\n\nimport requests\nfrom langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n\n# Next update: use langchain_huggingface\nfrom pydantic import SecretStr\nfrom tenacity import retry, stop_after_attempt, wait_fixed\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\n\nclass HuggingFaceInferenceAPIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"HuggingFace Embeddings Inference\"\n    description = \"Generate embeddings using HuggingFace Text Embeddings Inference (TEI)\"\n    documentation = \"https://huggingface.co/docs/text-embeddings-inference/index\"\n    icon = \"HuggingFace\"\n    name = \"HuggingFaceInferenceAPIEmbeddings\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            advanced=False,\n            info=\"Required for non-local inference endpoints. Local inference does not require an API Key.\",\n        ),\n        MessageTextInput(\n            name=\"inference_endpoint\",\n            display_name=\"Inference Endpoint\",\n            required=True,\n            value=\"https://api-inference.huggingface.co/models/\",\n            info=\"Custom inference endpoint URL.\",\n        ),\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"BAAI/bge-large-en-v1.5\",\n            info=\"The name of the model to use for text embeddings.\",\n            required=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def validate_inference_endpoint(self, inference_endpoint: str) -> bool:\n        parsed_url = urlparse(inference_endpoint)\n        if not all([parsed_url.scheme, parsed_url.netloc]):\n            msg = (\n                f\"Invalid inference endpoint format: '{self.inference_endpoint}'. \"\n                \"Please ensure the URL includes both a scheme (e.g., 'http://' or 'https://') and a domain name. \"\n                \"Example: 'http://localhost:8080' or 'https://api.example.com'\"\n            )\n            raise ValueError(msg)\n\n        try:\n            response = requests.get(f\"{inference_endpoint}/health\", timeout=5)\n        except requests.RequestException as e:\n            msg = (\n                f\"Inference endpoint '{inference_endpoint}' is not responding. \"\n                \"Please ensure the URL is correct and the service is running.\"\n            )\n            raise ValueError(msg) from e\n\n        if response.status_code != requests.codes.ok:\n            msg = f\"HuggingFace health check failed: {response.status_code}\"\n            raise ValueError(msg)\n        # returning True to solve linting error\n        return True\n\n    def get_api_url(self) -> str:\n        if \"huggingface\" in self.inference_endpoint.lower():\n            return f\"{self.inference_endpoint}\"\n        return self.inference_endpoint\n\n    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n    def create_huggingface_embeddings(\n        self, api_key: SecretStr, api_url: str, model_name: str\n    ) -> HuggingFaceInferenceAPIEmbeddings:\n        return HuggingFaceInferenceAPIEmbeddings(api_key=api_key, api_url=api_url, model_name=model_name)\n\n    def build_embeddings(self) -> Embeddings:\n        api_url = self.get_api_url()\n\n        is_local_url = (\n            api_url.startswith((\"http://localhost\", \"http://127.0.0.1\", \"http://0.0.0.0\", \"http://docker\"))\n            or \"huggingface.co\" not in api_url.lower()\n        )\n\n        if not self.api_key and is_local_url:\n            self.validate_inference_endpoint(api_url)\n            api_key = SecretStr(\"APIKeyForLocalDeployment\")\n        elif not self.api_key:\n            msg = \"API Key is required for non-local inference endpoints\"\n            raise ValueError(msg)\n        else:\n            api_key = SecretStr(self.api_key).get_secret_value()\n\n        try:\n            return self.create_huggingface_embeddings(api_key, api_url, self.model_name)\n        except Exception as e:\n            msg = \"Could not connect to HuggingFace Inference API.\"\n            raise ValueError(msg) from e\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "inference_endpoint": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "inference_endpoint",
        "value": "https://api-inference.huggingface.co/models/",
        "display_name": "Inference Endpoint",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Custom inference endpoint URL.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "BAAI/bge-large-en-v1.5",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The name of the model to use for text embeddings.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      }
    },
    "description": "Generate embeddings using HuggingFace Text Embeddings Inference (TEI)",
    "icon": "HuggingFace",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "HuggingFace Embeddings Inference",
    "documentation": "https://huggingface.co/docs/text-embeddings-inference/index",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "inference_endpoint",
      "model_name"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "NVIDIAEmbeddingsComponent": {
    "template": {
      "_type": "Component",
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "https://integrate.api.nvidia.com/v1",
        "display_name": "NVIDIA Base URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\nfrom langflow.schema.dotdict import dotdict\n\n\nclass NVIDIAEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"NVIDIA Embeddings\"\n    description: str = \"Generate embeddings using NVIDIA models.\"\n    icon = \"NVIDIA\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            options=[\n                \"nvidia/nv-embed-v1\",\n                \"snowflake/arctic-embed-I\",\n            ],\n            value=\"nvidia/nv-embed-v1\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"NVIDIA Base URL\",\n            refresh_button=True,\n            value=\"https://integrate.api.nvidia.com/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"nvidia_api_key\",\n            display_name=\"NVIDIA API Key\",\n            info=\"The NVIDIA API Key.\",\n            advanced=False,\n            value=\"NVIDIA_API_KEY\",\n            required=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"base_url\" and field_value:\n            try:\n                build_model = self.build_embeddings()\n                ids = [model.id for model in build_model.available_models]\n                build_config[\"model\"][\"options\"] = ids\n                build_config[\"model\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use the Nvidia model.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.nvidia_api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to NVIDIA API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "nvidia/nv-embed-v1",
          "snowflake/arctic-embed-I"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "nvidia/nv-embed-v1",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "nvidia_api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "nvidia_api_key",
        "value": "NVIDIA_API_KEY",
        "display_name": "NVIDIA API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The NVIDIA API Key.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Model Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate embeddings using NVIDIA models.",
    "icon": "NVIDIA",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "NVIDIA Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "base_url",
          "model",
          "nvidia_api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model",
      "base_url",
      "nvidia_api_key",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "EmbeddingModel": {
    "template": {
      "_type": "Component",
      "api_base": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_base",
        "value": "",
        "display_name": "API Base URL",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Base URL for the API. Leave empty for default.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "OpenAI API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Model Provider API key",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "chunk_size": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "chunk_size",
        "value": 1000,
        "display_name": "Chunk Size",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langchain_openai import OpenAIEmbeddings\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageTextInput,\n    SecretStrInput,\n)\nfrom langflow.schema.dotdict import dotdict\n\n\nclass EmbeddingModelComponent(LCEmbeddingsModel):\n    display_name = \"Embedding Model\"\n    description = \"Generate embeddings using a specified provider.\"\n    icon = \"binary\"\n    name = \"EmbeddingModel\"\n    category = \"embeddings\"\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Model Provider\",\n            options=[\"OpenAI\"],\n            value=\"OpenAI\",\n            info=\"Select the embedding model provider\",\n            real_time_refresh=True,\n            options_metadata=[{\"icon\": \"OpenAI\"}],\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            options=OPENAI_EMBEDDING_MODEL_NAMES,\n            value=OPENAI_EMBEDDING_MODEL_NAMES[0],\n            info=\"Select the embedding model to use\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider API key\",\n            required=True,\n            show=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"api_base\",\n            display_name=\"API Base URL\",\n            info=\"Base URL for the API. Leave empty for default.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have.\",\n            advanced=True,\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", advanced=True, value=3),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        provider = self.provider\n        model = self.model\n        api_key = self.api_key\n        api_base = self.api_base\n        dimensions = self.dimensions\n        chunk_size = self.chunk_size\n        request_timeout = self.request_timeout\n        max_retries = self.max_retries\n        show_progress_bar = self.show_progress_bar\n        model_kwargs = self.model_kwargs or {}\n\n        if provider == \"OpenAI\":\n            if not api_key:\n                msg = \"OpenAI API key is required when using OpenAI provider\"\n                raise ValueError(msg)\n            return OpenAIEmbeddings(\n                model=model,\n                dimensions=dimensions or None,\n                base_url=api_base or None,\n                api_key=api_key,\n                chunk_size=chunk_size,\n                max_retries=max_retries,\n                timeout=request_timeout or None,\n                show_progress_bar=show_progress_bar,\n                model_kwargs=model_kwargs,\n            )\n        msg = f\"Unknown provider: {provider}\"\n        raise ValueError(msg)\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None) -> dotdict:\n        if field_name == \"provider\" and field_value == \"OpenAI\":\n            build_config[\"model\"][\"options\"] = OPENAI_EMBEDDING_MODEL_NAMES\n            build_config[\"model\"][\"value\"] = OPENAI_EMBEDDING_MODEL_NAMES[0]\n            build_config[\"api_key\"][\"display_name\"] = \"OpenAI API Key\"\n            build_config[\"api_base\"][\"display_name\"] = \"OpenAI API Base URL\"\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "dimensions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "dimensions",
        "value": "",
        "display_name": "Dimensions",
        "advanced": true,
        "dynamic": false,
        "info": "The number of dimensions the resulting output embeddings should have.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 3,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "text-embedding-3-small",
          "text-embedding-3-large",
          "text-embedding-ada-002"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "text-embedding-3-small",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "Select the embedding model to use",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "Additional keyword arguments to pass to the model.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "provider": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "OpenAI"
        ],
        "options_metadata": [
          {
            "icon": "OpenAI"
          }
        ],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "provider",
        "value": "OpenAI",
        "display_name": "Model Provider",
        "advanced": false,
        "dynamic": false,
        "info": "Select the embedding model provider",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "request_timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "request_timeout",
        "value": "",
        "display_name": "Request Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "show_progress_bar": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "show_progress_bar",
        "value": false,
        "display_name": "Show Progress Bar",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate embeddings using a specified provider.",
    "icon": "binary",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Embedding Model",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "provider",
      "model",
      "api_key",
      "api_base",
      "dimensions",
      "chunk_size",
      "request_timeout",
      "max_retries",
      "show_progress_bar",
      "model_kwargs"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "MistalAIEmbeddings": {
    "template": {
      "_type": "Component",
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass MistralAIEmbeddingsComponent(LCModelComponent):\n    display_name = \"MistralAI Embeddings\"\n    description = \"Generate embeddings using MistralAI models.\"\n    icon = \"MistralAI\"\n    name = \"MistalAIEmbeddings\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\"mistral-embed\"],\n            value=\"mistral-embed\",\n        ),\n        SecretStrInput(name=\"mistral_api_key\", display_name=\"Mistral API Key\", required=True),\n        IntInput(\n            name=\"max_concurrent_requests\",\n            display_name=\"Max Concurrent Requests\",\n            advanced=True,\n            value=64,\n        ),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", advanced=True, value=5),\n        IntInput(name=\"timeout\", display_name=\"Request Timeout\", advanced=True, value=120),\n        MessageTextInput(\n            name=\"endpoint\",\n            display_name=\"API Endpoint\",\n            advanced=True,\n            value=\"https://api.mistral.ai/v1/\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not self.mistral_api_key:\n            msg = \"Mistral API Key is required\"\n            raise ValueError(msg)\n\n        api_key = SecretStr(self.mistral_api_key).get_secret_value()\n\n        return MistralAIEmbeddings(\n            api_key=api_key,\n            model=self.model,\n            endpoint=self.endpoint,\n            max_concurrent_requests=self.max_concurrent_requests,\n            max_retries=self.max_retries,\n            timeout=self.timeout,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "endpoint": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "endpoint",
        "value": "https://api.mistral.ai/v1/",
        "display_name": "API Endpoint",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "max_concurrent_requests": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_concurrent_requests",
        "value": 64,
        "display_name": "Max Concurrent Requests",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 5,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "mistral_api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "mistral_api_key",
        "value": "",
        "display_name": "Mistral API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "mistral-embed"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "mistral-embed",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "timeout",
        "value": 120,
        "display_name": "Request Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      }
    },
    "description": "Generate embeddings using MistralAI models.",
    "icon": "MistralAI",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "MistralAI Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model",
      "mistral_api_key",
      "max_concurrent_requests",
      "max_retries",
      "timeout",
      "endpoint"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "EmbeddingSimilarityComponent": {
    "template": {
      "_type": "Component",
      "embedding_vectors": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": true,
        "list_add_label": "Add More",
        "trace_as_input": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "embedding_vectors",
        "value": "",
        "display_name": "Embedding Vectors",
        "advanced": false,
        "input_types": [
          "Data"
        ],
        "dynamic": false,
        "info": "A list containing exactly two data objects with embedding vectors to compare.",
        "title_case": false,
        "type": "other",
        "_input_type": "DataInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import numpy as np\n\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, Output\nfrom langflow.schema import Data\n\n\nclass EmbeddingSimilarityComponent(Component):\n    display_name: str = \"Embedding Similarity\"\n    description: str = \"Compute selected form of similarity between two embedding vectors.\"\n    icon = \"equal\"\n\n    inputs = [\n        DataInput(\n            name=\"embedding_vectors\",\n            display_name=\"Embedding Vectors\",\n            info=\"A list containing exactly two data objects with embedding vectors to compare.\",\n            is_list=True,\n            required=True,\n        ),\n        DropdownInput(\n            name=\"similarity_metric\",\n            display_name=\"Similarity Metric\",\n            info=\"Select the similarity metric to use.\",\n            options=[\"Cosine Similarity\", \"Euclidean Distance\", \"Manhattan Distance\"],\n            value=\"Cosine Similarity\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Similarity Data\", name=\"similarity_data\", method=\"compute_similarity\"),\n    ]\n\n    def compute_similarity(self) -> Data:\n        embedding_vectors: list[Data] = self.embedding_vectors\n\n        # Assert that the list contains exactly two Data objects\n        if len(embedding_vectors) != 2:  # noqa: PLR2004\n            msg = \"Exactly two embedding vectors are required.\"\n            raise ValueError(msg)\n\n        embedding_1 = np.array(embedding_vectors[0].data[\"embeddings\"])\n        embedding_2 = np.array(embedding_vectors[1].data[\"embeddings\"])\n\n        if embedding_1.shape != embedding_2.shape:\n            similarity_score = {\"error\": \"Embeddings must have the same dimensions.\"}\n        else:\n            similarity_metric = self.similarity_metric\n\n            if similarity_metric == \"Cosine Similarity\":\n                score = np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))\n                similarity_score = {\"cosine_similarity\": score}\n\n            elif similarity_metric == \"Euclidean Distance\":\n                score = np.linalg.norm(embedding_1 - embedding_2)\n                similarity_score = {\"euclidean_distance\": score}\n\n            elif similarity_metric == \"Manhattan Distance\":\n                score = np.sum(np.abs(embedding_1 - embedding_2))\n                similarity_score = {\"manhattan_distance\": score}\n\n        # Create a Data object to encapsulate the similarity score and additional information\n        similarity_data = Data(\n            data={\n                \"embedding_1\": embedding_vectors[0].data[\"embeddings\"],\n                \"embedding_2\": embedding_vectors[1].data[\"embeddings\"],\n                \"similarity_score\": similarity_score,\n            },\n            text_key=\"similarity_score\",\n        )\n\n        self.status = similarity_data\n        return similarity_data\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "similarity_metric": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Cosine Similarity",
          "Euclidean Distance",
          "Manhattan Distance"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "similarity_metric",
        "value": "Cosine Similarity",
        "display_name": "Similarity Metric",
        "advanced": false,
        "dynamic": false,
        "info": "Select the similarity metric to use.",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Compute selected form of similarity between two embedding vectors.",
    "icon": "equal",
    "base_classes": [
      "Data"
    ],
    "display_name": "Embedding Similarity",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "similarity_data",
        "display_name": "Similarity Data",
        "method": "compute_similarity",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "embedding_vectors",
      "similarity_metric"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AIMLEmbeddings": {
    "template": {
      "_type": "Component",
      "aiml_api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "aiml_api_key",
        "value": "AIML_API_KEY",
        "display_name": "AI/ML API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langflow.base.embeddings.aiml_embeddings import AIMLEmbeddingsImpl\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput\nfrom langflow.io import SecretStrInput\n\n\nclass AIMLEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"AI/ML Embeddings\"\n    description = \"Generate embeddings using the AI/ML API.\"\n    icon = \"AI/ML\"\n    name = \"AIMLEmbeddings\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"aiml_api_key\",\n            display_name=\"AI/ML API Key\",\n            value=\"AIML_API_KEY\",\n            required=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return AIMLEmbeddingsImpl(\n            api_key=self.aiml_api_key,\n            model=self.model_name,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "text-embedding-3-small",
          "text-embedding-3-large",
          "text-embedding-ada-002"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Generate embeddings using the AI/ML API.",
    "icon": "AI/ML",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "AI/ML Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "aiml_api_key",
          "model_name"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model_name",
      "aiml_api_key"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "CloudflareWorkersAIEmbeddings": {
    "template": {
      "_type": "Component",
      "account_id": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "account_id",
        "value": "",
        "display_name": "Cloudflare account ID",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Find your account ID https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "api_base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_base_url",
        "value": "https://api.cloudflare.com/client/v4/accounts",
        "display_name": "Cloudflare API base URL",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "api_token": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_token",
        "value": "",
        "display_name": "Cloudflare API token",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "Create an API token https://developers.cloudflare.com/fundamentals/api/get-started/create-token/",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "batch_size": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "batch_size",
        "value": 50,
        "display_name": "Batch Size",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass CloudflareWorkersAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Cloudflare Workers AI Embeddings\"\n    description: str = \"Generate embeddings using Cloudflare Workers AI models.\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/text_embedding/cloudflare_workersai/\"\n    icon = \"Cloudflare\"\n    name = \"CloudflareWorkersAIEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"account_id\",\n            display_name=\"Cloudflare account ID\",\n            info=\"Find your account ID https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_token\",\n            display_name=\"Cloudflare API token\",\n            info=\"Create an API token https://developers.cloudflare.com/fundamentals/api/get-started/create-token/\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            info=\"List of supported models https://developers.cloudflare.com/workers-ai/models/#text-embeddings\",\n            required=True,\n            value=\"@cf/baai/bge-base-en-v1.5\",\n        ),\n        BoolInput(\n            name=\"strip_new_lines\",\n            display_name=\"Strip New Lines\",\n            advanced=True,\n            value=True,\n        ),\n        IntInput(\n            name=\"batch_size\",\n            display_name=\"Batch Size\",\n            advanced=True,\n            value=50,\n        ),\n        MessageTextInput(\n            name=\"api_base_url\",\n            display_name=\"Cloudflare API base URL\",\n            advanced=True,\n            value=\"https://api.cloudflare.com/client/v4/accounts\",\n        ),\n        DictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"Additional request headers\",\n            is_list=True,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            embeddings = CloudflareWorkersAIEmbeddings(\n                account_id=self.account_id,\n                api_base_url=self.api_base_url,\n                api_token=self.api_token,\n                batch_size=self.batch_size,\n                headers=self.headers,\n                model_name=self.model_name,\n                strip_new_lines=self.strip_new_lines,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to CloudflareWorkersAIEmbeddings API: {e!s}\"\n            raise ValueError(msg) from e\n\n        return embeddings\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "headers": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "headers",
        "value": {},
        "display_name": "Headers",
        "advanced": true,
        "dynamic": false,
        "info": "Additional request headers",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "@cf/baai/bge-base-en-v1.5",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "List of supported models https://developers.cloudflare.com/workers-ai/models/#text-embeddings",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "strip_new_lines": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "strip_new_lines",
        "value": true,
        "display_name": "Strip New Lines",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      }
    },
    "description": "Generate embeddings using Cloudflare Workers AI models.",
    "icon": "Cloudflare",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Cloudflare Workers AI Embeddings",
    "documentation": "https://python.langchain.com/docs/integrations/text_embedding/cloudflare_workersai/",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "account_id",
      "api_token",
      "model_name",
      "strip_new_lines",
      "batch_size",
      "api_base_url",
      "headers"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AstraVectorize": {
    "template": {
      "_type": "Component",
      "api_key_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key_name",
        "value": "",
        "display_name": "API Key name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The name of the embeddings provider API key stored on Astra. If set, it will override the 'ProviderKey' in the authentication parameters.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "authentication": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "authentication",
        "value": {},
        "display_name": "Authentication Parameters",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DictInput, DropdownInput, MessageTextInput, SecretStrInput\nfrom langflow.template.field.base import Output\n\n\nclass AstraVectorizeComponent(Component):\n    display_name: str = \"Astra Vectorize [DEPRECATED]\"\n    description: str = (\n        \"Configuration options for Astra Vectorize server-side embeddings. \"\n        \"This component is deprecated. Please use the Astra DB Component directly.\"\n    )\n    documentation: str = \"https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html\"\n    icon = \"AstraDB\"\n    name = \"AstraVectorize\"\n\n    VECTORIZE_PROVIDERS_MAPPING = {\n        \"Azure OpenAI\": [\"azureOpenAI\", [\"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"]],\n        \"Hugging Face - Dedicated\": [\"huggingfaceDedicated\", [\"endpoint-defined-model\"]],\n        \"Hugging Face - Serverless\": [\n            \"huggingface\",\n            [\n                \"sentence-transformers/all-MiniLM-L6-v2\",\n                \"intfloat/multilingual-e5-large\",\n                \"intfloat/multilingual-e5-large-instruct\",\n                \"BAAI/bge-small-en-v1.5\",\n                \"BAAI/bge-base-en-v1.5\",\n                \"BAAI/bge-large-en-v1.5\",\n            ],\n        ],\n        \"Jina AI\": [\n            \"jinaAI\",\n            [\n                \"jina-embeddings-v2-base-en\",\n                \"jina-embeddings-v2-base-de\",\n                \"jina-embeddings-v2-base-es\",\n                \"jina-embeddings-v2-base-code\",\n                \"jina-embeddings-v2-base-zh\",\n            ],\n        ],\n        \"Mistral AI\": [\"mistral\", [\"mistral-embed\"]],\n        \"NVIDIA\": [\"nvidia\", [\"NV-Embed-QA\"]],\n        \"OpenAI\": [\"openai\", [\"text-embedding-3-small\", \"text-embedding-3-large\", \"text-embedding-ada-002\"]],\n        \"Upstage\": [\"upstageAI\", [\"solar-embedding-1-large\"]],\n        \"Voyage AI\": [\n            \"voyageAI\",\n            [\"voyage-large-2-instruct\", \"voyage-law-2\", \"voyage-code-2\", \"voyage-large-2\", \"voyage-2\"],\n        ],\n    }\n    VECTORIZE_MODELS_STR = \"\\n\\n\".join(\n        [provider + \": \" + (\", \".join(models[1])) for provider, models in VECTORIZE_PROVIDERS_MAPPING.items()]\n    )\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Provider\",\n            options=VECTORIZE_PROVIDERS_MAPPING.keys(),\n            value=\"\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            info=\"The embedding model to use for the selected provider. Each provider has a different set of models \"\n            f\"available (full list at https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html):\\n\\n{VECTORIZE_MODELS_STR}\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"api_key_name\",\n            display_name=\"API Key name\",\n            info=\"The name of the embeddings provider API key stored on Astra. \"\n            \"If set, it will override the 'ProviderKey' in the authentication parameters.\",\n        ),\n        DictInput(\n            name=\"authentication\",\n            display_name=\"Authentication parameters\",\n            is_list=True,\n            advanced=True,\n        ),\n        SecretStrInput(\n            name=\"provider_api_key\",\n            display_name=\"Provider API Key\",\n            info=\"An alternative to the Astra Authentication that passes an API key for the provider with each request \"\n            \"to Astra DB. \"\n            \"This may be used when Vectorize is configured for the collection, \"\n            \"but no corresponding provider secret is stored within Astra's key management system.\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"authentication\",\n            display_name=\"Authentication Parameters\",\n            is_list=True,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"model_parameters\",\n            display_name=\"Model Parameters\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Vectorize\", name=\"config\", method=\"build_options\", types=[\"dict\"]),\n    ]\n\n    def build_options(self) -> dict[str, Any]:\n        provider_value = self.VECTORIZE_PROVIDERS_MAPPING[self.provider][0]\n        authentication = {**(self.authentication or {})}\n        api_key_name = self.api_key_name\n        if api_key_name:\n            authentication[\"providerKey\"] = api_key_name\n        return {\n            # must match astrapy.info.VectorServiceOptions\n            \"collection_vector_service_options\": {\n                \"provider\": provider_value,\n                \"modelName\": self.model_name,\n                \"authentication\": authentication,\n                \"parameters\": self.model_parameters or {},\n            },\n            \"collection_embedding_api_key\": self.provider_api_key,\n        }\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The embedding model to use for the selected provider. Each provider has a different set of models available (full list at https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html):\n\nAzure OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002\n\nHugging Face - Dedicated: endpoint-defined-model\n\nHugging Face - Serverless: sentence-transformers/all-MiniLM-L6-v2, intfloat/multilingual-e5-large, intfloat/multilingual-e5-large-instruct, BAAI/bge-small-en-v1.5, BAAI/bge-base-en-v1.5, BAAI/bge-large-en-v1.5\n\nJina AI: jina-embeddings-v2-base-en, jina-embeddings-v2-base-de, jina-embeddings-v2-base-es, jina-embeddings-v2-base-code, jina-embeddings-v2-base-zh\n\nMistral AI: mistral-embed\n\nNVIDIA: NV-Embed-QA\n\nOpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002\n\nUpstage: solar-embedding-1-large\n\nVoyage AI: voyage-large-2-instruct, voyage-law-2, voyage-code-2, voyage-large-2, voyage-2",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "model_parameters": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": true,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_parameters",
        "value": {},
        "display_name": "Model Parameters",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "provider": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "Azure OpenAI",
          "Hugging Face - Dedicated",
          "Hugging Face - Serverless",
          "Jina AI",
          "Mistral AI",
          "NVIDIA",
          "OpenAI",
          "Upstage",
          "Voyage AI"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "provider",
        "value": "",
        "display_name": "Provider",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "provider_api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "provider_api_key",
        "value": "",
        "display_name": "Provider API Key",
        "advanced": true,
        "input_types": [],
        "dynamic": false,
        "info": "An alternative to the Astra Authentication that passes an API key for the provider with each request to Astra DB. This may be used when Vectorize is configured for the collection, but no corresponding provider secret is stored within Astra's key management system.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      }
    },
    "description": "Configuration options for Astra Vectorize server-side embeddings. This component is deprecated. Please use the Astra DB Component directly.",
    "icon": "AstraDB",
    "base_classes": [
      "dict"
    ],
    "display_name": "Astra Vectorize [DEPRECATED]",
    "documentation": "https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "dict"
        ],
        "selected": "dict",
        "name": "config",
        "display_name": "Vectorize",
        "method": "build_options",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "provider",
      "model_name",
      "api_key_name",
      "authentication",
      "provider_api_key",
      "authentication",
      "model_parameters"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "Google Generative AI Embeddings": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "# from langflow.field_typing import Data\n\n# TODO: remove ignore once the google package is published with types\nfrom google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai._common import GoogleGenerativeAIError\n\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output, SecretStrInput\n\nMIN_DIMENSION_ERROR = \"Output dimensionality must be at least 1\"\nMAX_DIMENSION_ERROR = (\n    \"Output dimensionality cannot exceed 768. Google's embedding models only support dimensions up to 768.\"\n)\nMAX_DIMENSION = 768\nMIN_DIMENSION = 1\n\n\nclass GoogleGenerativeAIEmbeddingsComponent(Component):\n    display_name = \"Google Generative AI Embeddings\"\n    description = (\n        \"Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, \"\n        \"found in the langchain-google-genai package.\"\n    )\n    documentation: str = \"https://python.langchain.com/v0.2/docs/integrations/text_embedding/google_generative_ai/\"\n    icon = \"Google\"\n    name = \"Google Generative AI Embeddings\"\n\n    inputs = [\n        SecretStrInput(name=\"api_key\", display_name=\"API Key\", required=True),\n        MessageTextInput(name=\"model_name\", display_name=\"Model Name\", value=\"models/text-embedding-004\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not self.api_key:\n            msg = \"API Key is required\"\n            raise ValueError(msg)\n\n        class HotaGoogleGenerativeAIEmbeddings(GoogleGenerativeAIEmbeddings):\n            def __init__(self, *args, **kwargs) -> None:\n                super(GoogleGenerativeAIEmbeddings, self).__init__(*args, **kwargs)\n\n            def embed_documents(\n                self,\n                texts: list[str],\n                *,\n                batch_size: int = 100,\n                task_type: str | None = None,\n                titles: list[str] | None = None,\n                output_dimensionality: int | None = 768,\n            ) -> list[list[float]]:\n                \"\"\"Embed a list of strings.\n\n                Google Generative AI currently sets a max batch size of 100 strings.\n\n                Args:\n                    texts: List[str] The list of strings to embed.\n                    batch_size: [int] The batch size of embeddings to send to the model\n                    task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n                    titles: An optional list of titles for texts provided.\n                    Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n                    output_dimensionality: Optional reduced dimension for the output embedding.\n                    https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n                Returns:\n                    List of embeddings, one for each text.\n                \"\"\"\n                if output_dimensionality is not None and output_dimensionality < MIN_DIMENSION:\n                    raise ValueError(MIN_DIMENSION_ERROR)\n                if output_dimensionality is not None and output_dimensionality > MAX_DIMENSION:\n                    error_msg = MAX_DIMENSION_ERROR.format(output_dimensionality)\n                    raise ValueError(error_msg)\n\n                embeddings: list[list[float]] = []\n                batch_start_index = 0\n                for batch in GoogleGenerativeAIEmbeddings._prepare_batches(texts, batch_size):\n                    if titles:\n                        titles_batch = titles[batch_start_index : batch_start_index + len(batch)]\n                        batch_start_index += len(batch)\n                    else:\n                        titles_batch = [None] * len(batch)  # type: ignore[list-item]\n\n                    requests = [\n                        self._prepare_request(\n                            text=text,\n                            task_type=task_type,\n                            title=title,\n                            output_dimensionality=output_dimensionality,\n                        )\n                        for text, title in zip(batch, titles_batch, strict=True)\n                    ]\n\n                    try:\n                        result = self.client.batch_embed_contents(\n                            BatchEmbedContentsRequest(requests=requests, model=self.model)\n                        )\n                    except Exception as e:\n                        msg = f\"Error embedding content: {e}\"\n                        raise GoogleGenerativeAIError(msg) from e\n                    embeddings.extend([list(e.values) for e in result.embeddings])\n                return embeddings\n\n            def embed_query(\n                self,\n                text: str,\n                task_type: str | None = None,\n                title: str | None = None,\n                output_dimensionality: int | None = 768,\n            ) -> list[float]:\n                \"\"\"Embed a text.\n\n                Args:\n                    text: The text to embed.\n                    task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n                    title: An optional title for the text.\n                    Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n                    output_dimensionality: Optional reduced dimension for the output embedding.\n                    https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n\n                Returns:\n                    Embedding for the text.\n                \"\"\"\n                if output_dimensionality is not None and output_dimensionality < MIN_DIMENSION:\n                    raise ValueError(MIN_DIMENSION_ERROR)\n                if output_dimensionality is not None and output_dimensionality > MAX_DIMENSION:\n                    error_msg = MAX_DIMENSION_ERROR.format(output_dimensionality)\n                    raise ValueError(error_msg)\n\n                task_type = task_type or \"RETRIEVAL_QUERY\"\n                return self.embed_documents(\n                    [text],\n                    task_type=task_type,\n                    titles=[title] if title else None,\n                    output_dimensionality=output_dimensionality,\n                )[0]\n\n        return HotaGoogleGenerativeAIEmbeddings(model=self.model_name, google_api_key=self.api_key)\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "models/text-embedding-004",
        "display_name": "Model Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      }
    },
    "description": "Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, found in the langchain-google-genai package.",
    "icon": "Google",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Google Generative AI Embeddings",
    "documentation": "https://python.langchain.com/v0.2/docs/integrations/text_embedding/google_generative_ai/",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "model_name"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "OpenAIEmbeddings": {
    "template": {
      "_type": "Component",
      "chunk_size": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "chunk_size",
        "value": 1000,
        "display_name": "Chunk Size",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "client": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "client",
        "value": "",
        "display_name": "Client",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_openai import OpenAIEmbeddings\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIEmbeddings\"\n\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=OPENAI_EMBEDDING_MODEL_NAMES,\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\", required=True),\n        MessageTextInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        MessageTextInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. \"\n            \"Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            client=self.client or None,\n            model=self.model,\n            dimensions=self.dimensions or None,\n            deployment=self.deployment or None,\n            api_version=self.openai_api_version or None,\n            base_url=self.openai_api_base or None,\n            openai_api_type=self.openai_api_type or None,\n            openai_proxy=self.openai_proxy or None,\n            embedding_ctx_length=self.embedding_ctx_length,\n            api_key=self.openai_api_key or None,\n            organization=self.openai_organization or None,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            max_retries=self.max_retries,\n            timeout=self.request_timeout or None,\n            tiktoken_enabled=self.tiktoken_enable,\n            tiktoken_model_name=self.tiktoken_model_name or None,\n            show_progress_bar=self.show_progress_bar,\n            model_kwargs=self.model_kwargs,\n            skip_empty=self.skip_empty,\n            default_headers=self.default_headers or None,\n            default_query=self.default_query or None,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "default_headers": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "default_headers",
        "value": {},
        "display_name": "Default Headers",
        "advanced": true,
        "dynamic": false,
        "info": "Default headers to use for the API request.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "default_query": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "default_query",
        "value": {},
        "display_name": "Default Query",
        "advanced": true,
        "dynamic": false,
        "info": "Default query parameters to use for the API request.",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "deployment": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "deployment",
        "value": "",
        "display_name": "Deployment",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "dimensions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "dimensions",
        "value": "",
        "display_name": "Dimensions",
        "advanced": true,
        "dynamic": false,
        "info": "The number of dimensions the resulting output embeddings should have. Only supported by certain models.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "embedding_ctx_length": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "embedding_ctx_length",
        "value": 1536,
        "display_name": "Embedding Context Length",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 3,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "text-embedding-3-small",
          "text-embedding-3-large",
          "text-embedding-ada-002"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "text-embedding-3-small",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "model_kwargs": {
        "tool_mode": false,
        "trace_as_input": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_kwargs",
        "value": {},
        "display_name": "Model Kwargs",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "dict",
        "_input_type": "DictInput"
      },
      "openai_api_base": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_api_base",
        "value": "",
        "display_name": "OpenAI API Base",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "openai_api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "openai_api_key",
        "value": "OPENAI_API_KEY",
        "display_name": "OpenAI API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "openai_api_type": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_api_type",
        "value": "",
        "display_name": "OpenAI API Type",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "openai_api_version": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_api_version",
        "value": "",
        "display_name": "OpenAI API Version",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "openai_organization": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_organization",
        "value": "",
        "display_name": "OpenAI Organization",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "openai_proxy": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "openai_proxy",
        "value": "",
        "display_name": "OpenAI Proxy",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "request_timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "request_timeout",
        "value": "",
        "display_name": "Request Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "show_progress_bar": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "show_progress_bar",
        "value": false,
        "display_name": "Show Progress Bar",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "skip_empty": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "skip_empty",
        "value": false,
        "display_name": "Skip Empty",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "tiktoken_enable": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tiktoken_enable",
        "value": true,
        "display_name": "TikToken Enable",
        "advanced": true,
        "dynamic": false,
        "info": "If False, you must have transformers installed.",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "tiktoken_model_name": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "tiktoken_model_name",
        "value": "",
        "display_name": "TikToken Model Name",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      }
    },
    "description": "Generate embeddings using OpenAI models.",
    "icon": "OpenAI",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "OpenAI Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "openai_api_key"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "default_headers",
      "default_query",
      "chunk_size",
      "client",
      "deployment",
      "embedding_ctx_length",
      "max_retries",
      "model",
      "model_kwargs",
      "openai_api_key",
      "openai_api_base",
      "openai_api_type",
      "openai_api_version",
      "openai_organization",
      "openai_proxy",
      "request_timeout",
      "show_progress_bar",
      "skip_empty",
      "tiktoken_model_name",
      "tiktoken_enable",
      "dimensions"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "LMStudioEmbeddingsComponent": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "LMSTUDIO_API_KEY",
        "display_name": "LM Studio API Key",
        "advanced": true,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "http://localhost:1234/v1",
        "display_name": "LM Studio Base URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom typing_extensions import override\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.inputs.inputs import DropdownInput, SecretStrInput\nfrom langflow.io import FloatInput, MessageTextInput\n\n\nclass LMStudioEmbeddingsComponent(LCEmbeddingsModel):\n    display_name: str = \"LM Studio Embeddings\"\n    description: str = \"Generate embeddings using LM Studio.\"\n    icon = \"LMStudio\"\n\n    @override\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:1234/v1\"\n            build_config[\"model\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            refresh_button=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"LM Studio Base URL\",\n            refresh_button=True,\n            value=\"http://localhost:1234/v1\",\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n        except ImportError as e:\n            msg = \"Please install langchain-nvidia-ai-endpoints to use LM Studio Embeddings.\"\n            raise ImportError(msg) from e\n        try:\n            output = NVIDIAEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n                nvidia_api_key=self.api_key,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to LM Studio API. Error: {e}\"\n            raise ValueError(msg) from e\n        return output\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "temperature": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "temperature",
        "value": 0.1,
        "display_name": "Model Temperature",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      }
    },
    "description": "Generate embeddings using LM Studio.",
    "icon": "LMStudio",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "LM Studio Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "base_url",
          "model"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model",
      "base_url",
      "api_key",
      "temperature"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "AzureOpenAIEmbeddings": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "api_version": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "2022-12-01",
          "2023-03-15-preview",
          "2023-05-15",
          "2023-06-01-preview",
          "2023-07-01-preview",
          "2023-08-01-preview"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "api_version",
        "value": "2023-08-01-preview",
        "display_name": "API Version",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "azure_deployment": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "azure_deployment",
        "value": "",
        "display_name": "Deployment Name",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "azure_endpoint": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "azure_endpoint",
        "value": "",
        "display_name": "Azure Endpoint",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from langchain_openai import AzureOpenAIEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass AzureOpenAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI Embeddings\"\n    description: str = \"Generate embeddings using Azure OpenAI models.\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/text_embedding/azureopenai\"\n    icon = \"Azure\"\n    name = \"AzureOpenAIEmbeddings\"\n\n    API_VERSION_OPTIONS = [\n        \"2022-12-01\",\n        \"2023-03-15-preview\",\n        \"2023-05-15\",\n        \"2023-06-01-preview\",\n        \"2023-07-01-preview\",\n        \"2023-08-01-preview\",\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=OPENAI_EMBEDDING_MODEL_NAMES,\n            value=OPENAI_EMBEDDING_MODEL_NAMES[0],\n        ),\n        MessageTextInput(\n            name=\"azure_endpoint\",\n            display_name=\"Azure Endpoint\",\n            required=True,\n            info=\"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\",\n        ),\n        MessageTextInput(\n            name=\"azure_deployment\",\n            display_name=\"Deployment Name\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"api_version\",\n            display_name=\"API Version\",\n            options=API_VERSION_OPTIONS,\n            value=API_VERSION_OPTIONS[-1],\n            advanced=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            required=True,\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. \"\n            \"Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            embeddings = AzureOpenAIEmbeddings(\n                model=self.model,\n                azure_endpoint=self.azure_endpoint,\n                azure_deployment=self.azure_deployment,\n                api_version=self.api_version,\n                api_key=self.api_key,\n                dimensions=self.dimensions or None,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to AzureOpenAIEmbeddings API: {e}\"\n            raise ValueError(msg) from e\n\n        return embeddings\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "dimensions": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "dimensions",
        "value": "",
        "display_name": "Dimensions",
        "advanced": true,
        "dynamic": false,
        "info": "The number of dimensions the resulting output embeddings should have. Only supported by certain models.",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "text-embedding-3-small",
          "text-embedding-3-large",
          "text-embedding-ada-002"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model",
        "value": "text-embedding-3-small",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Generate embeddings using Azure OpenAI models.",
    "icon": "Azure",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Azure OpenAI Embeddings",
    "documentation": "https://python.langchain.com/docs/integrations/text_embedding/azureopenai",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model",
      "azure_endpoint",
      "azure_deployment",
      "api_version",
      "api_key",
      "dimensions"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "TextEmbedderComponent": {
    "template": {
      "_type": "Component",
      "embedding_model": {
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "embedding_model",
        "value": "",
        "display_name": "Embedding Model",
        "advanced": false,
        "input_types": [
          "Embeddings"
        ],
        "dynamic": false,
        "info": "The embedding model to use for generating embeddings.",
        "title_case": false,
        "type": "other",
        "_input_type": "HandleInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import logging\nfrom typing import TYPE_CHECKING\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, MessageInput, Output\nfrom langflow.schema import Data\n\nif TYPE_CHECKING:\n    from langflow.field_typing import Embeddings\n    from langflow.schema.message import Message\n\n\nclass TextEmbedderComponent(Component):\n    display_name: str = \"Text Embedder\"\n    description: str = \"Generate embeddings for a given message using the specified embedding model.\"\n    icon = \"binary\"\n    inputs = [\n        HandleInput(\n            name=\"embedding_model\",\n            display_name=\"Embedding Model\",\n            info=\"The embedding model to use for generating embeddings.\",\n            input_types=[\"Embeddings\"],\n            required=True,\n        ),\n        MessageInput(\n            name=\"message\",\n            display_name=\"Message\",\n            info=\"The message to generate embeddings for.\",\n            required=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Embedding Data\", name=\"embeddings\", method=\"generate_embeddings\"),\n    ]\n\n    def generate_embeddings(self) -> Data:\n        try:\n            embedding_model: Embeddings = self.embedding_model\n            message: Message = self.message\n\n            # Combine validation checks to reduce nesting\n            if not embedding_model or not hasattr(embedding_model, \"embed_documents\"):\n                msg = \"Invalid or incompatible embedding model\"\n                raise ValueError(msg)\n\n            text_content = message.text if message and message.text else \"\"\n            if not text_content:\n                msg = \"No text content found in message\"\n                raise ValueError(msg)\n\n            embeddings = embedding_model.embed_documents([text_content])\n            if not embeddings or not isinstance(embeddings, list):\n                msg = \"Invalid embeddings generated\"\n                raise ValueError(msg)\n\n            embedding_vector = embeddings[0]\n            self.status = {\"text\": text_content, \"embeddings\": embedding_vector}\n            return Data(data={\"text\": text_content, \"embeddings\": embedding_vector})\n        except Exception as e:\n            logging.exception(\"Error generating embeddings\")\n            error_data = Data(data={\"text\": \"\", \"embeddings\": [], \"error\": str(e)})\n            self.status = {\"error\": str(e)}\n            return error_data\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "message": {
        "trace_as_input": true,
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "message",
        "value": "",
        "display_name": "Message",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "The message to generate embeddings for.",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageInput"
      }
    },
    "description": "Generate embeddings for a given message using the specified embedding model.",
    "icon": "binary",
    "base_classes": [
      "Data"
    ],
    "display_name": "Text Embedder",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Data"
        ],
        "selected": "Data",
        "name": "embeddings",
        "display_name": "Embedding Data",
        "method": "generate_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "embedding_model",
      "message"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "WatsonxEmbeddingsComponent": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "The API Key to use for the model.",
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "import logging\nfrom typing import Any\n\nimport requests\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\nfrom langchain_ibm import WatsonxEmbeddings\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DropdownInput, IntInput, SecretStrInput, StrInput\nfrom langflow.schema.dotdict import dotdict\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass WatsonxEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"IBM watsonx.ai Embeddings\"\n    description = \"Generate embeddings using IBM watsonx.ai models.\"\n    icon = \"WatsonxAI\"\n    name = \"WatsonxEmbeddingsComponent\"\n\n    # models present in all the regions\n    _default_models = [\n        \"sentence-transformers/all-minilm-l12-v2\",\n        \"ibm/slate-125m-english-rtrvr-v2\",\n        \"ibm/slate-30m-english-rtrvr-v2\",\n        \"intfloat/multilingual-e5-large\",\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"url\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API.\",\n            value=None,\n            options=[\n                \"https://us-south.ml.cloud.ibm.com\",\n                \"https://eu-de.ml.cloud.ibm.com\",\n                \"https://eu-gb.ml.cloud.ibm.com\",\n                \"https://au-syd.ml.cloud.ibm.com\",\n                \"https://jp-tok.ml.cloud.ibm.com\",\n                \"https://ca-tor.ml.cloud.ibm.com\",\n            ],\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx project id\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The API Key to use for the model.\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[],\n            value=None,\n            dynamic=True,\n            required=True,\n        ),\n        IntInput(\n            name=\"truncate_input_tokens\",\n            display_name=\"Truncate Input Tokens\",\n            advanced=True,\n            value=200,\n        ),\n        BoolInput(\n            name=\"input_text\",\n            display_name=\"Include the original text in the output\",\n            value=True,\n            advanced=True,\n        ),\n    ]\n\n    @staticmethod\n    def fetch_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\n                \"version\": \"2024-09-16\",\n                \"filters\": \"function_embedding,!lifecycle_withdrawn:and\",\n            }\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:\n            logger.exception(\"Error fetching models\")\n            return WatsonxEmbeddingsComponent._default_models\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        \"\"\"Update model options when URL or API key changes.\"\"\"\n        logger.debug(\n            \"Updating build config. Field name: %s, Field value: %s\",\n            field_name,\n            field_value,\n        )\n\n        if field_name == \"url\" and field_value:\n            try:\n                models = self.fetch_models(base_url=build_config.url.value)\n                build_config.model_name.options = models\n                if build_config.model_name.value:\n                    build_config.model_name.value = models[0]\n                info_message = f\"Updated model options: {len(models)} models found in {build_config.url.value}\"\n                logger.info(info_message)\n            except Exception:\n                logger.exception(\"Error updating model options.\")\n\n    def build_embeddings(self) -> Embeddings:\n        credentials = Credentials(\n            api_key=SecretStr(self.api_key).get_secret_value(),\n            url=self.url,\n        )\n\n        api_client = APIClient(credentials)\n\n        params = {\n            EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: self.truncate_input_tokens,\n            EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": self.input_text},\n        }\n\n        return WatsonxEmbeddings(\n            model_id=self.model_name,\n            params=params,\n            watsonx_client=api_client,\n            project_id=self.project_id,\n        )\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "input_text": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "input_text",
        "value": true,
        "display_name": "Include the original text in the output",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "bool",
        "_input_type": "BoolInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "display_name": "Model Name",
        "advanced": false,
        "dynamic": true,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "project_id": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "project_id",
        "value": "",
        "display_name": "watsonx project id",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "StrInput"
      },
      "truncate_input_tokens": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "truncate_input_tokens",
        "value": 200,
        "display_name": "Truncate Input Tokens",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "url": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "https://us-south.ml.cloud.ibm.com",
          "https://eu-de.ml.cloud.ibm.com",
          "https://eu-gb.ml.cloud.ibm.com",
          "https://au-syd.ml.cloud.ibm.com",
          "https://jp-tok.ml.cloud.ibm.com",
          "https://ca-tor.ml.cloud.ibm.com"
        ],
        "options_metadata": [],
        "combobox": false,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "url",
        "display_name": "watsonx API Endpoint",
        "advanced": false,
        "dynamic": false,
        "info": "The base URL of the API.",
        "real_time_refresh": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Generate embeddings using IBM watsonx.ai models.",
    "icon": "WatsonxAI",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "IBM watsonx.ai Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "required_inputs": [
          "api_key",
          "model_name"
        ],
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "url",
      "project_id",
      "api_key",
      "model_name",
      "truncate_input_tokens",
      "input_text"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "CohereEmbeddings": {
    "template": {
      "_type": "Component",
      "api_key": {
        "load_from_db": true,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "api_key",
        "value": "",
        "display_name": "Cohere API Key",
        "advanced": false,
        "input_types": [],
        "dynamic": false,
        "info": "",
        "real_time_refresh": true,
        "title_case": false,
        "password": true,
        "type": "str",
        "_input_type": "SecretStrInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\n\nimport cohere\nfrom langchain_cohere import CohereEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, Output, SecretStrInput\n\nHTTP_STATUS_OK = 200\n\n\nclass CohereEmbeddingsComponent(LCModelComponent):\n    display_name = \"Cohere Embeddings\"\n    description = \"Generate embeddings using Cohere models.\"\n    icon = \"Cohere\"\n    name = \"CohereEmbeddings\"\n\n    inputs = [\n        SecretStrInput(name=\"api_key\", display_name=\"Cohere API Key\", required=True, real_time_refresh=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"embed-english-v2.0\",\n                \"embed-multilingual-v2.0\",\n                \"embed-english-light-v2.0\",\n                \"embed-multilingual-light-v2.0\",\n            ],\n            value=\"embed-english-v2.0\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        MessageTextInput(name=\"truncate\", display_name=\"Truncate\", advanced=True),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        MessageTextInput(name=\"user_agent\", display_name=\"User Agent\", advanced=True, value=\"langchain\"),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        data = None\n        try:\n            data = CohereEmbeddings(\n                cohere_api_key=self.api_key,\n                model=self.model_name,\n                truncate=self.truncate,\n                max_retries=self.max_retries,\n                user_agent=self.user_agent,\n                request_timeout=self.request_timeout or None,\n            )\n        except Exception as e:\n            msg = (\n                \"Unable to create Cohere Embeddings. \",\n                \"Please verify the API key and model parameters, and try again.\",\n            )\n            raise ValueError(msg) from e\n        # added status if not the return data would be serialised to create the status\n        return data\n\n    def get_model(self):\n        try:\n            co = cohere.ClientV2(self.api_key)\n            response = co.models.list(endpoint=\"embed\")\n            models = response.models\n            return [model.name for model in models]\n        except Exception as e:\n            msg = f\"Failed to fetch Cohere models. Error: {e}\"\n            raise ValueError(msg) from e\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name in {\"model_name\", \"api_key\"}:\n            if build_config.get(\"api_key\", {}).get(\"value\", None):\n                build_config[\"model_name\"][\"options\"] = self.get_model()\n        else:\n            build_config[\"model_name\"][\"options\"] = field_value\n        return build_config\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "max_retries": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "max_retries",
        "value": 3,
        "display_name": "Max Retries",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "int",
        "_input_type": "IntInput"
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [
          "embed-english-v2.0",
          "embed-multilingual-v2.0",
          "embed-english-light-v2.0",
          "embed-multilingual-light-v2.0"
        ],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "embed-english-v2.0",
        "display_name": "Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      },
      "request_timeout": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "request_timeout",
        "value": "",
        "display_name": "Request Timeout",
        "advanced": true,
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "float",
        "_input_type": "FloatInput"
      },
      "truncate": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "truncate",
        "value": "",
        "display_name": "Truncate",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "user_agent": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": false,
        "placeholder": "",
        "show": true,
        "name": "user_agent",
        "value": "langchain",
        "display_name": "User Agent",
        "advanced": true,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      }
    },
    "description": "Generate embeddings using Cohere models.",
    "icon": "Cohere",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Cohere Embeddings",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "api_key",
      "model_name",
      "truncate",
      "max_retries",
      "user_agent",
      "request_timeout"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  },
  "OllamaEmbeddings": {
    "template": {
      "_type": "Component",
      "base_url": {
        "tool_mode": false,
        "trace_as_input": true,
        "trace_as_metadata": true,
        "load_from_db": false,
        "list": false,
        "list_add_label": "Add More",
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "base_url",
        "value": "",
        "display_name": "Ollama Base URL",
        "advanced": false,
        "input_types": [
          "Message"
        ],
        "dynamic": false,
        "info": "",
        "title_case": false,
        "type": "str",
        "_input_type": "MessageTextInput"
      },
      "code": {
        "type": "code",
        "required": true,
        "placeholder": "",
        "list": false,
        "show": true,
        "multiline": true,
        "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, URL_LIST\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, MessageTextInput, Output\n\nHTTP_STATUS_OK = 200\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Ollama Model\",\n            value=\"\",\n            options=[],\n            real_time_refresh=True,\n            refresh_button=True,\n            combobox=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"\",\n            required=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(model=self.model_name, base_url=self.base_url)\n        except Exception as e:\n            msg = (\n                \"Unable to connect to the Ollama API. \",\n                \"Please verify the base URL, ensure the relevant Ollama model is pulled, and try again.\",\n            )\n            raise ValueError(msg) from e\n        return output\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name in {\"base_url\", \"model_name\"} and not await self.is_valid_ollama_url(field_value):\n            # Check if any URL in the list is valid\n            valid_url = \"\"\n            for url in URL_LIST:\n                if await self.is_valid_ollama_url(url):\n                    valid_url = url\n                    break\n            build_config[\"base_url\"][\"value\"] = valid_url\n        if field_name in {\"model_name\", \"base_url\", \"tool_model_enabled\"}:\n            if await self.is_valid_ollama_url(self.base_url):\n                build_config[\"model_name\"][\"options\"] = await self.get_model(self.base_url)\n            elif await self.is_valid_ollama_url(build_config[\"base_url\"].get(\"value\", \"\")):\n                build_config[\"model_name\"][\"options\"] = await self.get_model(build_config[\"base_url\"].get(\"value\", \"\"))\n            else:\n                build_config[\"model_name\"][\"options\"] = []\n\n        return build_config\n\n    async def get_model(self, base_url_value: str) -> list[str]:\n        \"\"\"Get the model names from Ollama.\"\"\"\n        model_ids = []\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n            model_ids = [model[\"name\"] for model in data.get(\"models\", [])]\n            # this to ensure that not embedding models are included.\n            # not even the base models since models can have 1b 2b etc\n            # handles cases when embeddings models have tags like :latest - etc.\n            model_ids = [\n                model\n                for model in model_ids\n                if any(model.startswith(f\"{embedding_model}\") for embedding_model in OLLAMA_EMBEDDING_MODELS)\n            ]\n\n        except (ImportError, ValueError, httpx.RequestError) as e:\n            msg = \"Could not get model names from Ollama.\"\n            raise ValueError(msg) from e\n\n        return model_ids\n\n    async def is_valid_ollama_url(self, url: str) -> bool:\n        try:\n            async with httpx.AsyncClient() as client:\n                return (await client.get(f\"{url}/api/tags\")).status_code == HTTP_STATUS_OK\n        except httpx.RequestError:\n            return False\n",
        "fileTypes": [],
        "file_path": "",
        "password": false,
        "name": "code",
        "advanced": true,
        "dynamic": true,
        "info": "",
        "load_from_db": false,
        "title_case": false
      },
      "model_name": {
        "tool_mode": false,
        "trace_as_metadata": true,
        "options": [],
        "options_metadata": [],
        "combobox": true,
        "dialog_inputs": {},
        "toggle": false,
        "required": true,
        "placeholder": "",
        "show": true,
        "name": "model_name",
        "value": "",
        "display_name": "Ollama Model",
        "advanced": false,
        "dynamic": false,
        "info": "",
        "real_time_refresh": true,
        "refresh_button": true,
        "title_case": false,
        "type": "str",
        "_input_type": "DropdownInput"
      }
    },
    "description": "Generate embeddings using Ollama models.",
    "icon": "Ollama",
    "base_classes": [
      "Embeddings"
    ],
    "display_name": "Ollama Embeddings",
    "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
      {
        "types": [
          "Embeddings"
        ],
        "selected": "Embeddings",
        "name": "embeddings",
        "display_name": "Embeddings",
        "method": "build_embeddings",
        "value": "__UNDEFINED__",
        "cache": true,
        "allows_loop": false,
        "tool_mode": true
      }
    ],
    "field_order": [
      "model_name",
      "base_url"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false
  }
}